&&& FILE: ./.gitignore
&&& CONTENT:
# Ignore local data files
data_local/

# Python cache
__pycache__/
*.pyc

# Environment variables
.env

# Other ignores
*.so

# tileset
backend/tileset*/
!backend/tileset/README.md

*.gml
*.gfs
*.obj

#venv
venv/
.venv/
.conda

node_modules/

postgres_data/*
postgres_backups/*

data/

&&& FILE: ./docker-compose.yml
&&& CONTENT:
services:
  easyopen_postgis:
    container_name: easyopen_postgis
    image: postgis/postgis:17-3.5
    restart: always
    ports:
      - 8735:5432
    environment:
      POSTGRES_PASSWORD: barcelona
    volumes:
      - ./data/postgres_data:/var/lib/postgresql/data
      - ./backend/db/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 10s
      timeout: 5s
      retries: 20
    networks:
      - easyopen_network

  easyopen_backend:
    container_name: easyopen_backend
    build:
      context: ./backend
      dockerfile: Dockerfile
    command: uvicorn app.main:app --host 0.0.0.0 --port 5400 --reload
    volumes:
      - ./backend:/app
      - ./data/tileset:/data/tileset
      - ./data/tileset_combined:/data/tileset_combined
      - ./data/tempfiles:/data/tempfiles
    ports:
      - 5400:5400
    environment:
      DATABASE_URL: postgresql+asyncpg://postgres:barcelona@easyopen_postgis:5432/easyopendata_database
    depends_on:
      easyopen_postgis:
        condition: service_healthy
    networks:
      - easyopen_network

  easyopen_frontend:
    container_name: easyopen_frontend
    build:
      context: ./frontend
      dockerfile: Dockerfile
    volumes:
      - ./frontend:/app
      - /app/node_modules
    ports:
      - 5173:5173
    environment:
      VITE_BACKEND_URL: https://easyopen-server.i-am-hellguz.uk
    networks:
      - easyopen_network

  easyopen_tileset:
    container_name: easyopen_tileset
    image: nginx:alpine
    restart: always
    ports:
      - 5576:80
    volumes:
      - ./data/tileset:/usr/share/nginx/html:ro
      - ./backend/tileset.conf:/etc/nginx/conf.d/default.conf:ro
    networks:
      - easyopen_network

  easyopen_backup_service:
    build: ./backend/db_backup # path to your Dockerfile with cron and pg_dump
    container_name: easyopen_backup_service
    depends_on:
      - easyopen_postgis
    volumes:
      - ./data/postgres_backups:/backups
    environment:
      POSTGRES_PASSWORD: barcelona
    networks:
      - easyopen_network

  easyopen_pgadmin:
    container_name: easyopen_pgadmin
    image: dpage/pgadmin4
    restart: always
    ports:
      - 5050:80
    environment:
      PGADMIN_DEFAULT_EMAIL: hellguz@gmail.com
      PGADMIN_DEFAULT_PASSWORD: barcelona
    depends_on:
      - easyopen_postgis
    networks:
      - easyopen_network

  easyopen_cloudflared:
    container_name: easyopen_cloudflared
    image: cloudflare/cloudflared:latest
    depends_on:
      - easyopen_backend
    restart: always
    command: 'tunnel --no-autoupdate run --token eyJhIjoiMjViZjczZmU2MmZlMzQwN2Y3MTI4NGU4MjZlZGQ0MjEiLCJ0IjoiYjAwODUzY2QtYmE1MC00MzA0LTg3NzktMWQ1Y2RjMzI4YWIxIiwicyI6IlltUTNOMlpqWTJVdE1tVmpZaTAwWmpoaUxUZzRPVGt0WkRVMlpUUmhZVE5sTkdWaiJ9'
    networks:
      - easyopen_network

volumes:
  postgres_data:

networks:
  easyopen_network:
    driver: bridge

&&& FILE: ./README.md
&&& CONTENT:
# EasyOpenData

## Open Data Extractor for German Spatial Datasets

### Overview

EasyOpenData is a platform that provides an easy-to-use interface for accessing and downloading spatial data from German open data sources, covering all BundeslÃ¤nder. It standardizes the diverse formats in which these datasets are available and presents them to users via an intuitive web interface.

Users can interact with a map, select areas of interest (via polygons), and choose the data layers they wish to download. Available data types include:

- **3D Buildings (LOD1/LOD2)**

The platform processes the selected data and provides it in the user's desired format after payment, either as a direct download or via email.

---

### Key Features

- **Map-Based User Interface**: Allows users to interact with spatial data visually by selecting areas on a map.
- **Multi-Format Support**: Data is standardized into a unified format, enabling seamless querying and export.
- **Dynamic Data Processing**: Fetches and processes data only for the area selected by the user, ensuring efficiency.
- **Real-Time Visualization**: Users can preview data layers on the map as they select their areas of interest.
- **Payment Integration**: Secure payment processing via Stripe.
- **Scalable Backend**: Built for performance, capable of handling large datasets from multiple regions.

---

### Technologies Used

#### Backend

- **PostGIS**: Spatial database for storing and querying geographic data efficiently.
- **Python FastAPI**: Backend framework for building APIs to handle user requests and interact with PostGIS.
- **GDAL**: Used for data conversion between formats.
- **Docker**: Containerized deployment for portability and scalability.
- **Stripe API**: For handling payment transactions.

#### Frontend

- **React**: JavaScript library for building user interfaces.
- **MapLibre GL JS**: Interactive map interface for visualization and area selection.
- **Deck.gl**: For rendering 3D data on the map.
- **Mapbox Draw**: Allows users to draw shapes on the map.

---

### Getting Started

#### Prerequisites

- **Docker** and **Docker Compose** installed on your system.
- **Node.js** and **npm/yarn** (for frontend development).
- **Python 3.10+** (for backend development).

#### Setup Instructions

##### Clone the Repository

```bash
git clone https://github.com/your-repo/easyopendata.git
cd easyopendata
```

##### Environment Variables

Create a `.env` file in both the `backend` and `frontend` directories if needed to set environment variables.

##### Run with Docker Compose

```bash
docker-compose up --build
```

This command will:

- Start the PostGIS database.
- Build and run the backend FastAPI server.
- Build and run the frontend React application.

##### Access the Application

- **Frontend**: [http://localhost:5173](http://localhost:5173)
- **Backend API**: [http://localhost:5400](http://localhost:5400)

---

### Usage

1. **Open the Web Application**: Navigate to [http://localhost:5173](http://localhost:5173) in your browser.
2. **Select an Area**: Use the drawing tools to select an area on the map.
3. **Choose Data Layers**: Select the data layers you wish to download.
4. **Payment**: Proceed to payment if required.
5. **Download Data**: After processing, download your data in the desired format.

---

### Project Structure

- **backend/**: Contains the FastAPI backend application.
  - `app/`: FastAPI application code.
  - `db/`: Database initialization scripts.
  - `ingestion/`: Scripts for data ingestion and processing.
- **frontend/**: Contains the React frontend application.
  - `src/`: React application source code.
  - `public/`: Static assets.

---

### Contributing

Contributions are welcome! Please fork the repository and submit a pull request with your changes. Ensure all code adheres to the style guide and includes proper documentation.

---

### License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.

---

### Contact

For questions, feedback, or support, contact us at:

- **Email**: support@easyopendata.com

&&& FILE: ./backend\Dockerfile
&&& CONTENT:
# ./backend/Dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir -r /app/requirements.txt

COPY . /app

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]


&&& FILE: ./backend\README.md
&&& CONTENT:
# EasyOpenData Backend

## Overview

The backend of EasyOpenData is a FastAPI application that serves as the core of the platform, handling data retrieval, processing, and API endpoints for the frontend application. It interacts with a PostGIS database to store and query spatial data.

---

## Key Components

- **FastAPI Application**: Provides RESTful APIs for data retrieval and processing.
- **PostGIS Database**: Stores spatial data and provides efficient spatial queries.
- **Data Ingestion Scripts**: Scripts to download, transform, and ingest data into the database.
- **Payment Processing**: Integration with Stripe API for handling payments.

---

## Setup Instructions

### Prerequisites

- **Docker** and **Docker Compose**
- **Python 3.10+**
- **GDAL** and **OGR** libraries
- **Conda** (optional, for managing environments)
- **Node.js** and **npm** (for gltf-pipeline)

### Running with Docker

The easiest way to run the backend is using Docker Compose.

#### Build and Run

```bash
docker-compose up --build
```

This will start the PostGIS database and the FastAPI backend.

#### Accessing the Backend API

- The backend API will be available at: [http://localhost:5400](http://localhost:5400)

### Manual Setup

If you prefer to run the backend without Docker, follow these steps.

#### 1. Create a Virtual Environment

Using Conda:

```bash
conda create -n easyopendata_env python=3.10
conda activate easyopendata_env
```

#### 2. Install Dependencies

Install GDAL and its libraries:

```bash
conda install -c conda-forge gdal libgdal
```

Install other dependencies:

```bash
pip install -r backend/requirements.txt
```

#### 3. Install gltf-pipeline

```bash
npm install -g gltf-pipeline
```

#### 4. Setup the Database

Ensure PostgreSQL and PostGIS are installed.

```bash
sudo apt install postgresql postgis
```

Create the database and extensions:

```sql
CREATE DATABASE easyopendata_database;
\c easyopendata_database;
CREATE EXTENSION postgis;
```

#### 5. Run Database Initialization Scripts

```bash
psql -U postgres -d easyopendata_database -f backend/db/init.sql
```

#### 6. Run the FastAPI Application

```bash
cd backend
uvicorn app.main:app --reload
```

---

## Data Ingestion

The `backend/ingestion` directory contains scripts to download and process data.

### Ingesting Data

1. **Navigate to the ingestion directory**

```bash
cd backend/ingestion
```

2. **Run the Ingestion Script**

The `bayern.py` script processes Meta4 files to download and ingest data.

```bash
python bayern.py
```

**Note**: Ensure you have the necessary Meta4 files in `data_sources/` and that you have adjusted the `META4_PATH` in the script.

### Requirements

- **lxml**
- **psycopg2**
- **GDAL/OGR** command-line tools (`ogr2ogr`)
- **pg2b3dm** executable (must be in `libs/`)

---

## Directory Structure

- **app/**: FastAPI application code.
  - `main.py`: Main application file.
  - `database.py`: Database connection setup.
  - `models.py`: Database models and Pydantic schemas.
  - `retrieve_geom.py`: Functions to retrieve and process geometries.

- **db/**: Database scripts.
  - `init.sql`: Database initialization script.
  - `index.sql`: Indexing script for the database.

- **ingestion/**: Data ingestion scripts.
  - `bayern.py`: Script to process and ingest data.
  - `data_sources/`: Directory containing Meta4 files.
  - `data_local/`: Directory where downloaded data will be stored.
  - `libs/`: Contains necessary executables like `pg2b3dm.exe`.

---

## API Endpoints

- **GET /**: Root endpoint to check if the backend is running.
- **POST /retrieve_obj**: Accepts a GeoJSON region and returns an OBJ file with the buildings within that region.
- **POST /create-payment-intent**: Creates a Stripe payment intent for processing payments.

---

## Environment Variables

- **DATABASE_URL**: The connection string for the PostGIS database.
- **STRIPE_API_KEY**: Your Stripe secret API key for payment processing.

---

## Payment Processing

The backend integrates with Stripe to handle payments. Ensure you have set your Stripe API keys in the environment variables.

---

## Contributing

Contributions are welcome! Please ensure any changes to the backend code are thoroughly tested.

---

## License

This project is licensed under the MIT License.

&&& FILE: ./backend\requirements.txt
&&& CONTENT:
fastapi
uvicorn[standard]
asyncpg
psycopg2-binary
sqlalchemy
geoalchemy2
# Additional dependencies
python-multipart
geojson
anyio
pydantic
packaging
redis
stripe
pyproj
lxml

&&& FILE: ./backend\tileset.conf
&&& CONTENT:
server {
    listen 80;
    server_name localhost;

    root /usr/share/nginx/html;

    # Add the CORS header
    add_header 'Access-Control-Allow-Origin' '*' always;

    # If you want to allow specific headers or methods, add them too
    # add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS' always;
    # add_header 'Access-Control-Allow-Headers' 'Authorization, Content-Type' always;

    location / {
        try_files $uri $uri/ =404;
    }
}


&&& FILE: ./backend\app\database.py
&&& CONTENT:
import os
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from app.models import Base

DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql+asyncpg://postgres:barcelona@localhost:8735/easyopendata_database')

engine = create_async_engine(DATABASE_URL, echo=True)
async_session = sessionmaker(
    bind=engine, class_=AsyncSession, expire_on_commit=False
)

# Function to create tables
async def init_db():
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)



&&& FILE: ./backend\app\main.py
&&& CONTENT:
# ./backend/main.py

import asyncio
import tempfile
from typing import List
from urllib import request
import uuid
from fastapi import FastAPI, Depends, HTTPException
from pydantic import BaseModel
from sqlalchemy.ext.asyncio import AsyncSession
import stripe
from app.database import async_session, init_db
from app.models import Building, RegionRequest
from app.retrieve_geom import retrieve_obj_file
from sqlalchemy.future import select
from geoalchemy2.functions import ST_AsGeoJSON, ST_Intersects, ST_GeomFromText, ST_SimplifyPreserveTopology
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
import json
import math
import redis
import subprocess
import os
import logging
from fastapi.staticfiles import StaticFiles

import shutil

# Configure Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()


app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "https://easyopen.i-am-hellguz.uk", "https://easyopen-*.i-am-hellguz.uk"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def on_startup():
    await init_db()

async def get_db():
    async with async_session() as session:
        yield session

@app.get("/")
async def read_root():
    return {"message": "Easy Open Data v1.0"}

@app.post("/retrieve_obj")
async def retrieve_obj(request: RegionRequest):
    print(f"Received region: {request.region}")

    try:
        # Generate a unique filename using UUID
        random_filename = f"{uuid.uuid4()}.obj"
        temp_path = os.path.join("/data/tempfiles", random_filename)
        await retrieve_obj_file(request.region, temp_path)
        return FileResponse(
            temp_path,
            media_type="application/octet-stream",
            filename=f"object.txt")
    except Exception as e:
        logger.error(f"Error in retrieve_obj: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


async def remove_temp_file(file_path: str):
    await asyncio.sleep(0)  # Ensure this runs after the response is sent
    if os.path.exists(file_path):
        os.unlink(file_path)

        
stripe.api_key = 'REMOVED_STRIPE_KEY'


def calculate_order_amount(amount: float):
    # Replace this constant with a calculation of the order's amount
    # Calculate the order total on the server to prevent
    # people from directly manipulating the amount on the client
    return int(amount*100)

class PaymentIntentRequest(BaseModel): 
    amount: float

@app.post("/create-payment-intent")
async def create_payment_intent(data: PaymentIntentRequest):
    try:
        intent = stripe.PaymentIntent.create(
            amount=calculate_order_amount(data.amount),  # Amount in cents
            currency="eur",
            automatic_payment_methods={"enabled": True},
        )
        return {"clientSecret": intent.client_secret}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
app.mount("/tileset", StaticFiles(directory="../data/tileset"), name="tileset")


&&& FILE: ./backend\app\models.py
&&& CONTENT:
from sqlalchemy import Column, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from geoalchemy2 import Geometry

Base = declarative_base()

class Building(Base):
    __tablename__ = 'building'

    gml_id = Column(String, primary_key=True)
    name = Column(String)
    geom = Column(Geometry('MULTIPOLYGONZ', srid=4326))  # Adjust geometry type as needed

from pydantic import BaseModel

class RegionRequest(BaseModel):
    region: dict  # Adjust the type if you have a more specific structure


&&& FILE: ./backend\app\retrieve_geom.py
&&& CONTENT:
# ./backend/retrieve_obj.py

import asyncio
import json
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from geoalchemy2 import functions as func
from app.database import async_session
from app.models import Building
import os
from pyproj import Transformer

async def retrieve_obj_file(region_geojson, output_path):
    """
    Generates an OBJ file with buildings within the given polygon region.

    Args:
        region_geojson (dict): The GeoJSON representing the input region.
        output_path (str): The file path where the OBJ file will be saved.
    """
    # Parse the input GeoJSON to get the polygon geometry
    features = region_geojson.get('features', [])
    if not features:
        raise ValueError("No features found in the input GeoJSON.")
    
    polygon_feature = features[0]
    polygon_geometry = polygon_feature.get('geometry', {})
    if polygon_geometry.get('type') != 'Polygon':
        raise ValueError("The geometry must be of type 'Polygon'.")

    # Convert GeoJSON geometry to GeoJSON string
    polygon_geojson_str = json.dumps(polygon_geometry)

    # Choose the appropriate projection (e.g., UTM zone 32N for Germany)
    # You may need to adjust the EPSG code based on your location
    source_crs = 'EPSG:4326'  # WGS84 Latitude/Longitude
    target_crs = 'EPSG:25832'  # ETRS89 / UTM zone 32N (adjust as needed)

    # Create a Transformer object for coordinate transformation
    transformer = Transformer.from_crs(source_crs, target_crs, always_xy=True)
    
    polygon = func.ST_SetSRID(func.ST_GeomFromGeoJSON(polygon_geojson_str), 4326)

    # Start an asynchronous database session
    async with async_session() as session:
        # Query the database for buildings within the polygon
        stmt = (
            select(
                Building.gml_id,
                func.ST_AsGeoJSON(Building.geom).label('geom_geojson')
            )
            .where(
                # First, a bounding box check using the '&&' operator (leverages GIST index)
                Building.geom.op('&&')(polygon),
                # Then the exact intersection
                func.ST_Intersects(Building.geom, polygon)
            )
        )
        
        result = await session.execute(stmt)
        buildings = result.fetchall()
        
        if not buildings:
            print("No buildings found within the given region.")
            return

        # Initialize lists to store OBJ data
        obj_vertices = []
        obj_faces = []
        vertex_offset = 0  # Offset for indexing vertices in faces

        # Process each building geometry
        for building in buildings:
            gml_id = building.gml_id
            geom_geojson_str = building.geom_geojson
            if not geom_geojson_str:
                continue  # Skip if geometry is null

            # Load geometry from GeoJSON
            geom_geojson = json.loads(geom_geojson_str)

            # Handle Polygon and MultiPolygon geometries
            geom_type = geom_geojson.get('type')
            coordinates = geom_geojson.get('coordinates')

            if geom_type == 'Polygon':
                polygons = [coordinates]
            elif geom_type == 'MultiPolygon':
                polygons = coordinates
            else:
                print(f"Skipping unsupported geometry type (ID: {gml_id}, Type: {geom_type})")
                continue

            for polygon in polygons:
                ring_vertex_indices = []

                # Process exterior ring
                exterior_coords = polygon[0]
                exterior_indices = []
                for coord in exterior_coords:
                    lon, lat = coord[:2]
                    z = coord[2] if len(coord) > 2 else 0
                    # Transform coordinates
                    x, y = transformer.transform(lon, lat)
                    obj_vertices.append(f"v {x} {z} {y}")
                    vertex_offset += 1
                    exterior_indices.append(vertex_offset)
                # Add face for exterior ring
                obj_faces.append(f"f {' '.join(map(str, exterior_indices))}")

                # Process interior rings (holes)
                for interior_coords in polygon[1:]:
                    interior_indices = []
                    for coord in interior_coords:
                        lon, lat = coord[:2]
                        z = coord[2] if len(coord) > 2 else 0
                        # Transform coordinates
                        x, y = transformer.transform(lon, lat)
                        obj_vertices.append(f"v {x} {z} {y}")
                        vertex_offset += 1
                        interior_indices.append(vertex_offset)
                    # Add face for interior ring (negative indices to denote holes are not standard in OBJ)
                    # Some software may not support holes directly
                    # So, we can skip adding faces for holes or handle them as separate objects
                    # For now, we'll skip adding faces for holes
                    print(f"Skipping interior ring (hole) in building ID: {gml_id}")

        # Write to OBJ file
        with open(output_path, 'w') as obj_file:
            obj_file.write("# OBJ file generated from buildings\n")
            obj_file.write("\n".join(obj_vertices))
            obj_file.write("\n")
            obj_file.write("\n".join(obj_faces))

        print(f"OBJ file successfully written to {output_path}")
        return


&&& FILE: ./backend\db\index.sql
&&& CONTENT:
-- Create the index if it doesn't exist
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 
        FROM pg_indexes 
        WHERE schemaname = 'public' 
        AND tablename = 'building' 
        AND indexname = 'buildings_geom_idx'
    ) THEN
        CREATE INDEX buildings_geom_idx ON building USING GIST(geom);
    END IF;
END $$;

-- Add the unique constraint if it doesn't exist
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 
        FROM pg_constraint 
        WHERE conname = 'gml_id_unique' 
        AND conrelid = 'public.building'::regclass
    ) THEN
        ALTER TABLE public.building ADD CONSTRAINT gml_id_unique UNIQUE (gml_id);
    END IF;
END $$;

UPDATE building
SET geom = ST_MakeValid(geom)
WHERE NOT ST_IsValid(geom);

-- Cluster the table using the index
CLUSTER building USING buildings_geom_idx;

-- Analyze to update statistics
ANALYZE building;


&&& FILE: ./backend\db\init.sql
&&& CONTENT:
-- Part 1: Database and extension creation (in transaction)
CREATE DATABASE easyopendata_database;
\c easyopendata_database;
CREATE EXTENSION IF NOT EXISTS postgis;

-- Part 2: System settings (must be outside transaction)
\connect easyopendata_database
\echo 'Setting system parameters...'

\set ON_ERROR_STOP off
ALTER SYSTEM SET maintenance_work_mem TO '1GB';
ALTER SYSTEM SET work_mem TO '256MB';
ALTER SYSTEM SET max_parallel_workers_per_gather TO 4;
\set ON_ERROR_STOP on



&&& FILE: ./backend\db_backup\backup_script.sh
&&& CONTENT:
#!/bin/bash

BACKUP_DIR=/backups
mkdir -p $BACKUP_DIR
export PGPASSWORD="barcelona"

# Create a new backup
NEW_BACKUP=$BACKUP_DIR/backup_$(date +%Y-%m-%d_%H-%M-%S).dump
pg_dump -U postgres -h postgis -F c easyopendata_database > $NEW_BACKUP

# Check if the backup was successful
if [ $? -eq 0 ]; then
    echo "Backup created: $NEW_BACKUP"
else
    echo "Backup failed!" >&2
    exit 1
fi

# Delete old backups, keeping only the last 3
NUM_BACKUPS_TO_KEEP=3
BACKUP_COUNT=$(ls -1 $BACKUP_DIR | wc -l)

if [ $BACKUP_COUNT -gt $NUM_BACKUPS_TO_KEEP ]; then
    echo "Cleaning up old backups..."
    ls -1t $BACKUP_DIR | tail -n +$(($NUM_BACKUPS_TO_KEEP + 1)) | while read OLD_BACKUP; do
        rm -f "$BACKUP_DIR/$OLD_BACKUP"
        echo "Deleted old backup: $BACKUP_DIR/$OLD_BACKUP"
    done
fi


&&& FILE: ./backend\db_backup\Dockerfile
&&& CONTENT:
FROM postgres:17
RUN apt-get update && apt-get install -y cron && rm -rf /var/lib/apt/lists/*
COPY backup_script.sh /usr/local/bin/backup_script.sh
RUN chmod +x /usr/local/bin/backup_script.sh

# Add cron job
RUN echo "0 0 * * * root /usr/local/bin/backup_script.sh" >> /etc/crontab

CMD ["cron", "-f"]


&&& FILE: ./backend\ingestion\bayern.py
&&& CONTENT:
#!/usr/bin/env python3
"""
process_meta4.py

A script to sequentially download GML files from a Meta4 file, transform them by embedding polygons,
ingest them into a PostgreSQL database using a temporary table, convert them to 3D tiles,
append to the main building table, and remove the original files.

Usage:
    python process_meta4.py file.meta4

Requirements:
    - Python 3.x
    - lxml
    - psycopg2
    - ogr2ogr (from GDAL)
    - pg2b3dm_new command available in PATH
    - gltf-pipeline (for Draco compression)
"""

import sys
import os
import subprocess
import hashlib
import logging
import shutil
from urllib.parse import urlparse
from urllib.request import urlopen, Request
from urllib.error import URLError, HTTPError
from lxml import etree
import psycopg2

# Constants
META4_PATH = 'backend/ingestion/data_sources/bayern.meta4'
DATA_DIR = 'backend/ingestion/data_local/bayern'
DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql://postgres:barcelona@localhost:8735/easyopendata_database')
CACHE_DIR = 'data/tileset'
PG2B3DM_PATH = 'backend/ingestion/libs/pg2b3dm.exe'
SQL_INDEX_PATH = 'backend/db/index.sql'
TEMP_TABLE = 'idx_building'  # Temporary table name
MAIN_TABLE = 'building'      # Main building table name
BATCH_N = 20 # number of gml files for which there will be created a separate tileset

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

def parse_meta4(meta4_file):
    """
    Parses the Meta4 XML file and extracts file information.

    Args:
        meta4_file (str): Path to the Meta4 XML file.

    Returns:
        list of dict: List containing information about each file.
    """
    logging.info(f"Parsing Meta4 file: {meta4_file}")
    tree = etree.parse(meta4_file)
    root = tree.getroot()
    ns = {'metalink': 'urn:ietf:params:xml:ns:metalink'}

    files = []
    for file_elem in root.findall('metalink:file', namespaces=ns):
        name = file_elem.get('name')
        size = int(file_elem.find('metalink:size', namespaces=ns).text)
        hash_elem = file_elem.find('metalink:hash', namespaces=ns)
        hash_type = hash_elem.get('type')
        hash_value = hash_elem.text
        urls = [url_elem.text for url_elem in file_elem.findall('metalink:url', namespaces=ns)]
        files.append({
            'name': name,
            'size': size,
            'hash_type': hash_type,
            'hash_value': hash_value,
            'urls': urls
        })
    logging.info(f"Found {len(files)} files in Meta4.")
    return files

def download_file(url, dest_path):
    """
    Downloads a file from a URL to a destination path.

    Args:
        url (str): URL to download from.
        dest_path (str): Destination file path.

    Returns:
        bool: True if download was successful, False otherwise.
    """
    try:
        logging.info(f"Downloading from URL: {url}")
        headers = {'User-Agent': 'Mozilla/5.0'}
        req = Request(url, headers=headers)
        with urlopen(req) as response, open(dest_path, 'wb') as out_file:
            shutil.copyfileobj(response, out_file)
        logging.info(f"Downloaded file to: {dest_path}")
        return True
    except HTTPError as e:
        logging.warning(f"HTTP Error: {e.code} when downloading {url}")
    except URLError as e:
        logging.warning(f"URL Error: {e.reason} when downloading {url}")
    except Exception as e:
        logging.warning(f"Unexpected error when downloading {url}: {e}")
    return False

def verify_file(file_path, expected_size, expected_hash, hash_type='sha-256'):
    """
    Verifies the size and hash of a downloaded file.

    Args:
        file_path (str): Path to the file.
        expected_size (int): Expected file size in bytes.
        expected_hash (str): Expected hash value.
        hash_type (str): Hash algorithm, default 'sha-256'.

    Returns:
        bool: True if verification succeeds, False otherwise.
    """
    logging.info(f"Verifying file: {file_path}")
    # Check size
    actual_size = os.path.getsize(file_path)
    if actual_size != expected_size:
        logging.error(f"Size mismatch for {file_path}: expected {expected_size}, got {actual_size}")
        return False
    # Check hash
    hash_func = hashlib.new(hash_type)
    with open(file_path, 'rb') as f:
        for chunk in iter(lambda: f.read(8192), b''):
            hash_func.update(chunk)
    actual_hash = hash_func.hexdigest()
    if actual_hash.lower() != expected_hash.lower():
        logging.error(f"Hash mismatch for {file_path}: expected {expected_hash}, got {actual_hash}")
        return False
    logging.info(f"Verification passed for {file_path}")
    return True

def get_all_namespaces(gml_tree):
    """
    Extracts all namespaces from the GML tree and assigns a unique prefix to the default namespace.

    Args:
        gml_tree (etree.ElementTree): Parsed GML tree.

    Returns:
        dict: Namespace prefix to URI mapping.
    """
    nsmap = gml_tree.getroot().nsmap.copy()
    # Handle default namespace (None key)
    if None in nsmap:
        nsmap['default'] = nsmap.pop(None)
    # Ensure 'xlink' is included
    if 'xlink' not in nsmap:
        # Attempt to find the xlink namespace
        for prefix, uri in nsmap.items():
            if uri == 'http://www.w3.org/1999/xlink':
                nsmap['xlink'] = uri
                break
        else:
            # If not found, add it manually
            nsmap['xlink'] = 'http://www.w3.org/1999/xlink'
    return nsmap

def transform_gml(input_file, output_file):
    """
    Transforms the input GML file by embedding polygons into surfaceMember elements.

    Args:
        input_file (str): Path to the input GML file.
        output_file (str): Path to the output transformed GML file.
    """
    # Parse the GML file
    logging.info(f"Parsing input GML file: {input_file}")
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(input_file, parser)
    root = tree.getroot()

    # Extract all namespaces
    namespaces = get_all_namespaces(tree)
    # logging.debug("Namespaces detected:")
    # for prefix, uri in namespaces.items():
    #     logging.debug(f"  Prefix: '{prefix}' => URI: '{uri}'")

    # Build a dictionary of gml:id to Polygon elements for quick lookup
    logging.info("Indexing all <gml:Polygon> elements by gml:id...")
    polygon_dict = {}
    for polygon in root.xpath('.//gml:Polygon', namespaces=namespaces):
        polygon_id = polygon.get('{http://www.opengis.net/gml}id')
        if polygon_id:
            polygon_dict[polygon_id] = polygon
    logging.info(f"Indexed {len(polygon_dict)} polygons.")

    # Find all <gml:surfaceMember> elements with xlink:href
    logging.info("Finding all <gml:surfaceMember> elements with xlink:href...")
    surface_members = root.xpath('.//gml:surfaceMember[@xlink:href]', namespaces=namespaces)
    logging.info(f"Found {len(surface_members)} <gml:surfaceMember> elements with xlink:href.")

    for sm in surface_members:
        href = sm.get('{http://www.w3.org/1999/xlink}href')
        if not href:
            continue
        # Extract the referenced polygon ID (remove the '#' prefix)
        polygon_id = href.lstrip('#')
        # logging.debug(f"Processing surfaceMember referencing Polygon ID: {polygon_id}")
        polygon = polygon_dict.get(polygon_id)
        if not polygon:
            logging.warning(f"Polygon with gml:id='{polygon_id}' not found. Skipping.")
            continue
        # Deep copy the polygon element
        polygon_copy = etree.fromstring(etree.tostring(polygon))
        # Remove any existing 'gml:id' to avoid duplicate IDs
        polygon_copy.attrib.pop('{http://www.opengis.net/gml}id', None)
        # Replace the surfaceMember's xlink:href attribute with the actual Polygon
        sm.clear()  # Remove existing children and attributes
        sm.append(polygon_copy)
        # logging.debug(f"Embedded Polygon ID: {polygon_id} into surfaceMember.")

    # Optionally, remove standalone <gml:Polygon> elements that were referenced
    # logging.info("Removing standalone <gml:Polygon> elements that were referenced...")
    # removed_count = 0
    # for polygon_id in polygon_dict.keys():
    #     # Find and remove the standalone polygon
    #     polygons_to_remove = root.xpath(f'.//gml:Polygon[@gml:id="{polygon_id}"]', namespaces=namespaces)
    #     for polygon in polygons_to_remove:
    #         parent = polygon.getparent()
    #         if parent is not None:
    #             parent.remove(polygon)
    #             removed_count += 1
    #             logging.debug(f"Removed standalone Polygon ID: {polygon_id}.")
    # logging.info(f"Removed {removed_count} standalone polygons.")

    # Write the transformed GML to the output file
    logging.info(f"Writing transformed GML to: {output_file}")
    tree.write(output_file, pretty_print=True, xml_declaration=True, encoding='UTF-8')
    logging.info("Transformation complete.")

def ingest_gml_file(gml_file, database_url, table_name):
    """
    Ingests a GML file into a PostgreSQL database using ogr2ogr into a specified table.

    Args:
        gml_file (str): Path to the GML file.
        database_url (str): PostgreSQL connection URL.
        table_name (str): Target table name for ingestion.
    """
    logging.info(f"Ingesting GML file into database table '{table_name}': {gml_file}")
    cmd = [
        'ogr2ogr',
        '-f', 'PostgreSQL',
        '-nln', table_name,              # Specify the target table name
        '-progress',
        '-lco', 'GEOMETRY_NAME=geom',
        '-skipfailures',
        '-nlt', 'MULTIPOLYGONZ',
        '-dim', 'XYZ',
        '-s_srs', 'EPSG:25832',
        '-t_srs', 'EPSG:4326',
        database_url,
        gml_file
    ]
    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    if result.returncode != 0:
        logging.error(f"ogr2ogr failed for {gml_file}: {result.stderr}")
        raise RuntimeError(f"ogr2ogr failed: {result.stderr}")
    logging.info(f"Ingested {gml_file} into table '{table_name}' successfully.")

def execute_sql_file(sql_file_path, database_url):
    """Executes a SQL file in the database."""
    logging.info(f"Executing SQL file: {sql_file_path}")
    url = urlparse(database_url)
    conn = psycopg2.connect(
        dbname=url.path[1:],
        user=url.username,
        password=url.password,
        host=url.hostname,
        port=url.port
    )
    try:
        with conn.cursor() as cur:
            with open(sql_file_path, 'r') as f:
                cur.execute(f.read())
            conn.commit()
        logging.info("SQL file executed successfully")
    except Exception as e:
        logging.error(f"Failed to execute SQL file: {e}")
        conn.rollback()
        raise
    finally:
        conn.close()

def update_geometries(database_url, table_name):
    """
    Updates the geometries in the specified table to put them on ground level.

    Args:
        database_url (str): PostgreSQL connection URL.
        table_name (str): Table to update.
    """
    logging.info(f"Updating geometries to ground level in table '{table_name}'.")
    url = urlparse(database_url)
    conn = psycopg2.connect(
        dbname=url.path[1:],
        user=url.username,
        password=url.password,
        host=url.hostname,
        port=url.port
    )
    try:
        with conn.cursor() as cur:
            cur.execute(f"""
                UPDATE {table_name}
                SET geom = ST_Translate(geom, 0, 0, -ST_ZMin(geom))
                WHERE ST_ZMin(geom) != 0;
            """)
            conn.commit()
        logging.info(f"Geometries in table '{table_name}' updated successfully.")
    except Exception as e:
        logging.error(f"Failed to update geometries in table '{table_name}': {e}")
        conn.rollback()
        raise
    finally:
        conn.close()

def convert_to_3d_tiles(cache_dir, database_url, table_name):
    """
    Converts buildings from the specified table in the database to 3D tiles using pg2b3dm.

    Args:
        cache_dir (str): Output directory for 3D tiles.
        database_url (str): PostgreSQL connection URL.
        table_name (str): Table to convert to 3D tiles.
    """
    logging.info(f"Converting table '{table_name}' to 3D tiles with pg2b3dm.")
    # Parse the database URL for parameters
    url = urlparse(database_url)
    dbname = url.path[1:]
    user = url.username
    host = url.hostname or 'localhost'
    port = url.port
    # Assume password is handled via environment or .pgpass
    cmd = [
        PG2B3DM_PATH,
        '-h', f"{host}:{port}",
        '-U', user,
        '-c', 'geom',
        '-t', table_name,
        '-d', dbname,
        '-o', cache_dir, 
         '--use_implicit_tiling', 'false'  # Uncomment if needed
    ]
    # To handle password, set PGPASSWORD environment variable if available
    env = os.environ.copy()
    if url.password:
        env['PGPASSWORD'] = url.password
    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, env=env)
    if result.returncode != 0:
        logging.error(f"pg2b3dm failed: {result.stderr}")
        raise RuntimeError(f"pg2b3dm failed: {result.stderr}")
    logging.info("3D tiles generated successfully.")

def apply_draco_compression(cache_dir):
    """
    Applies Draco compression to all .glb files in the specified directory.

    Args:
        cache_dir (str): Directory containing .glb files.
    """
    logging.info("Applying Draco compression to glTF files.")
    for root, dirs, files in os.walk(cache_dir):
        for file in files:
            if file.endswith('.glb'):
                gltf_file = os.path.join(root, file)

                # Check if the first line of the file contains "draco"
                try:
                    with open(gltf_file, 'rb') as f:
                        first_line = f.readline().decode('utf-8', errors='ignore')
                        if "draco" in first_line.lower():
                            logging.info(f"File {gltf_file} already contains Draco; skipping compression.")
                            continue
                except Exception as e:
                    logging.error(f"Error reading file {gltf_file}: {e}")
                    continue

                # Proceed with Draco compression
                compressed_file = os.path.join(root, f"{os.path.splitext(file)[0]}_draco.glb")
                cmd = [
                    "gltf-pipeline",
                    '-i', gltf_file,
                    '-o', compressed_file,
                    '--draco.compressionLevel', '7'
                ]
                print(" ".join(cmd))
                result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                if result.returncode != 0:
                    logging.error(f"Draco compression failed for {gltf_file}: {result.stderr}")
                else:
                    os.replace(compressed_file, gltf_file)
                    logging.info(f"Applied Draco compression to {gltf_file}")
                    
def append_temp_to_main(database_url, temp_table, main_table):
    """
    Appends data from the temporary table to the main table by copying all columns.
    If the main table does not have certain columns, they will be created.
    Handles duplicates by ignoring records that violate primary key constraints.

    Args:
        database_url (str): PostgreSQL connection URL.
        temp_table (str): Temporary table name.
        main_table (str): Main table name.
    """
    logging.info(f"Appending data from '{temp_table}' to '{main_table}'.")
    url = urlparse(database_url)
    conn = psycopg2.connect(
        dbname=url.path[1:],
        user=url.username,
        password=url.password,
        host=url.hostname,
        port=url.port
    )

    try:
        with conn.cursor() as cur:
            # Fetch main table columns
            cur.execute(f"""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_name = '{main_table}'
                ORDER BY ordinal_position;
            """)
            main_columns = [row[0] for row in cur.fetchall()]

            # Fetch temp table columns and their data types
            cur.execute(f"""
                SELECT column_name, data_type
                FROM information_schema.columns
                WHERE table_name = '{temp_table}'
                ORDER BY ordinal_position;
            """)
            temp_columns_info = cur.fetchall()
            temp_columns = [row[0] for row in temp_columns_info]

            # Add missing columns to main_table
            for col_name, data_type in temp_columns_info:
                if col_name not in main_columns:
                    logging.info(f"Column '{col_name}' does not exist in '{main_table}'. Adding it.")
                    # Add the column with the same data_type as in temp_table
                    # Note: For complex types or special columns, you may need a more robust mapping.
                    alter_sql = f'ALTER TABLE "{main_table}" ADD COLUMN "{col_name}" {data_type};'
                    cur.execute(alter_sql)
                    main_columns.append(col_name)
                    logging.info(f"Column '{col_name}' added to '{main_table}'.")

            # Now all temp_columns should exist in main_table
            # We will insert all columns from temp_table to main_table
            columns_str = ', '.join([f'"{col}"' for col in temp_columns])

            # Fetch primary key columns from the main table
            cur.execute(f"""
                SELECT a.attname
                FROM pg_index i
                JOIN pg_attribute a ON a.attrelid = i.indrelid
                                     AND a.attnum = ANY(i.indkey)
                WHERE i.indrelid = '{main_table}'::regclass
                  AND i.indisprimary;
            """)
            pk_columns = [row[0] for row in cur.fetchall()]
            if not pk_columns:
                raise ValueError(f"No primary key defined for table '{main_table}'.")

            # Construct the ON CONFLICT clause
            conflict_target = ', '.join([f'"{col}"' for col in pk_columns])
            on_conflict_clause = f"ON CONFLICT ({conflict_target}) DO NOTHING"

            logging.info(f"Using ON CONFLICT clause on columns: {conflict_target}")

            # Execute the INSERT statement with ON CONFLICT
            insert_sql = f"""
                INSERT INTO "{main_table}" ({columns_str})
                SELECT {columns_str} FROM "{temp_table}"
                {on_conflict_clause};
            """
            cur.execute(insert_sql)

            inserted_count = cur.rowcount
            conn.commit()
            logging.info(f"Data appended from '{temp_table}' to '{main_table}' successfully. Inserted {inserted_count} records.")

    except Exception as e:
        logging.error(f"Failed to append data from '{temp_table}' to '{main_table}': {e}")
        conn.rollback()
        raise
    finally:
        conn.close()



def drop_temp_table(database_url, temp_table):
    """
    Drops the temporary table from the database.

    Args:
        database_url (str): PostgreSQL connection URL.
        temp_table (str): Temporary table name.
    """
    logging.info(f"Dropping temporary table '{temp_table}'.")
    url = urlparse(database_url)
    conn = psycopg2.connect(
        dbname=url.path[1:],
        user=url.username,
        password=url.password,
        host=url.hostname,
        port=url.port
    )
    try:
        with conn.cursor() as cur:
            cur.execute(f"DROP TABLE IF EXISTS {temp_table};")
            conn.commit()
        logging.info(f"Temporary table '{temp_table}' dropped successfully.")
    except Exception as e:
        logging.error(f"Failed to drop temporary table '{temp_table}': {e}")
        conn.rollback()
        raise
    finally:
        conn.close()

def remove_file(file_path):
    """
    Removes a file from the filesystem.

    Args:
        file_path (str): Path to the file.
    """
    try:
        os.remove(file_path)
        logging.info(f"Removed file: {file_path}")
    except OSError as e:
        logging.warning(f"Failed to remove file {file_path}: {e}")

import os
import json
import math

def merge_tilesets_into_one(output_path, input_tilesets):
    """
    Merges multiple region-based tilesets into a single tileset.json that references all of them as external.
    If some tilesets do not exist or do not have a region boundingVolume, they are skipped.
    If no valid tilesets remain, creates a minimal tileset with no children.
    
    Args:
        output_path (str): Path to the final merged tileset.json output file.
        input_tilesets (list[str]): Paths to the input tileset.json files to merge.

    Returns:
        None. Writes the merged tileset.json to output_path.
    """
    
    # Load all valid tilesets
    loaded_tilesets = []
    for ts_path in input_tilesets:
        if not os.path.isfile(ts_path):
            # Skip if the file doesn't exist
            continue
        try:
            with open(ts_path, 'r', encoding='utf-8') as f:
                ts = json.load(f)
                loaded_tilesets.append((ts_path, ts))
        except (IOError, json.JSONDecodeError):
            # Skip if the file cannot be read or is not valid JSON
            continue
    
    # Filter down to only those with a region boundingVolume
    all_regions = []
    valid_tilesets = []
    for ts_path, ts in loaded_tilesets:
        root = ts.get("root", {})
        bv = root.get("boundingVolume", {})
        region = bv.get("region")
        
        if region and isinstance(region, list) and len(region) == 6:
            all_regions.append(region)
            valid_tilesets.append((ts_path, ts))
        # If there's no valid region, skip this tileset
    
    if not valid_tilesets:
        # No valid tilesets found, create an empty tileset
        # with a minimal boundingVolume and no children.
        # We'll use a generic region that covers no area.
        # For example, we can pick a degenerate region:
        degenerate_region = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        empty_tileset = {
            "asset": {
                "version": "1.1"
            },
            "geometricError": 0,
            "root": {
                "boundingVolume": {
                    "region": degenerate_region
                },
                "refine": "ADD",
                "geometricError": 0,
                "children": []
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(empty_tileset, f, indent=2)
        print(f"No valid tilesets found. Created an empty merged tileset at {output_path}")
        return
    
    # Compute the encompassing region for all valid tilesets
    west = min(r[0] for r in all_regions)
    south = min(r[1] for r in all_regions)
    east = max(r[2] for r in all_regions)
    north = max(r[3] for r in all_regions)
    minH = min(r[4] for r in all_regions)
    maxH = max(r[5] for r in all_regions)
    merged_region = [west, south, east, north, minH, maxH]

    # Construct children for the merged tileset
    children = []
    output_dir = os.path.dirname(os.path.abspath(output_path))
    for ts_path, ts in valid_tilesets:
        ts_abs = os.path.abspath(ts_path)
        rel_path = os.path.relpath(ts_abs, output_dir)
        
        child = {
            "boundingVolume": ts["root"]["boundingVolume"],
            "geometricError": ts["root"]["geometricError"],
            "refine": ts["root"].get("refine", "ADD").upper(),
            "content": {
                "uri": rel_path
            }
        }
        children.append(child)

    # Determine the maximum geometricError for the parent tileset
    parent_geometric_error = max(ts["root"]["geometricError"] for _, ts in valid_tilesets)
    
    # Create the merged tileset JSON structure
    merged_tileset = { 
        "asset": {
            "version": "1.1"
        },
        "geometricError": parent_geometric_error,
        "root": {
            "boundingVolume": {
                "region": merged_region
            },
            "refine": "ADD",
            "geometricError": parent_geometric_error,
            "children": children
        }
    }

    # Write the merged tileset to disk
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(merged_tileset, f, indent=2)
    print(f"Merged tileset written to {output_path}")


def main(meta4_file):
    # Ensure DATA_DIR and CACHE_DIR exist
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(CACHE_DIR, exist_ok=True)
    # Parse the Meta4 file
    files = parse_meta4(meta4_file)

    # Drop the temporary table
    drop_temp_table(DATABASE_URL, TEMP_TABLE)

    for ix, file_info in enumerate(files):
        
        # if ix < 130 * BATCH_N:
        #     continue
        
        file_name = file_info['name']
        size = file_info['size']
        hash_type = file_info['hash_type']
        hash_value = file_info['hash_value']
        urls = file_info['urls']

        logging.info(f"â¶ï¸   FILE {ix+1}/{len(files)}")
        logging.info(f"Processing file: {file_name}")


        temp_tileset_dir = os.path.join(CACHE_DIR, 'sub', str(ix // BATCH_N))  
        main_tileset_dir = CACHE_DIR
           
        os.makedirs(main_tileset_dir, exist_ok=True)   
        os.makedirs(temp_tileset_dir, exist_ok=True)

        # Determine download paths
        download_path = os.path.join(DATA_DIR, file_name)
        transformed_file_name = os.path.splitext(file_name)[0] + '_trs.gml'
        transformed_path = os.path.join(DATA_DIR, transformed_file_name)

        # Download the file from available URLs
        downloaded = False
        for url in urls:
            if download_file(url, download_path):
                # Verify the file
                if verify_file(download_path, size, hash_value, hash_type):
                    downloaded = True
                    break
                else:
                    logging.warning(f"Verification failed for {download_path}. Trying next URL.")
                    remove_file(download_path)
        if not downloaded:
            logging.error(f"Failed to download and verify {file_name} from all URLs. Skipping.")
            continue

        try:
            # Transform the GML file
            transform_gml(download_path, transformed_path)

            # Ingest the transformed GML into the temporary table
            ingest_gml_file(transformed_path, DATABASE_URL, TEMP_TABLE)

            # Update geometries in the temporary table
            update_geometries(DATABASE_URL, TEMP_TABLE)



            if ix % BATCH_N == 0 or ix == len(files) - 1:            
                # Convert the temporary table to 3D tiles
                convert_to_3d_tiles(temp_tileset_dir, DATABASE_URL, TEMP_TABLE)

                # Apply Draco compression to the newly generated tiles
                apply_draco_compression(temp_tileset_dir)

                # Append data from temporary table to main table
                append_temp_to_main(DATABASE_URL, TEMP_TABLE, MAIN_TABLE)

                # Drop the temporary table
                drop_temp_table(DATABASE_URL, TEMP_TABLE)

                batch_count = ix // BATCH_N + 1

                # Collect all input tileset.json paths
                input_tileset_paths = [
                    os.path.join(CACHE_DIR, 'sub', str(b), 'tileset.json')
                    for b in range(batch_count)
                ]

                merged_tileset_path = os.path.join(CACHE_DIR, 'tileset.json')

                logging.info(f"Merging {len(input_tileset_paths)} tilesets into {merged_tileset_path}...")

                try:
                    # Call our custom merging function
                    merge_tilesets_into_one(merged_tileset_path, input_tileset_paths)
                    logging.info("Merged tileset into main tileset successfully.")
                except Exception as e:
                    logging.error(f"Failed to combine merged tilesets: {e}")
                    raise RuntimeError(f"Failed to combine merged tilesets: {e}")

            # npx 3d-tiles-tools combine -i backend/tileset\ -o backend/tileset_combined -f

            if ix == 0:
                # Execute SQL indexing file once before processing
                execute_sql_file(SQL_INDEX_PATH, DATABASE_URL)
                
            # Remove the transformed GML file
            remove_file(transformed_path)
            # Remove the transformed GFS file if it exists
            transformed_gfs_path = os.path.splitext(transformed_path)[0] + ".gfs"
            if os.path.isfile(transformed_gfs_path):
                remove_file(transformed_gfs_path)

            # Optionally, remove the original downloaded GML file
            remove_file(download_path)

            logging.info(f"â  Completed processing for {file_name}")

        except Exception as e:
            logging.error(f"An error occurred while processing {file_name}: {e}")
            # Optionally, clean up files or continue
            continue

    logging.info("ð All files processed.")

if __name__ == '__main__':
    meta4_file = META4_PATH
    if not os.path.isfile(meta4_file):
        logging.error(f"Meta4 file '{meta4_file}' does not exist.")
        sys.exit(1)
    main(meta4_file)


&&& FILE: ./backend\ingestion\README.md
&&& CONTENT:
# EasyOpenData Data Ingestion

## Overview

The `ingestion` directory contains scripts and resources for downloading, transforming, and ingesting 3D building data into a PostGIS database. It also includes tools for generating and optimizing 3D tilesets from the database.

---

## Setup Instructions

### Prerequisites

- **Conda** (optional, for managing Python environments)
- **GDAL** and **OGR** libraries
- **Python 3.x**
- **Node.js** and **npm** (for `gltf-pipeline` and other utilities)
- **PostgreSQL with PostGIS extension**

---

### Steps

#### 1. Create a Conda Environment (Optional)

```bash
conda create -n easyopendata_env python=3.10
conda activate easyopendata_env
```

#### 2. Install GDAL and Libraries

```bash
conda install -c conda-forge gdal libgdal
```

#### 3. Install Python Dependencies

```bash
pip install -r ../../requirements.txt
```

#### 4. Install gltf-pipeline

```bash
npm install -g gltf-pipeline
```

#### 5. Set Up PostgreSQL Database

Ensure you have a PostgreSQL instance running with the PostGIS extension installed. Update the `DATABASE_URL` in the script to point to your database.

#### 6. Prepare Required Executables

- Place the `pg2b3dm` executable in the `libs/` directory.
- Ensure `ogr2ogr` is available in your system's `PATH`.

---

## Data Ingestion Script

### `process_meta4.py`

This script sequentially downloads GML files from a `.meta4` file, transforms them by embedding polygons into `surfaceMember` elements, ingests them into a PostgreSQL database using a temporary table, converts the data into 3D tiles, and merges them into a single tileset.

#### Workflow

1. **Parse Meta4**: Extract file metadata (URLs, hashes, etc.).
2. **Download**: Fetch files and verify size and hash integrity.
3. **Transform GML**: Embed referenced polygons into the GML structure.
4. **Ingest**: Load transformed data into a temporary PostgreSQL table.
5. **Update Geometries**: Align geometries to the ground level.
6. **Generate 3D Tiles**: Use `pg2b3dm` to create tilesets.
7. **Optimize with Draco Compression**: Compress `.glb` files to reduce size.
8. **Merge Tilesets**: Combine batch tilesets into a single `tileset.json`.
9. **Append Data**: Append data from the temporary table to the main table.
10. **Clean Up**: Remove processed files and drop temporary tables.

---

### Usage

```bash
python process_meta4.py file.meta4
```

#### Configuration Variables

- **`META4_PATH`**: Path to the `.meta4` file.
- **`DATABASE_URL`**: PostgreSQL connection string.
- **`CACHE_DIR`**: Directory for storing tileset outputs.
- **`DATA_DIR`**: Directory for storing temporary data files.
- **`TEMP_TABLE`**: Temporary table name.
- **`MAIN_TABLE`**: Main table name.

---

## Directory Structure

- **`data_sources/`**: Contains `.meta4` files listing URLs for GML data.
- **`data_local/`**: Directory for downloaded GML files.
- **`libs/`**: Contains utilities like `pg2b3dm`.
- **`tileset/`**: Directory for generated tilesets.

---

## Tools and Dependencies

- **GDAL/OGR**: Used for transforming and ingesting GML files.
- **PostGIS**: PostgreSQL extension for spatial data management.
- **pg2b3dm**: Converts PostGIS data to 3D tiles compatible with CesiumJS.
- **gltf-pipeline**: Optimizes and compresses 3D tiles with Draco compression.

---

## Performance Tips

- **Batch Size**: The `BATCH_N` parameter controls the number of GML files processed per batch. Adjust it based on available memory and performance needs.
- **Indexes**: Ensure database indexes are optimized for spatial operations by executing the provided `index.sql`.

---

## Troubleshooting
 
- **Failed Downloads**: Ensure URLs in the `.meta4` file are valid and reachable.
- **Database Errors**: Check that the database URL and permissions are correctly configured.
- **Script Errors**: Refer to the detailed logs for troubleshooting.


&&& FILE: ./backend\ingestion\data_sources\bamberg.meta4
&&& CONTENT:
<?xml version="1.0" encoding="UTF-8"?>
<metalink xmlns="urn:ietf:params:xml:ns:metalink">
  <generator>BVV-MetaLinker</generator>
  <published>2024-11-26T22:13:07Z</published>
  <file name="636_5524.g
................................

&&& FILE: ./backend\ingestion\data_sources\bayern.meta4
&&& CONTENT:
<?xml version="1.0" encoding="UTF-8"?>
<metalink xmlns="urn:ietf:params:xml:ns:metalink">
  <generator>BVV-MetaLinker</generator>
  <published>2024-11-19T22:09:01Z</published>
  <file name="792_5432.g
................................

&&& FILE: ./backend\ingestion\data_sources\munchen.meta4
&&& CONTENT:
<?xml version="1.0" encoding="UTF-8"?>
<metalink xmlns="urn:ietf:params:xml:ns:metalink">
  <generator>BVV-MetaLinker</generator>
  <published>2024-11-19T22:08:58Z</published>
  <file name="680_5342.g
................................

&&& FILE: ./backend\ingestion\libs\pg2b3dm.exe
&&& ERROR: Could not read file: 'utf-8' codec can't decode byte 0x90 in position 2: invalid start byte

&&& FILE: ./frontend\.dockerignore
&&& CONTENT:
node_modules


&&& FILE: ./frontend\.env
&&& CONTENT:
HTTPS=true

&&& FILE: ./frontend\.gitignore
&&& CONTENT:
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*

node_modules
dist
dist-ssr
*.local
.vite/*

# Editor directories and files
.vscode/*
!.vscode/extensions.json
.idea
.DS_Store
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

yarn.lock

&&& FILE: ./frontend\Dockerfile
&&& CONTENT:
FROM node:18-alpine

WORKDIR /app

# Copy package.json and yarn.lock first for dependency installation
COPY package.json ./

# Install dependencies
RUN yarn install 

# Copy the rest of the frontend code
COPY . .

EXPOSE 5173

CMD ["yarn", "dev", "--host", "0.0.0.0"]

&&& FILE: ./frontend\eslint.config.js
&&& CONTENT:
import js from '@eslint/js'
import globals from 'globals'
import react from 'eslint-plugin-react'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'

export default [
  { ignores: ['dist'] },
  {
    files: ['**/*.{js,jsx}'],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
      parserOptions: {
        ecmaVersion: 'latest',
        ecmaFeatures: { jsx: true },
        sourceType: 'module',
      },
    },
    settings: { react: { version: '18.3' } },
    plugins: {
      react,
      'react-hooks': reactHooks,
      'react-refresh': reactRefresh,
    },
    rules: {
      ...js.configs.recommended.rules,
      ...react.configs.recommended.rules,
      ...react.configs['jsx-runtime'].rules,
      ...reactHooks.configs.recommended.rules,
      'react/jsx-no-target-blank': 'off',
      'react-refresh/only-export-components': [
        'warn',
        { allowConstantExport: true },
      ],
    },
  },
]


&&& FILE: ./frontend\index.html
&&& CONTENT:
<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>EasyOpenData</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://api.mapbox.com/mapbox-gl-js/v2.6.1/mapbox-gl.css" rel="stylesheet">
  <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/plugins/mapbox-gl-draw/v1.3.0/mapbox-gl-draw.css" type="text/css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
</head>

<body>
  <div id="root"></div>
  <script type="module" src="./src/index.tsx"></script>
</body>

</html>


&&& FILE: ./frontend\package.json
&&& CONTENT:
{
  "name": "frontend",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "lint": "eslint .",
    "preview": "vite preview"

................................

&&& FILE: ./frontend\README.md
&&& CONTENT:
# EasyOpenData Frontend

## Overview

The frontend of EasyOpenData is a React application that provides an interactive map interface for users to select areas of interest, choose data layers, and download spatial data. It integrates with the backend API and handles payment processing via Stripe.

---

## Key Features

- **Interactive Map Interface**: Built with React and MapLibre GL JS for map rendering.
- **Drawing Tools**: Allows users to draw polygons to select regions.
- **3D Visualization**: Uses Deck.gl to render 3D buildings.
- **Search Functionality**: Integrated with Nominatim for location search.
- **Payment Processing**: Secure payments via Stripe.
- **Responsive Design**: Adapts to mobile and desktop screens.

---

## Setup Instructions

### Prerequisites

- **Node.js** and **npm** or **yarn**

### Installation

1. **Install Dependencies**

```bash
cd frontend
yarn install
# or
npm install
```

### Running the Development Server

```bash
yarn dev
# or
npm run dev
```

The application will be available at [http://localhost:5173](http://localhost:5173)

### Build for Production

```bash
yarn build
# or
npm run build
```

The production build will be in the `dist/` directory.

---

## Configuration

### Environment Variables

Create a `.env` file in the `frontend` directory to set environment variables.

- **VITE_BACKEND_URL**: The URL of the backend API.

Example `.env` file:

```
VITE_BACKEND_URL=http://localhost:5400
```

### Map Style

The map uses a custom style defined in `src/basemap.json`. You can customize the map style by modifying this file or replacing it with another style.

---

## Project Structure

- **src/**: Contains the source code of the React application.
  - `App.tsx`: Main application component.
  - `CheckoutForm.tsx`: Handles payment form and Stripe integration.
  - `FloatingPanel.tsx`: UI component for user interactions.
  - `Legals.tsx`: Displays legal documents.
  - `draw-control.ts`: Custom control for drawing polygons.
  - `basemap.json`: Custom map style.

- **public/**: Contains static assets.

---

## Key Components

- **Map Integration**: Uses MapLibre GL JS for map rendering and Mapbox Draw for drawing tools.
- **3D Rendering**: Deck.gl is used to render 3D buildings.
- **Payment Integration**: Stripe is integrated using `@stripe/react-stripe-js` and `@stripe/stripe-js`.
- **Legal Documents**: Legal documents are displayed using `ReactMarkdown`.

---

## Customization

- **Map Style**: Modify `src/basemap.json` to change the map appearance.
- **API Endpoints**: Ensure `VITE_BACKEND_URL` points to the correct backend API.
- **Stripe Keys**: Update the publishable key in `FloatingPanel.tsx` and ensure the backend has the correct secret key.

---

## Legal Documents

The `docs/` directory contains Markdown files for legal documents such as:

- `agb.md`: Terms and Conditions
- `datenschutz.md`: Privacy Policy
- `impressum.md`: Imprint
- `widerruf.md`: Cancellation Policy

These documents are displayed in the application using the `Legals.tsx` component.

---

## Contributing

Contributions are welcome! Please ensure any changes to the frontend code are thoroughly tested.

---

## License

This project is licensed under the MIT License.

&&& FILE: ./frontend\tsconfig.json
&&& CONTENT:
{
    "compilerOptions": {
      "target": "es2020",
      "jsx": "react",
      "moduleResolution": "node",
      "allowSyntheticDefaultImports": true
    }
  }
................................

&&& FILE: ./frontend\vite.config.js
&&& CONTENT:
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vite.dev/config/
export default defineConfig({
  server: {
    watch: {
      usePolling: true, // For Docker environments
    },
  },
  plugins: [react()],
})


&&& FILE: ./frontend\yarn.lock
&&& CONTENT:
# THIS IS AN AUTOGENERATED FILE. DO NOT EDIT THIS FILE DIRECTLY.
# yarn lockfile v1


"@ampproject/remapping@^2.2.0":
  version "2.3.0"
  resolved "https://registry.npmjs.org/@ampproject/remapping/-/r
................................

&&& FILE: ./frontend\docs\agb.md
&&& CONTENT:
# Allgemeine GeschÃ¤ftsbedingungen

## 1. Geltungsbereich

Diese Allgemeinen GeschÃ¤ftsbedingungen gelten fÃ¼r alle Bestellungen, die Kunden Ã¼ber unsere Website [Ihre Website-URL] tÃ¤tigen.

## 2. Vertragsschluss

Der Vertrag kommt zustande, wenn wir Ihre Bestellung durch eine AuftragsbestÃ¤tigung per E-Mail unmittelbar nach dem Erhalt Ihrer Bestellung annehmen.

## 3. Preise und Zahlungsbedingungen

Alle Preise sind Endpreise und enthalten die gesetzliche Mehrwertsteuer. Die Zahlung erfolgt per [Ihre Zahlungsmethoden, z.B. Kreditkarte, PayPal].

## 4. Lieferung und Downloadbereitstellung

Die bestellten Daten werden nach Zahlungseingang zum Download bereitgestellt oder per E-Mail zugesandt.

## 5. Widerrufsrecht

Sie haben das Recht, binnen vierzehn Tagen ohne Angabe von GrÃ¼nden diesen Vertrag zu widerrufen. Die Widerrufsfrist betrÃ¤gt vierzehn Tage ab dem Tag des Vertragsabschlusses.

## 6. Haftungsausschluss

Wir Ã¼bernehmen keine Haftung fÃ¼r die Richtigkeit, VollstÃ¤ndigkeit und AktualitÃ¤t der bereitgestellten Daten.

## 7. Urheberrecht

Die heruntergeladenen Daten dÃ¼rfen nur fÃ¼r den persÃ¶nlichen Gebrauch verwendet werden. Eine Weitergabe an Dritte ist nicht gestattet.

## 8. Datenschutz

Wir verarbeiten Ihre personenbezogenen Daten gemÃ¤Ã unserer DatenschutzerklÃ¤rung.

## 9. Schlussbestimmungen

Es gilt das Recht der Bundesrepublik Deutschland. Gerichtsstand ist [Ihr Gerichtsstand].

&&& FILE: ./frontend\docs\datenschutz.md
&&& CONTENT:
# DatenschutzerklÃ¤rung

Stand: [Aktuelles Datum]

## 1. Datenschutz auf einen Blick

### Allgemeine Hinweise
Die folgenden Hinweise geben einen einfachen Ãberblick darÃ¼ber, was mit Ihren personenbezogenen Daten passiert, wenn Sie diese Website besuchen.

### Datenerfassung auf dieser Website

#### Wer ist verantwortlich fÃ¼r die Datenerfassung auf dieser Website?
Die Datenverarbeitung auf dieser Website erfolgt durch den Websitebetreiber. Dessen Kontaktdaten kÃ¶nnen Sie dem Impressum dieser Website entnehmen.

#### Wie erfassen wir Ihre Daten?
Ihre Daten werden zum einen dadurch erhoben, dass Sie uns diese mitteilen. Hierbei kann es sich z.B. um Daten handeln, die Sie in ein Kontaktformular eingeben.

Andere Daten werden automatisch beim Besuch der Website durch unsere IT-Systeme erfasst. Das sind vor allem technische Daten (z.B. Internetbrowser, Betriebssystem oder Uhrzeit des Seitenaufrufs).

#### WofÃ¼r nutzen wir Ihre Daten?
Ein Teil der Daten wird erhoben, um eine fehlerfreie Bereitstellung der Website zu gewÃ¤hrleisten. Andere Daten kÃ¶nnen zur Analyse Ihres Nutzerverhaltens verwendet werden.

#### Welche Rechte haben Sie bezÃ¼glich Ihrer Daten?
Sie haben jederzeit das Recht unentgeltlich Auskunft Ã¼ber Herkunft, EmpfÃ¤nger und Zweck Ihrer gespeicherten personenbezogenen Daten zu erhalten. Sie haben auÃerdem ein Recht, die Berichtigung, Sperrung oder LÃ¶schung dieser Daten zu verlangen. Hierzu sowie zu weiteren Fragen zum Thema Datenschutz kÃ¶nnen Sie sich jederzeit unter der im Impressum angegebenen Adresse an uns wenden.

## 2. Allgemeine Hinweise und Pflichtinformationen

### Datenschutz
Die Betreiber dieser Seiten nehmen den Schutz Ihrer persÃ¶nlichen Daten sehr ernst. Wir behandeln Ihre personenbezogenen Daten vertraulich und entsprechend der gesetzlichen Datenschutzvorschriften sowie dieser DatenschutzerklÃ¤rung.

### Hinweis zur verantwortlichen Stelle
Die verantwortliche Stelle fÃ¼r die Datenverarbeitung auf dieser Website ist:

[Ihr Name]
[Ihre StraÃe und Hausnummer]
[Ihre Postleitzahl und Stadt]

Telefon: [Ihre Telefonnummer]
E-Mail: [Ihre E-Mail-Adresse]

Verantwortliche Stelle ist die natÃ¼rliche oder juristische Person, die allein oder gemeinsam mit anderen Ã¼ber die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten (z.B. Namen, E-Mail-Adressen o. Ã.) entscheidet.

## 3. Datenerfassung auf unserer Website

### Cookies
Die Internetseiten verwenden teilweise so genannte Cookies. Cookies richten auf Ihrem Rechner keinen Schaden an und enthalten keine Viren. Cookies dienen dazu, unser Angebot nutzerfreundlicher, effektiver und sicherer zu machen.

### Server-Log-Dateien
Der Provider der Seiten erhebt und speichert automatisch Informationen in so genannten Server-Log-Dateien, die Ihr Browser automatisch an uns Ã¼bermittelt. Dies sind:

- Browsertyp und Browserversion
- verwendetes Betriebssystem
- Referrer URL
- Hostname des zugreifenden Rechners
- Uhrzeit der Serveranfrage
- IP-Adresse

Eine ZusammenfÃ¼hrung dieser Daten mit anderen Datenquellen wird nicht vorgenommen.

### Kontaktformular
Wenn Sie uns per Kontaktformular Anfragen zukommen lassen, werden Ihre Angaben aus dem Anfrageformular inklusive der von Ihnen dort angegebenen Kontaktdaten zwecks Bearbeitung der Anfrage und fÃ¼r den Fall von Anschlussfragen bei uns gespeichert. Diese Daten geben wir nicht ohne Ihre Einwilligung weiter.

### Zahlungsdaten
Bei der Bezahlung Ã¼ber unsere Website werden Ihre Zahlungsdaten zur Abwicklung der Transaktion an unseren Zahlungsdienstleister Ã¼bermittelt. Diese Daten werden verschlÃ¼sselt Ã¼bertragen und nur fÃ¼r den Zweck der Zahlungsabwicklung verwendet.

## 4. Analyse Tools und Werbung

[Falls Sie Analyse-Tools wie Google Analytics verwenden, fÃ¼gen Sie hier die entsprechenden Informationen ein]

## 5. Plugins und Tools

[Falls Sie Plugins oder Tools von Drittanbietern verwenden, fÃ¼gen Sie hier die entsprechenden Informationen ein]

## 6. Ihre Rechte als betroffene Person

Sie haben das Recht:

- gemÃ¤Ã Art. 15 DSGVO Auskunft Ã¼ber Ihre von uns verarbeiteten personenbezogenen Daten zu verlangen;
- gemÃ¤Ã Art. 16 DSGVO unverzÃ¼glich die Berichtigung unrichtiger oder VervollstÃ¤ndigung Ihrer bei uns gespeicherten personenbezogenen Daten zu verlangen;
- gemÃ¤Ã Art. 17 DSGVO die LÃ¶schung Ihrer bei uns gespeicherten personenbezogenen Daten zu verlangen;
- gemÃ¤Ã Art. 18 DSGVO die EinschrÃ¤nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen;
- gemÃ¤Ã Art. 20 DSGVO Ihre personenbezogenen Daten, die Sie uns bereitgestellt haben, in einem strukturierten, gÃ¤ngigen und maschinenlesbaren Format zu erhalten oder die Ãbermittlung an einen anderen Verantwortlichen zu verlangen;
- gemÃ¤Ã Art. 21 DSGVO Widerspruch gegen die Verarbeitung Ihrer personenbezogenen Daten einzulegen;
- gemÃ¤Ã Art. 77 DSGVO sich bei einer AufsichtsbehÃ¶rde zu beschweren.


&&& FILE: ./frontend\docs\impressum.md
&&& CONTENT:
# Impressum

Angaben gemÃ¤Ã Â§ 5 TMG:

[Ihr Name]
[Ihre StraÃe und Hausnummer]
[Ihre Postleitzahl und Stadt]

## Kontakt
Telefon: [Ihre Telefonnummer]
E-Mail: [Ihre E-Mail-Adresse]

## Verantwortlich fÃ¼r den Inhalt nach Â§ 55 Abs. 2 RStV:
[Ihr Name]
[Ihre Adresse]

## Haftungsausschluss

### Haftung fÃ¼r Inhalte
Die Inhalte unserer Seiten wurden mit grÃ¶Ãter Sorgfalt erstellt. FÃ¼r die Richtigkeit, VollstÃ¤ndigkeit und AktualitÃ¤t der Inhalte kÃ¶nnen wir jedoch keine GewÃ¤hr Ã¼bernehmen.

### Haftung fÃ¼r Links
Unser Angebot enthÃ¤lt Links zu externen Webseiten Dritter, auf deren Inhalte wir keinen Einfluss haben. Deshalb kÃ¶nnen wir fÃ¼r diese fremden Inhalte auch keine GewÃ¤hr Ã¼bernehmen.

### Urheberrecht
Die durch die Seitenbetreiber erstellten Inhalte und Werke auf diesen Seiten unterliegen dem deutschen Urheberrecht.

&&& FILE: ./frontend\docs\quellen.md
&&& CONTENT:
| Bundesland               | Lizenz    | Quelle                                                                                                                                      |
|--------------------------|-----------|---------------------------------------------------------------------------------------------------------------------------------------------|
| Baden-WÃ¼rttemberg        | BY 2.0    | [OpenGeoData Baden-WÃ¼rttemberg](https://opengeodata.lgl-bw.de/#/(sidenav:product/13))                                                       |
| Bayern                   | CC BY 4.0 | [OpenGeoData Bayern](https://geodaten.bayern.de/opengeodata/OpenDataDetail.html?pn=lod2&active=MASSENDOWNLOAD)                              |
| Berlin                   | BY 2.0    | [GDI Berlin](https://gdi.berlin.de/geonetwork/srv/eng/catalog.search#/metadata/3c7c49af-00a4-3bcd-bc00-20e7f0f1b7bf)                        |
| Brandenburg              |           |                                                                                                                                             |
| Bremen                   |           |                                                                                                                                             |
| Hamburg                  |           |                                                                                                                                             |
| Hessen                   | ?         | [Hessen Geodaten](https://gds.hessen.de/INTERSHOP/web/WFS/HLBG-Geodaten-Site/de_DE/-/EUR/ViewDownloadcenter-Start?path=3D-Daten/3D-Geb%C3%A4udemodelle/3D-Geb%C3%A4udemodelle%20LoD2/Hochtaunuskreis) |
| Mecklenburg-Vorpommern   |           |                                                                                                                                             |
| Niedersachsen            |           |                                                                                                                                             |
| Nordrhein-Westfalen      |           |                                                                                                                                             |
| Rheinland-Pfalz          |           |                                                                                                                                             |
| Saarland                 |           |                                                                                                                                             |
| Sachsen                  |           |                                                                                                                                             |
| Sachsen-Anhalt           |           |                                                                                                                                             |
| Schleswig-Holstein       |           |                                                                                                                                             |
| ThÃ¼ringen                |           |                                                                                                                                             |


&&& FILE: ./frontend\docs\widerruf.md
&&& CONTENT:
# Widerrufsbelehrung

## Widerrufsrecht

Sie haben das Recht, binnen vierzehn Tagen ohne Angabe von GrÃ¼nden diesen Vertrag zu widerrufen.

Die Widerrufsfrist betrÃ¤gt vierzehn Tage ab dem Tag des Vertragsabschlusses.

Um Ihr Widerrufsrecht auszuÃ¼ben, mÃ¼ssen Sie uns ([Ihr Name, Ihre Anschrift, Ihre Telefonnummer und E-Mail-Adresse]) mittels einer eindeutigen ErklÃ¤rung (z. B. ein mit der Post versandter Brief oder E-Mail) Ã¼ber Ihren Entschluss, diesen Vertrag zu widerrufen, informieren.

Zur Wahrung der Widerrufsfrist reicht es aus, dass Sie die Mitteilung Ã¼ber die AusÃ¼bung des Widerrufsrechts vor Ablauf der Widerrufsfrist absenden.

## Folgen des Widerrufs

Wenn Sie diesen Vertrag widerrufen, haben wir Ihnen alle Zahlungen, die wir von Ihnen erhalten haben, einschlieÃlich der Lieferkosten (mit Ausnahme der zusÃ¤tzlichen Kosten, die sich daraus ergeben, dass Sie eine andere Art der Lieferung als die von uns angebotene, gÃ¼nstigste Standardlieferung gewÃ¤hlt haben), unverzÃ¼glich und spÃ¤testens binnen vierzehn Tagen ab dem Tag zurÃ¼ckzuzahlen, an dem die Mitteilung Ã¼ber Ihren Widerruf dieses Vertrags bei uns eingegangen ist. FÃ¼r diese RÃ¼ckzahlung verwenden wir dasselbe Zahlungsmittel, das Sie bei der ursprÃ¼nglichen Transaktion eingesetzt haben, es sei denn, mit Ihnen wurde ausdrÃ¼cklich etwas anderes vereinbart; in keinem Fall werden Ihnen wegen dieser RÃ¼ckzahlung Entgelte berechnet.

Haben Sie verlangt, dass die Dienstleistungen wÃ¤hrend der Widerrufsfrist beginnen sollen, so haben Sie uns einen angemessenen Betrag zu zahlen, der dem Anteil der bis zu dem Zeitpunkt, zu dem Sie uns von der AusÃ¼bung des Widerrufsrechts hinsichtlich dieses Vertrags unterrichten, bereits erbrachten Dienstleistungen im Vergleich zum Gesamtumfang der im Vertrag vorgesehenen Dienstleistungen entspricht.

&&& FILE: ./frontend\public\basemap.json
&&& CONTENT:
{
  "version": 8,
  "name": "Positron",
  "metadata": {
    "mapbox:autocomposite": false,
    "mapbox:groups": {
      "101da9f13b64a08fa4b6ac1168e89e5f": {
        "collapsed": false,
        "name"
................................

&&& FILE: ./frontend\public\vite.svg
&&& CONTENT:
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="31.88" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 257"><defs><linearGradient id="IconifyId1813088fe1fbc01fb466" x1="-.828%" x2="57.636%" y1="7.652%" y2="78.411%"><stop offset="0%" stop-color="#41D1FF"></stop><stop offset="100%" stop-color="#BD34FE"></stop></linearGradient><linearGradient id="IconifyId1813088fe1fbc01fb467" x1="43.376%" x2="50.316%" y1="2.242%" y2="89.03%"><stop offset="0%" stop-color="#FFEA83"></stop><stop offset="8.333%" stop-color="#FFDD35"></stop><stop offset="100%" stop-color="#FFA800"></stop></linearGradient></defs><path fill="url(#IconifyId1813088fe1fbc01fb466)" d="M255.153 37.938L134.897 252.976c-2.483 4.44-8.862 4.466-11.382.048L.875 37.958c-2.746-4.814 1.371-10.646 6.827-9.67l120.385 21.517a6.537 6.537 0 0 0 2.322-.004l117.867-21.483c5.438-.991 9.574 4.796 6.877 9.62Z"></path><path fill="url(#IconifyId1813088fe1fbc01fb467)" d="M185.432.063L96.44 17.501a3.268 3.268 0 0 0-2.634 3.014l-5.474 92.456a3.268 3.268 0 0 0 3.997 3.378l24.777-5.718c2.318-.535 4.413 1.507 3.936 3.838l-7.361 36.047c-.495 2.426 1.782 4.5 4.151 3.78l15.304-4.649c2.372-.72 4.652 1.36 4.15 3.788l-11.698 56.621c-.732 3.542 3.979 5.473 5.943 2.437l1.313-2.028l72.516-144.72c1.215-2.423-.88-5.186-3.54-4.672l-25.505 4.922c-2.396.462-4.435-1.77-3.759-4.114l16.646-57.705c.677-2.35-1.37-4.583-3.769-4.113Z"></path></svg>

&&& FILE: ./frontend\src\App.tsx
&&& CONTENT:
// App.tsx
import React, { useCallback, useState, useRef, useEffect } from "react";
import { createRoot } from "react-dom/client";
import {
  Map,
  NavigationControl,
  Popup,
  useControl,
} from "react-map-gl/maplibre";
import { Tile3DLayer, MapViewState, AmbientLight, DirectionalLight, LightingEffect } from "deck.gl";
import { MapboxOverlay as DeckOverlay } from "@deck.gl/mapbox";
import "maplibre-gl/dist/maplibre-gl.css";
import type { Tileset3D } from "@loaders.gl/tiles";
import MapboxDraw from "@mapbox/mapbox-gl-draw";
import "@mapbox/mapbox-gl-draw/dist/mapbox-gl-draw.css";
import FloatingPanel from "./FloatingPanel";
import Logo from "./Logo";
import * as turf from "@turf/turf";
import "bootstrap/dist/css/bootstrap.min.css";

import DrawControl from './draw-control';
import LegalDocuments from "./Legals";

import './styles.css'
import './colors.css'

const BACKEND_URL = import.meta.env.VITE_BACKEND_URL
const TILESET_URL = 'https://easyopen-tiles.i-am-hellguz.uk/tileset.json';

const INITIAL_VIEW_STATE: MapViewState = { 
  latitude: 49.8917,
  longitude: 10.8863,  
  pitch: 45, 
  maxPitch: 60,
  bearing: 0,
  minZoom: 2,
  maxZoom: 30,
  zoom: 17,
};

const MAP_STYLE = "/basemap.json";

function DeckGLOverlay(props: any) {
  const overlay = useControl(() => new DeckOverlay(props));
  overlay.setProps(props);
  return null;
}

function Root() {
  const [selected, setSelected] = useState<any>(null);
  const [viewState, setViewState] = useState<MapViewState>(INITIAL_VIEW_STATE);
  const [features, setFeatures] = useState<Record<string, any>>({});
  const [isLod2Visible, setIsLod2Visible] = useState(true);
  const [polygonArea, setPolygonArea] = useState<number | null>(null);
  const mapRef = useRef<any>(null); // Reference to the map instance

  const drawRef = useRef<MapboxDraw | null>(null); // Reference to the MapboxDraw instance

  // Initialize MapboxDraw and add it to the map
  const handleMapLoad = useCallback(() => {
    const map = mapRef.current.getMap();

    // Initialize MapboxDraw if not already initialized
    if (!drawRef.current) {
      drawRef.current = new MapboxDraw({
        displayControlsDefault: false,
        controls: {
          polygon: false,
          trash: false,
        },
      });
      map.addControl(drawRef.current);
    }
  }, []);
  // const onTilesetLoad = (tileset: Tileset3D) => {
  //   const { cartographicCenter, zoom } = tileset;
  //   setViewState((prev) => ({
  //     ...prev,
  //     longitude: cartographicCenter[0],
  //     latitude: cartographicCenter[1],
  //     zoom,
  //   }));
  // };


  const onUpdate = useCallback((e) => {
    setFeatures((currFeatures) => {
      const newFeatures = { ...currFeatures };
      for (const f of e.features) {
        newFeatures[f.id] = f;
      }
      return newFeatures;
    });

    // Calculate polygon area
    if (e.features && e.features.length > 0) {
      const polygon = e.features[0];
      const area = turf.area(polygon) / 1e6; // Convert from mÂ² to kmÂ²
      setPolygonArea(area);
    }
  }, []);

  const onDelete = useCallback((e) => {
    setFeatures((currFeatures) => {
      const newFeatures = { ...currFeatures };
      for (const f of e.features) {
        delete newFeatures[f.id];
      }
      return newFeatures;
    });
    setPolygonArea(null);
  }, []);

  const handleDrawPolygon = () => {
    if (drawRef.current) {
      drawRef.current.deleteAll();
      drawRef.current.changeMode("draw_polygon");
    }
  };

  const handleRemovePolygon = () => {
    if (drawRef.current) {
      drawRef.current.deleteAll();
    }
  };

  const handleFetchObjFile = async () => {
    console.info("getFetchObjFile")
    if (drawRef.current) {
      const data = drawRef.current.getAll();
      if (data.features.length > 0) {
        try {
          const response = await fetch(BACKEND_URL + "/retrieve_obj", {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
            },
            body: JSON.stringify({ region: data }),
          });

          if (response.ok) {
            const blob = await response.blob();
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement("a");
            a.style.display = "none";
            a.href = url;
            // Use the filename from the Content-Disposition header if available
            const contentDisposition = response.headers.get("Content-Disposition");
            const filenameMatch =
              contentDisposition && contentDisposition.match(/filename="?(.+)"?/i);
            a.download = filenameMatch
              ? filenameMatch[1]
              : `object_file.obj`;
            document.body.appendChild(a);
            a.click();
            window.URL.revokeObjectURL(url);
          } else {
            console.error("Failed to fetch obj file");
          }
        } catch (error) {
          console.error("Error fetching obj file:", error);
        }
      } else {
        console.error("No polygon drawn");
      }
    }
  };

  const handleZoomChange = (event: any) => {
    const newZoom = event.viewState.zoom;
    setViewState(event.viewState);

    // Toggle visibility based on zoom level
    if (newZoom < 1) {
      setIsLod2Visible(false);
    } else {
      setIsLod2Visible(true);
    }
  };

// Create ambient light
const ambientLight = new AmbientLight({
  color: [240, 255, 255],
  intensity: 1.0
});

// Create directional light
const directionalLight1 = new DirectionalLight({
  color: [220, 255, 255],
  intensity: 0.6,
  direction: [-1, -3, -1]
});

// Create directional light
const directionalLight2 = new DirectionalLight({
  color:  [255, 220, 255],
  intensity: 1,
  direction: [1, -3, 1]
});

// Create lighting effect
const lightingEffect = new LightingEffect({ambientLight, directionalLight1 ,directionalLight2});


  const layers = [
    new Tile3DLayer({
      id: "tile-3d-layer",
      data: TILESET_URL,
      // pickable: true,
      // autoHighlight: false,
      // onClick: (info, event) => console.log("Clicked:", info, event),
      // getPickingInfo: (pickParams) => console.log("PickInfo", pickParams),
      // onTilesetLoad,
      visible: isLod2Visible,
      // For ScenegraphLayer (b3dm or i3dm format)
      //_lighting: 'pbr',
      //effects: [lightingEffect],
      loadOptions: {
        tileset: {
          maxRequests: 16,
          updateTransforms: false,
          maximumMemoryUsage: 512
          //maximumScreenSpaceError: 16, // Adjust this value as needed
          //viewDistanceScale: 1.5 // Adjust this value as needed
        }
      },
      // Additional sublayer props for fine-grained control
      _subLayerProps: {
        scenegraph: {
          getColor: (d) => [254, 254, 254, 255], // Blue color for scenegraph models (alternative method)
      //effects: [lightingEffect]
        }
      }
    }),
  ];

  const handleSearch = async (query: string) => {
    const response = await fetch(
      `https://nominatim.openstreetmap.org/search?format=json&q=${encodeURIComponent(query)}&countrycodes=de`
    );
    const data = await response.json();
    return data;
  };
  
  const handleSelectResult = (result: any) => {
    // Fly to the selected location
    const map = mapRef.current.getMap();

    map.flyTo({
      center: [parseFloat(result.lon), parseFloat(result.lat)],
      zoom: 14
    });
  };

  return (
    <div style={{ position: "relative", width: "100%", height: "100vh" }}>
      <Map
        initialViewState={viewState}
        mapStyle={MAP_STYLE}
        onLoad={handleMapLoad} // Ensure map is passed here
        onMove={handleZoomChange}
        ref={mapRef}
        style={{ width: "100%", height: "100%" }}
        hash={true}
      >
        {selected && (
          <Popup
            key={selected.properties.name}
            anchor="bottom"
            style={{ zIndex: 10 }}
            longitude={selected.geometry.coordinates[0]}
            latitude={selected.geometry.coordinates[1]}
          >
            {selected.properties.name} ({selected.properties.abbrev})
          </Popup>
        )}
        <DeckGLOverlay layers={layers}   //effects={[lightingEffect]} // Apply the custom lighting effect globally
 />
        {/* <DrawControl
          ref={drawRef}
          onCreate={onUpdate}
          onUpdate={onUpdate}
          onDelete={onDelete}
        /> */}
        <NavigationControl position="top-left" />

        <DrawControl
          position="top-right"
          displayControlsDefault={false}
          controls={{
            polygon: false,
            trash: false
          }}
          defaultMode="draw_polygon"
          onCreate={onUpdate}
          onUpdate={onUpdate}
          onDelete={onDelete}
        />
      </Map>
      <FloatingPanel
        onDrawPolygon={handleDrawPolygon}
        onRemovePolygon={handleRemovePolygon}
        onFetchObjFile={handleFetchObjFile}
        polygonArea={polygonArea}
  onSearch={handleSearch}
  onSelectResult={handleSelectResult}
      />
      <Logo/>
      
      <LegalDocuments />
    </div>
  );
}

interface DrawControlProps {
  onCreate: (e: any) => void;
  onUpdate: (e: any) => void;
  onDelete: (e: any) => void;
}

export default Root;


&&& FILE: ./frontend\src\CheckoutForm.tsx
&&& CONTENT:
import React, { useState } from "react";
import { useStripe, useElements, CardElement } from "@stripe/react-stripe-js";

interface CheckoutFormProps {
  price: number;
  onFetchObjFile: () => void;
}

interface CustomerData {
  email: string;
  name: string;
  address: {
    line1: string;
    postal_code: string;
    city: string;
    country: string;
  };
}
const CheckoutForm: React.FC<CheckoutFormProps> = ({ price, onFetchObjFile }) => {
  const stripe = useStripe();
  const elements = useElements();
  const [loading, setLoading] = useState(false);
  const [isExpanded, setIsExpanded] = useState(false);
  const [customerData, setCustomerData] = useState<CustomerData>({
    email: "",
    name: "",
    address: {
      line1: "",
      postal_code: "",
      city: "",
      country: "DE",
    },
  });

  // If price is 0, render only the download button
  if (price === 0) {
    return (
      <>
      <div className="text-center mb-3">
      GrundstÃ¼cke unter 0.05 kmÂ² kÃ¶nnen kostenfrei heruntergeladen werden
      </div>
      <button 
        onClick={onFetchObjFile}
        className="btn btn-secondary btn-sm mt-2"
      >
        Download File
      </button>
      </>
    );
  }

  const handleFocus = () => {
    setIsExpanded(true);
  };
  const handleFocusOut = () => {
    setIsExpanded(false);
  };

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!stripe || !elements) return;

    if (!customerData.email || !customerData.name || !customerData.address.line1 || 
        !customerData.address.postal_code || !customerData.address.city) {
      document.getElementById("payment-message")!.textContent = "Please fill in all required fields.";
      return;
    }

    setLoading(true);

    try {
      const response = await fetch(import.meta.env.VITE_BACKEND_URL + "/create-payment-intent", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ 
          amount: Math.round(price * 100),
          customer: customerData
        }),
      });

      if (!response.ok) throw new Error("Failed to create PaymentIntent.");

      const { clientSecret } = await response.json();

      const result = await stripe.confirmCardPayment(clientSecret, {
        payment_method: {
          billing_details: {
            name: customerData.name,
            email: customerData.email,
            address: {
              line1: customerData.address.line1,
              postal_code: customerData.address.postal_code,
              city: customerData.address.city,
              country: customerData.address.country,
            },
          },
          card: elements.getElement(CardElement)!,
        },
      });

      if (result.error) {
        document.getElementById("payment-message")!.textContent = result.error.message;
      } else if (result.paymentIntent?.status === "succeeded") {
        document.getElementById("payment-message")!.textContent = "Success! Your download will start soon.";
        onFetchObjFile();
      }
    } catch (error) {
      console.error("Payment error:", error);
    } finally {
      setLoading(false);
    }
  };
  return (
    <form onSubmit={handleSubmit} className="d-flex flex-column gap-2">
      
      {/* Secure Payment Badge */}
      <div className="d-flex align-items-center gap-1 text-secondary small">
        <span className="bi bi-lock-fill"></span>
        Secure payment via Stripe
      </div>
  
      {/* Price Details */}
      <div className="text-secondary small">
        <p>
          <strong>Order Total:</strong> â¬{price.toFixed(2)}
        </p>
        <p>No additional fees. Youâll only be charged this amount.</p>
      </div>
  
      {isExpanded && (
        <div className="mt-2 animate__animated animate__fadeIn">
          {/* Customer Details */}
          <input
            type="email"
            placeholder="Email *"
            required
            value={customerData.email}
            onChange={(e) =>
              setCustomerData({ ...customerData, email: e.target.value })
            }
            className="form-control form-control-sm mb-2"
          />
          <input
            type="text"
            placeholder="Full Name *"
            required
            value={customerData.name}
            onChange={(e) =>
              setCustomerData({ ...customerData, name: e.target.value })
            }
            className="form-control form-control-sm mb-2"
          />
          <input
            type="text"
            placeholder="Street Address *"
            required
            value={customerData.address.line1}
            onChange={(e) =>
              setCustomerData({
                ...customerData,
                address: { ...customerData.address, line1: e.target.value },
              })
            }
            className="form-control form-control-sm mb-2"
          />
          <div className="d-flex gap-2 mb-2">
            <input
              type="text"
              placeholder="Postal Code *"
              required
              value={customerData.address.postal_code}
              onChange={(e) =>
                setCustomerData({
                  ...customerData,
                  address: {
                    ...customerData.address,
                    postal_code: e.target.value,
                  },
                })
              }
              className="form-control form-control-sm"
            />
            <input
              type="text"
              placeholder="City *"
              required
              value={customerData.address.city}
              onChange={(e) =>
                setCustomerData({
                  ...customerData,
                  address: { ...customerData.address, city: e.target.value },
                })
              }
              className="form-control form-control-sm"
            />
          </div>
        </div>
      )}
  
      {/* Card Element */}
      <CardElement
      
      onFocus={handleFocus}
      onFocusOut={handleFocusOut}
        options={{
          style: {
            base: {
              fontSize: "14px",
            },
          },
        }}
      />
      <div id="payment-message" className="text-danger small"></div>
  
      {/* Link to Stripe Security Info */}
      <div className="small mt-7">
        <a
          href="https://stripe.com/docs/security"
          target="_blank"
          rel="noopener noreferrer"
          className="text-secondary"
        >
          Learn more about how your payment information is secured.
        </a>
      </div>
  
      <button
        type="submit"
        disabled={!stripe || loading}
        className="btn btn-secondary btn-sm mt-2"
      >
        {loading ? "Processing..." : `Pay â¬${price.toFixed(2)}`}
      </button>
    </form>
  );
  
};
export default CheckoutForm;

&&& FILE: ./frontend\src\colors.css
&&& CONTENT:
:root {
    --bs-primary: #2a7a92; /* Your desired primary color */
    --bs-secondary: #3e66bb;
    --bs-success: #28a745;
    --bs-danger: #f38093;
    --bs-warning: #ffc107;
    --bs-info: #17a2b8;
    --bs-light: #f8f9fa;
    --bs-dark: #343a40;
  }
  
  .btn-primary {
    background-color: var(--bs-primary) !important;
    border-color: var(--bs-primary) !important;
  }
  .btn-secondary {
    background-color: var(--bs-secondary) !important;
    border-color: var(--bs-secondary) !important;
  }
  
  .btn-danger {
    background-color: var(--bs-danger) !important;
    border-color: var(--bs-danger) !important;
  }
  


&&& FILE: ./frontend\src\draw-control.ts
&&& CONTENT:
import MapboxDraw from '@mapbox/mapbox-gl-draw';
import {useControl} from 'react-map-gl';

import type {MapRef, ControlPosition} from 'react-map-gl';

type DrawControlProps = ConstructorParameters<typeof MapboxDraw>[0] & {
  position?: ControlPosition;

  onCreate?: (evt: {features: object[]}) => void;
  onUpdate?: (evt: {features: object[]; action: string}) => void;
  onDelete?: (evt: {features: object[]}) => void;
};

export default function DrawControl(props: DrawControlProps) {
  useControl<MapboxDraw>(
    () => new MapboxDraw(props),
    ({map}: {map: MapRef}) => {
      map.on('draw.create', props.onCreate);
      map.on('draw.update', props.onUpdate);
      map.on('draw.delete', props.onDelete);
    },
    ({map}: {map: MapRef}) => {
      map.off('draw.create', props.onCreate);
      map.off('draw.update', props.onUpdate);
      map.off('draw.delete', props.onDelete);
    },
    {
      position: props.position
    }
  );

  return null;
}

DrawControl.defaultProps = {
  onCreate: () => {},
  onUpdate: () => {},
  onDelete: () => {}
};

&&& FILE: ./frontend\src\FloatingPanel.tsx
&&& CONTENT:
import React, { useState, useEffect } from "react";
import { loadStripe } from "@stripe/stripe-js";
import { Elements } from "@stripe/react-stripe-js";
import CheckoutForm from "./CheckoutForm";
import "bootstrap/dist/css/bootstrap.min.css";
// Import FontAwesome
import { FontAwesomeIcon } from '@fortawesome/react-fontawesome';
import { faChevronUp, faChevronDown } from '@fortawesome/free-solid-svg-icons';

// Initialize Stripe with your publishable key
const stripePromise = loadStripe(
  "hidden_api"
);

interface FloatingPanelProps {
  onDrawPolygon: () => void;
  onRemovePolygon: () => void;
  onFetchObjFile: () => void;
  polygonArea: number | null; // in square kilometers
  onSearch: (query: string) => Promise<any>;
  onSelectResult: (result: any) => void;
}

const FloatingPanel: React.FC<FloatingPanelProps> = ({
  onDrawPolygon,
  onRemovePolygon,
  onFetchObjFile,
  polygonArea,
  onSearch,
  onSelectResult,
}) => {
  const [price, setPrice] = useState<number>(0);

  const [searchQuery, setSearchQuery] = useState("");
  const [searchResults, setSearchResults] = useState<any[]>([]);
  const [isSearching, setIsSearching] = useState(false);

  // New state for expanded tab
  const [expandedTab, setExpandedTab] = useState<string | null>(null);

  const [isMobile, setIsMobile] = useState<boolean>(
    window.innerWidth <= 768
  );

  useEffect(() => {
    if (polygonArea !== null) {
      if (polygonArea < 0.05) {
        setPrice(0);
      } else {
        setPrice(20 * polygonArea);
      }
    }
  }, [polygonArea]);

  // Handle window resize to update isMobile state
  useEffect(() => {
    const handleResize = () => {
      setIsMobile(window.innerWidth <= 768);
    };

    window.addEventListener("resize", handleResize);
    return () => window.removeEventListener("resize", handleResize);
  }, []);

  // Handle search functionality
  const handleSearch = async (e: React.ChangeEvent<HTMLInputElement>) => {
    const query = e.target.value;
    setSearchQuery(query);

    if (query.length > 2) {
      setIsSearching(true);
      try {
        const results = await onSearch(query);
        setSearchResults(results);
      } catch (error) {
        console.error("Search error:", error);
        setSearchResults([]);
      }
      setIsSearching(false);
    } else {
      setSearchResults([]);
    }
  };

  // Extracted content functions for reuse
  const renderAuswahlContent = () => (
    <>
      {/* Search Input */}
      <div className="w-100 mb-3 position-relative">
        <input
          type="text"
          className="form-control form-control-sm"
          placeholder="Search location..."
          value={searchQuery}
          onChange={handleSearch}
        />

        {/* Search Results */}
        {searchResults.length > 0 && (
          <div
            className="position-absolute bg-white shadow-sm rounded mt-1 w-100 overflow-auto"
            style={{
              maxHeight: "150px",
              zIndex: 1060,
            }}
          >
            {searchResults.map((result, index) => (
              <div
                key={index}
                className="p-2 hover-bg-light cursor-pointer"
                onClick={() => {
                  onSelectResult(result);
                  setSearchResults([]);
                  setSearchQuery("");
                }}
                style={{ cursor: "pointer" }}
              >
                {result.display_name || result.name}
              </div>
            ))}
          </div>
        )}

        {isSearching && (
          <div className="text-center mt-2">
            <small>Suche...</small>
          </div>
        )}
      </div>

      {/* Polygon Area and Price Display */}
      {polygonArea !== null ? (
        <div className="text-center mb-3">
          <strong>GebietflÃ¤che:</strong> {polygonArea.toFixed(2)} kmÂ²
        </div>
      ) : (
        <p className="text-center mb-3">Zeichnen Sie das Gebiet ein</p>
      )}

      {/* Action Buttons */}
      <div className="btn-group w-100" role="group">
        <button
          type="button"
          className="btn btn-primary btn-sm mt-2"
          onClick={onDrawPolygon}
        >
          Polygon einzeichnen
        </button>
        <button
          type="button"
          className="btn btn-danger btn-sm mt-2"
          onClick={onRemovePolygon}
        >
          Entfernen
        </button>
      </div>
    </>
  );

  const renderHerunterladenContent = () => (
    <>
      <Elements stripe={stripePromise}>
        <CheckoutForm price={price} onFetchObjFile={onFetchObjFile} />
      </Elements>
    </>
  );

  // Mobile Layout
  if (isMobile) {
    return (
      <div
        className="position-fixed"
        style={{
          bottom: "40px",
          left: "0",
          right: "0",
          zIndex: 1050,
          pointerEvents: "none",
        }}
      >
        {/* Auswahl Tab */}
        <div
          style={{
            pointerEvents: "auto",
            marginBottom: expandedTab === 'Auswahl' ? '0' : '10px',
          }}
        >
          <div
            className={`bg-white shadow p-3 d-flex justify-content-between align-items-center ${
              expandedTab === 'Auswahl' ? 'rounded-top' : 'rounded'
            }`}
            style={{
              margin: "0 20px",
              cursor: "pointer",
            }}
            onClick={() => {
              setExpandedTab(expandedTab === 'Auswahl' ? null : 'Auswahl');
            }}
          >
            <h5 className="mb-0">Auswahl</h5>
            {/* Expand/Collapse Icon */}
            <FontAwesomeIcon
              icon={expandedTab === 'Auswahl' ? faChevronUp : faChevronDown}
            />
          </div>
          {expandedTab === 'Auswahl' && (
            <div
              className="bg-white shadow p-3 rounded-bottom"
              style={{
                margin: "0 20px 20px",
              }}
            >
              {renderAuswahlContent()}
            </div>
          )}
        </div>

        {/* Herunterladen Tab */}
        <div
          style={{
            pointerEvents: "auto",
            marginBottom: expandedTab === 'Herunterladen' ? '0' : '10px',
          }}
        >
          <div
            className={`bg-white shadow p-3 d-flex justify-content-between align-items-center ${
              expandedTab === 'Herunterladen' ? 'rounded-top' : 'rounded'
            }`}
            style={{
              margin: "0 20px",
              cursor: "pointer",
            }}
            onClick={() => {
              setExpandedTab(expandedTab === 'Herunterladen' ? null : 'Herunterladen');
            }}
          >
            <h5 className="mb-0">Herunterladen</h5>
            {/* Expand/Collapse Icon */}
            <FontAwesomeIcon
              icon={expandedTab === 'Herunterladen' ? faChevronUp : faChevronDown}
            />
          </div>
          {expandedTab === 'Herunterladen' && (
            <div
              className="bg-white shadow p-3 rounded-bottom"
              style={{
                margin: "0 20px 20px",
              }}
            >
              {renderHerunterladenContent()}
            </div>
          )}
        </div>
      </div>
    );
  }

  // Desktop Layout remains unchanged
  return (
    <div
      className="d-flex align-items-end"
      style={{
        position: "absolute",
        bottom: "20px",
        left: "20px",
        right: "20px",
        gap: "20px",
        zIndex: 1050,
        pointerEvents: "none",
      }}
    >
      <div
        className="d-flex gap-3"
        style={{
          marginRight: "auto",
          pointerEvents: "none",
        }}
      >
        {/* Auswahl Panel */}
        <div
          className="bg-white rounded shadow p-3 d-flex flex-column align-items-center justify-content-center"
          style={{
            width: "300px",
            minHeight: "200px",
            pointerEvents: "auto",
          }}
        >
          <h5 className="mb-3 text-center">Auswahl</h5>
          {renderAuswahlContent()}
        </div>
      </div>

      {/* Herunterladen Panel */}
      <div
        className="bg-white rounded shadow p-3 d-flex flex-column "
        style={{
          width: "300px",
          minHeight: "50px",
          marginLeft: "auto",
          transition: "height 0.3s ease-in-out",
          pointerEvents: "auto",
        }}
      >
        <h5 className="mb-3 text-center">Herunterladen</h5>
        {renderHerunterladenContent()}
      </div>
    </div>
  );
};

export default FloatingPanel;


&&& FILE: ./frontend\src\index.tsx
&&& CONTENT:
import React from 'react';
import ReactDOM from 'react-dom/client';
import App from './App';

ReactDOM.createRoot(document.getElementById('root')!).render(<App />);


&&& FILE: ./frontend\src\Legals.tsx
&&& CONTENT:
import React, { useState, useEffect } from 'react';
import ReactMarkdown from 'react-markdown';
import remarkGfm from 'remark-gfm';

const LegalDocumentPanel: React.FC<{
  documentType: string;
  isOpen: boolean;
  setOpenDocument: (doc: string | null) => void;
}> = ({ documentType, isOpen, setOpenDocument }) => {
  const [content, setContent] = useState('');

  useEffect(() => {
    if (isOpen && !content) {
      const fetchContent = async () => {
        try {
          const response = await fetch(`/docs/${documentType}.md`);
          const text = await response.text();
          setContent(text);
        } catch (error) {
          console.error(`Error loading ${documentType}:`, error);
          setContent('Failed to load content.');
        }
      };
      fetchContent();
    }
  }, [isOpen, content, documentType]);

  const handleClick = () => {
    setOpenDocument(isOpen ? null : documentType);
  };

  return (
    <>
      <button
        onClick={handleClick}
        className="btn btn-sm btn-light"
        style={{
          display: 'inline-block',
          padding: '0.25rem 0.5rem',
        }}
      >
        {documentType.charAt(0).toUpperCase() + documentType.slice(1)}
      </button>

      {isOpen && (
        <div
          className="bg-white rounded shadow p-3"
          style={{
            position: 'absolute',
            top: '50px',
            right: '10px',
            maxWidth: '600px',
            width: '80vw',
            maxHeight: '50vh',
            overflowY: 'auto',
            zIndex: 1070,
          }}
        >
          <button
            className="btn btn-sm btn-close float-end"
            onClick={() => setOpenDocument(null)}
          />
          <h5 className="mb-3">
            {documentType.charAt(0).toUpperCase() + documentType.slice(1)}
          </h5>
          <ReactMarkdown remarkPlugins={[remarkGfm]}>{content}</ReactMarkdown>
        </div>
      )}
    </>
  );
};

const LegalDocuments: React.FC = () => {
  const [openDocument, setOpenDocument] = useState<string | null>(null);

  return (
    <div
      style={{
        position: 'absolute',
        top: '10px',
        right: '10px',
        display: 'flex',
        gap: '20px',
        zIndex: 1060,
      }}
    >
    <LegalDocumentPanel
      documentType="quellen"
      isOpen={openDocument === 'quellen'}
      setOpenDocument={setOpenDocument}
    />
    <LegalDocumentPanel
      documentType="impressum"
      isOpen={openDocument === 'impressum'}
      setOpenDocument={setOpenDocument}
    />
      <LegalDocumentPanel
        documentType="datenschutz"
        isOpen={openDocument === 'datenschutz'}
        setOpenDocument={setOpenDocument}
      />
      <LegalDocumentPanel
        documentType="agb"
        isOpen={openDocument === 'agb'}
        setOpenDocument={setOpenDocument}
      />
      <LegalDocumentPanel
        documentType="widerruf"
        isOpen={openDocument === 'widerruf'}
        setOpenDocument={setOpenDocument}
      />
    </div>
  );
};

export default LegalDocuments;


&&& FILE: ./frontend\src\Logo.tsx
&&& CONTENT:
import React from "react";
import "bootstrap/dist/css/bootstrap.min.css";
import "./styles.css"; // Assuming the CSS class `.space-grotesk-extrabold` is defined here

const Logo: React.FC = () => {
  return (
    <div
      className="position-fixed text-center"
      style={{
        top: "60px",
        left: "0",
        right: "0",
        zIndex: 1050,
        pointerEvents: "none", // Makes it non-interactive
      }}
    >
      <div
        className="d-inline-block px-5 py-2 rounded shadow bg-light"
        style={{
          color: "black",
          maxWidth: "90%", // Ensure it fits smaller screens
          pointerEvents: "auto", // Enables interactions if needed
          fontWeight: 500, // Ensures the font is clearly visible
        }}
      >
        <span
          className="space-grotesk-regular"
          style={{
            fontSize: "1.5rem",
          }}
        >
          EasyOpenData
        </span>
      </div>
    </div>
  );
};

export default Logo;


&&& FILE: ./frontend\src\styles.css
&&& CONTENT:
@import url('https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap');
@import url('https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&family=Space+Grotesk:wght@300..700&display=swap');

html, body {
   
  height: 100svh; /* Ensure the root elements cover the entire viewport */
  overflow: hidden; /* Prevent scrolling */
  margin: 0; /* Remove any default margins */
}


/* Override all fonts */
* {
  font-family: "Space Grotesk", sans-serif;
  font-optical-sizing: auto;
  font-weight: 300;
  font-style: normal;
}
.ibm-plex-sans-thin {
    font-family: "IBM Plex Sans", sans-serif;
    font-weight: 100;
    font-style: normal;
  }
  
  .ibm-plex-sans-extralight {
    font-family: "IBM Plex Sans", sans-serif;
    font-weight: 200;
    font-style: normal;
  }
  
  .ibm-plex-sans-light {
    font-family: "IBM Plex Sans", sans-serif;
    font-weight: 300;
    font-style: normal;
  }
  
  .ibm-plex-sans-regular {
    font-family: "IBM Plex Sans", sans-serif;
    font-weight: 400;
    font-style: normal;
  }
  
  .ibm-plex-sans-medium {
    font-family: "IBM Plex Sans", sans-serif;
    font-weight: 500;
    font-style: normal;
  }
  
  .ibm-plex-sans-semibold {
    font-family: "IBM Plex Sans", sans-serif;
    font-weight: 600;
    font-style: normal;
  }
  
  .ibm-plex-sans-bold {
    font-family: "IBM Plex Sans", sans-serif;
    font-weight: 700;
    font-style: normal;
  }
  
  .ibm-plex-sans-thin-italic {
    font-family: "IBM Plex Sans", sans-serif;
    font-weight: 100;
    font-style: italic;
  }
  
  .ibm-plex-sans-extralight-italic {
    font-family: "IBM Plex Sans", sans-serif;
    font-weight: 200;
    font-style: italic;
  }
  
  .ibm-plex-sans-light-italic {
    font-family: "IBM Plex Sans", sans-serif;
    font-weight: 300;
    font-style: italic;
  }
  
  .ibm-plex-sans-regular-italic {
    font-family: "IBM Plex Sans", sans-serif;
    font-weight: 400;
    font-style: italic;
  }
  
  .ibm-plex-sans-medium-italic {
    font-family: "IBM Plex Sans", sans-serif;
    font-weight: 500;
    font-style: italic;
  }
  
  .ibm-plex-sans-semibold-italic {
    font-family: "IBM Plex Sans", sans-serif;
    font-weight: 600;
    font-style: italic;
  }
  
  .ibm-plex-sans-bold-italic {
    font-family: "IBM Plex Sans", sans-serif;
    font-weight: 700;
    font-style: italic;
  }
  
.space-grotesk-light {
  font-family: "Space Grotesk", sans-serif;
  font-optical-sizing: auto;
  font-weight: 300;
  font-style: normal;
}
.space-grotesk-regular {
  font-family: "Space Grotesk", sans-serif;
  font-optical-sizing: auto;
  font-weight: 400;
  font-style: normal;
}
.space-grotesk-bold {
  font-family: "Space Grotesk", sans-serif;
  font-optical-sizing: auto;
  font-weight: 500;
  font-style: normal;
}
.space-grotesk-extrabold {
  font-family: "Space Grotesk", sans-serif;
  font-optical-sizing: auto;
  font-weight: 700;
  font-style: normal;
}

