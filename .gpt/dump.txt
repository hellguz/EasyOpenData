<./.env>
# Frontend Variables
VITE_BASE_URL=https://easyopen-server.i-am-hellguz.uk
VITE_TILESET_URL=https://easyopen-tiles.i-am-hellguz.uk/tileset.json
FRONTEND_URL=https://easyopen.i-am-hellguz.uk

# Backend Variables
DATABASE_USER=postgres
DATABASE_PASSWORD=barcelona
DATABASE_HOST=easyopen_postgis
DATABASE_PORT=5432
DATABASE_NAME=easyopendata_database

# PGAdmin Variables
PGADMIN_DEFAULT_EMAIL=hellguz@gmail.com
PGADMIN_DEFAULT_PASSWORD=barcelona

# Stripe Variables
VITE_STRIPE_PUBLISHABLE_KEY=hidden_api
VITE_STRIPE_SECRET_KEY=hidden_ap_2

# Cloudflared Variables
CLOUDFLARED_TUNNEL_TOKEN=your_token_here


<./.env.sample>
# Frontend Variables
VITE_BASE_URL=https://easyopen-server.i-am-hellguz.uk
VITE_TILESET_URL=https://easyopen-tiles.i-am-hellguz.uk/tileset.json
FRONTEND_URL=https://easyopen.i-am-hellguz.uk

# Backend Variables
DATABASE_USER=postgres
DATABASE_PASSWORD=barcelona
DATABASE_HOST=easyopen_postgis
DATABASE_PORT=5432
DATABASE_NAME=easyopendata_database

# PGAdmin Variables
PGADMIN_DEFAULT_EMAIL=hellguz@gmail.com
PGADMIN_DEFAULT_PASSWORD=barcelona

# Stripe Variables
VITE_STRIPE_PUBLISHABLE_KEY=your_public_key_here
VITE_STRIPE_SECRET_KEY=your_private_key_here

# Cloudflared Variables
CLOUDFLARED_TUNNEL_TOKEN=your_token_here


<./.gitignore>
# Ignore local data files
data_local/ # Old, can be removed if data/ is the new standard

# Python cache
__pycache__/
*.pyc

# Environment variables
.env
# For .env.dev, .env.prod
*.env.* 

# Other ignores
*.so

# tileset
backend/tileset*/ # Keep if backend itself writes here temporarily during dev/ingestion
!backend/tileset/README.md

*.gml
*.gfs
*.obj

#venv
venv/
.venv/
.conda

node_modules/

# Main data directory
data/


<./docker-compose.dev.yml>
# DEVELOPMENT docker-compose.dev.yml
# This file defines the development environment.
# Use `docker-compose -f docker-compose.dev.yml up --build` to run.

services:
  easyopen_postgis_dev: # Dev specific service name
    container_name: easyopen_postgis_dev
    image: postgis/postgis:17-3.5
    restart: always
    ports:
      - "${DEV_DB_PORT:-8735}:5432" # Use dev port from .env or default
    environment:
      POSTGRES_PASSWORD: ${DATABASE_DEV_PASSWORD:-barcelona_dev} # Use specific dev password
      POSTGRES_USER: ${DATABASE_USER:-postgres}
      POSTGRES_DB: ${DATABASE_DEV_NAME:-easyopendata_database_dev} # Use specific dev DB name
    volumes:
      - ./data/postgres_data_dev:/var/lib/postgresql/data # Dev specific volume path
      - ./backend/db/init.sql:/docker-entrypoint-initdb.d/init.sql # Can share init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 10s
      timeout: 5s
      retries: 20
    networks:
      - easyopen_dev_network

  easyopen_backend_dev: # Dev specific service name
    container_name: easyopen_backend_dev
    build:
      context: ./backend
      dockerfile: Dockerfile # Original Dockerfile is fine for dev with command override
    command: uvicorn app.main:app --host 0.0.0.0 --port 5400 --reload # --reload for development
    volumes:
      - ./backend:/app # Mount for live code changes
      - ./data/tileset:/data/tileset # Mount shared data directories
      - ./data/tempfiles:/data/tempfiles
    ports:
      - "${DEV_BACKEND_PORT:-5400}:5400"
    environment:
      DATABASE_URL: postgresql+asyncpg://${DATABASE_USER:-postgres}:${DATABASE_DEV_PASSWORD:-barcelona_dev}@easyopen_postgis_dev:5432/${DATABASE_DEV_NAME:-easyopendata_database_dev}
      STRIPE_SECRET_KEY: ${VITE_STRIPE_SECRET_KEY} # Should be a TEST key for dev from .env
      FRONTEND_URL: ${FRONTEND_DEV_URL:-http://localhost:${DEV_FRONTEND_PORT:-5173}}
      PYTHONUNBUFFERED: "1"
      LOG_LEVEL: "DEBUG"
      ENVIRONMENT: "development"
    depends_on:
      easyopen_postgis_dev:
        condition: service_healthy
    networks:
      - easyopen_dev_network

  easyopen_frontend_dev: # Dev specific service name
    container_name: easyopen_frontend_dev
    build:
      context: ./frontend
      dockerfile: Dockerfile # Original Dockerfile is for Vite dev server
      # No build args needed here as Vite dev server uses runtime env vars
    volumes:
      - ./frontend:/app # Mount for live code changes
      - /app/node_modules # Keep node_modules in container to avoid host conflicts
    ports:
      - "${DEV_FRONTEND_PORT:-5173}:5173"
    environment:
      # Vite dev server needs to know where the backend (on host) is
      VITE_BASE_URL: http://localhost:${DEV_BACKEND_PORT:-5400}
      VITE_TILESET_URL: http://localhost:${DEV_TILESET_PORT:-5576}/tileset.json # Points to dev tileset server
      VITE_STRIPE_PUBLISHABLE_KEY: ${VITE_STRIPE_PUBLISHABLE_KEY} # Test publishable key
      CHOKIDAR_USEPOLLING: "true" # For file watching in Docker
    networks:
      - easyopen_dev_network

  easyopen_tileset_dev: # Dev specific service name
    container_name: easyopen_tileset_dev
    image: nginx:alpine
    restart: always
    ports:
      - "${DEV_TILESET_PORT:-5576}:80"
    volumes:
      - ./data/tileset:/usr/share/nginx/html:ro
      - ./backend/tileset.conf:/etc/nginx/conf.d/default.conf:ro
    networks:
      - easyopen_dev_network

  easyopen_pgadmin_dev: # Dev specific service name
    container_name: easyopen_pgadmin_dev
    image: dpage/pgadmin4
    restart: always
    ports:
      - "${DEV_PGADMIN_PORT:-5050}:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD} # Use dev-specific pgadmin pass if needed
    volumes:
      - ./data/pgadmin_data_dev:/var/lib/pgadmin
    depends_on:
      - easyopen_postgis_dev
    networks:
      - easyopen_dev_network

  # Backup service might not be needed for typical local development
  # easyopen_backup_service_dev: ...

  # Cloudflared might be used for exposing dev environment if needed
  # easyopen_cloudflared_dev: ...

volumes:
  postgres_data_dev: # Renamed for development
    name: easyopen_pgdata_dev_main
  pgadmin_data_dev: # Added for pgAdmin dev persistence
    name: easyopen_pgadmin_data_dev_main

networks:
  easyopen_dev_network: # Renamed for development
    name: easyopen_development_net
    driver: bridge


<./docker-compose.yml>
# PRODUCTION docker-compose.yml
# This file defines the production environment.
# Use `docker-compose -f docker-compose.yml up --build -d` to run.

services:
  easyopen_postgis:
    container_name: easyopen_postgis_prod # Prod specific name
    image: postgis/postgis:17-3.5
    restart: always
    # ports: # Avoid exposing DB port directly in production unless absolutely necessary
    #   - "8735:5432"
    environment:
      POSTGRES_PASSWORD: ${DATABASE_PASSWORD:?err_DATABASE_PASSWORD_not_set}
      POSTGRES_USER: ${DATABASE_USER:-postgres}
      POSTGRES_DB: ${DATABASE_NAME:-easyopendata_database}
    volumes:
      - ./data/postgres_data_prod:/var/lib/postgresql/data # Prod specific volume
      - ./backend/db/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$DATABASE_NAME"]
      interval: 10s
      timeout: 5s
      retries: 20
    networks:
      - easyopen_prod_network

  easyopen_backend:
    container_name: easyopen_backend_prod # Prod specific name
    build:
      context: ./backend
      dockerfile: Dockerfile # Assumes this Dockerfile is suitable for prod (no dev tools)
    command: uvicorn app.main:app --host 0.0.0.0 --port 5400 # NO --reload for production
    volumes:
      # Code is built into the image for production. Mount only persistent data.
      - ./data/tileset:/data/tileset # Path for tileset data
      - ./data/tempfiles:/data/tempfiles # Path for temporary OBJ files
    ports:
      - "5400:5400"
    environment:
      DATABASE_URL: postgresql+asyncpg://${DATABASE_USER:-postgres}:${DATABASE_PASSWORD}@easyopen_postgis:5432/${DATABASE_NAME:-easyopendata_database}
      STRIPE_SECRET_KEY: ${VITE_STRIPE_SECRET_KEY:?err_VITE_STRIPE_SECRET_KEY_not_set} # Use actual prod key here via .env
      FRONTEND_URL: ${FRONTEND_URL}
      PYTHONUNBUFFERED: "1"
      LOG_LEVEL: "INFO"
      ENVIRONMENT: "production"
    depends_on:
      easyopen_postgis:
        condition: service_healthy
    networks:
      - easyopen_prod_network

  easyopen_frontend:
    container_name: easyopen_frontend_prod # Prod specific name
    build:
      context: ./frontend
      dockerfile: Dockerfile.production # Use the new production Dockerfile
      args: # Pass build arguments needed by Dockerfile.production
        VITE_BASE_URL: ${VITE_BASE_URL}
        VITE_TILESET_URL: ${VITE_TILESET_URL}
        VITE_STRIPE_PUBLISHABLE_KEY: ${VITE_STRIPE_PUBLISHABLE_KEY} # Use actual prod key here via .env
    ports:
      - "5173:80" # Nginx in Dockerfile.production will serve on port 80
    # No volumes for code/node_modules needed as it's a static build served by Nginx
    # Environment variables here are for Nginx if needed, Vite vars are build-time.
    networks:
      - easyopen_prod_network

  easyopen_tileset: # Tileset server remains largely the same
    container_name: easyopen_tileset_prod # Prod specific name
    image: nginx:alpine
    restart: always
    ports:
      - "5576:80"
    volumes:
      - ./data/tileset:/usr/share/nginx/html:ro # Serve from the main data directory
      - ./backend/tileset.conf:/etc/nginx/conf.d/default.conf:ro
    networks:
      - easyopen_prod_network

  easyopen_backup_service: # Backup service remains largely the same
    container_name: easyopen_backup_service_prod # Prod specific name
    build: ./backend/db_backup
    depends_on:
      - easyopen_postgis
    volumes:
      - ./data/postgres_backups:/backups
    environment:
      PGHOST: easyopen_postgis # Explicitly set PGHOST for pg_dump
      PGUSER: ${DATABASE_USER:-postgres}
      PGDATABASE: ${DATABASE_NAME:-easyopendata_database}
      PGPASSWORD: ${DATABASE_PASSWORD} # PGPASSWORD is used by pg_dump
    networks:
      - easyopen_prod_network

  # PGAdmin and Cloudflared are often for dev/staging or specific prod needs.
  # Keep them if needed for production, otherwise they might be in docker-compose.dev.yml only.
  easyopen_pgadmin:
    container_name: easyopen_pgadmin_prod # Prod specific name
    image: dpage/pgadmin4
    restart: always
    ports:
      - "5050:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD}
    volumes: # Add a volume for pgAdmin data persistence
      - ./data/pgadmin_data_prod:/var/lib/pgadmin
    depends_on:
      - easyopen_postgis
    networks:
      - easyopen_prod_network

  easyopen_cloudflared: # Assumed needed for production as well
    container_name: easyopen_cloudflared_prod # Prod specific name
    image: cloudflare/cloudflared:latest
    restart: always
    command: 'tunnel --no-autoupdate run --token ${CLOUDFLARED_TUNNEL_TOKEN:?err_CLOUDFLARED_TOKEN_not_set}'
    depends_on: # Ensure backend/frontend are up if tunnel points to them via localhost mapping
      - easyopen_backend # Or whichever service the tunnel primarily serves
      - easyopen_frontend
    networks:
      - host # Or a specific network if tunneling internal services by name

volumes:
  postgres_data_prod: # Renamed for production
    name: easyopen_pgdata_prod_main
  pgadmin_data_prod: # Added for pgAdmin persistence
    name: easyopen_pgadmin_data_prod_main

networks:
  easyopen_prod_network: # Renamed for production
    name: easyopen_production_net
    driver: bridge


<./README.md>
# EasyOpenData

## Open Data Extractor for German Spatial Datasets

### Overview

EasyOpenData is a platform that provides an easy-to-use interface for accessing and downloading spatial data from German open data sources, covering all Bundesländer. It standardizes the diverse formats in which these datasets are available and presents them to users via an intuitive web interface.

Users can interact with a map, select areas of interest (via polygons), and choose the data layers they wish to download. Available data types include:

- **3D Buildings (LOD1/LOD2)**

The platform processes the selected data and provides it in the user's desired format after payment, either as a direct download or via email.

---

### Key Features

- **Map-Based User Interface**: Allows users to interact with spatial data visually by selecting areas on a map.
- **Multi-Format Support**: Data is standardized into a unified format, enabling seamless querying and export.
- **Dynamic Data Processing**: Fetches and processes data only for the area selected by the user, ensuring efficiency.
- **Real-Time Visualization**: Users can preview data layers on the map as they select their areas of interest.
- **Payment Integration**: Secure payment processing via Stripe.
- **Scalable Backend**: Built for performance, capable of handling large datasets from multiple regions.

---

### Technologies Used

#### Backend

- **PostGIS**: Spatial database for storing and querying geographic data efficiently.
- **Python FastAPI**: Backend framework for building APIs to handle user requests and interact with PostGIS.
- **GDAL**: Used for data conversion between formats.
- **Docker**: Containerized deployment for portability and scalability.
- **Stripe API**: For handling payment transactions.

#### Frontend

- **React**: JavaScript library for building user interfaces.
- **MapLibre GL JS**: Interactive map interface for visualization and area selection.
- **Deck.gl**: For rendering 3D data on the map.
- **Mapbox Draw**: Allows users to draw shapes on the map.

---

### Getting Started

#### Prerequisites

- **Docker** and **Docker Compose** installed on your system.
- **Node.js** and **npm/yarn** (for frontend development).
- **Python 3.10+** (for backend development).

#### Setup Instructions

##### Clone the Repository

```bash
git clone https://github.com/your-repo/easyopendata.git
cd easyopendata
```

##### Environment Variables

Create a `.env` file in both the `backend` and `frontend` directories if needed to set environment variables.

##### Run with Docker Compose

```bash
docker-compose up --build
```

This command will:

- Start the PostGIS database.
- Build and run the backend FastAPI server.
- Build and run the frontend React application.

##### Access the Application

- **Frontend**: [http://localhost:5173](http://localhost:5173)
- **Backend API**: [http://localhost:5400](http://localhost:5400)

---

### Usage

1. **Open the Web Application**: Navigate to [http://localhost:5173](http://localhost:5173) in your browser.
2. **Select an Area**: Use the drawing tools to select an area on the map.
3. **Choose Data Layers**: Select the data layers you wish to download.
4. **Payment**: Proceed to payment if required.
5. **Download Data**: After processing, download your data in the desired format.

---

### Project Structure

- **backend/**: Contains the FastAPI backend application.
  - `app/`: FastAPI application code.
  - `db/`: Database initialization scripts.
  - `ingestion/`: Scripts for data ingestion and processing.
- **frontend/**: Contains the React frontend application.
  - `src/`: React application source code.
  - `public/`: Static assets.

---

### Contributing

Contributions are welcome! Please fork the repository and submit a pull request with your changes. Ensure all code adheres to the style guide and includes proper documentation.

---

### License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.

---

### Contact

For questions, feedback, or support, contact us at:

- **Email**: support@easyopendata.com

<./backend\Dockerfile>
# ./backend/Dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir -r /app/requirements.txt

COPY . /app

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]


<./backend\README.md>
# EasyOpenData Backend

## Overview

The backend of EasyOpenData is a FastAPI application that serves as the core of the platform, handling data retrieval, processing, and API endpoints for the frontend application. It interacts with a PostGIS database to store and query spatial data.

---

## Key Components

- **FastAPI Application**: Provides RESTful APIs for data retrieval and processing.
- **PostGIS Database**: Stores spatial data and provides efficient spatial queries.
- **Data Ingestion Scripts**: Scripts to download, transform, and ingest data into the database.
- **Payment Processing**: Integration with Stripe API for handling payments.

---

## Setup Instructions

### Prerequisites

- **Docker** and **Docker Compose**
- **Python 3.10+**
- **GDAL** and **OGR** libraries
- **Conda** (optional, for managing environments)
- **Node.js** and **npm** (for gltf-pipeline)

### Running with Docker

The easiest way to run the backend is using Docker Compose.

#### Build and Run

```bash
docker-compose up --build
```

This will start the PostGIS database and the FastAPI backend.

#### Accessing the Backend API

- The backend API will be available at: [http://localhost:5400](http://localhost:5400)

### Manual Setup

If you prefer to run the backend without Docker, follow these steps.

#### 1. Create a Virtual Environment

Using Conda:

```bash
conda create -n easyopendata_env python=3.10
conda activate easyopendata_env
```

#### 2. Install Dependencies

Install GDAL and its libraries:

```bash
conda install -c conda-forge gdal libgdal
```

Install other dependencies:

```bash
pip install -r backend/requirements.txt
```

#### 3. Install gltf-pipeline

```bash
npm install -g gltf-pipeline
```

#### 4. Setup the Database

Ensure PostgreSQL and PostGIS are installed.

```bash
sudo apt install postgresql postgis
```

Create the database and extensions:

```sql
CREATE DATABASE easyopendata_database;
\c easyopendata_database;
CREATE EXTENSION postgis;
```

#### 5. Run Database Initialization Scripts

```bash
psql -U postgres -d easyopendata_database -f backend/db/init.sql
```

#### 6. Run the FastAPI Application

```bash
cd backend
uvicorn app.main:app --reload
```

---

## Data Ingestion

The `backend/ingestion` directory contains scripts to download and process data.

### Ingesting Data

1. **Navigate to the ingestion directory**

```bash
cd backend/ingestion
```

2. **Run the Ingestion Script**

The `bayern.py` script processes Meta4 files to download and ingest data.

```bash
python bayern.py
```

**Note**: Ensure you have the necessary Meta4 files in `data_sources/` and that you have adjusted the `META4_PATH` in the script.

### Requirements

- **lxml**
- **psycopg2**
- **GDAL/OGR** command-line tools (`ogr2ogr`)
- **pg2b3dm** executable (must be in `libs/`)

---

## Directory Structure

- **app/**: FastAPI application code.
  - `main.py`: Main application file.
  - `database.py`: Database connection setup.
  - `models.py`: Database models and Pydantic schemas.
  - `retrieve_geom.py`: Functions to retrieve and process geometries.

- **db/**: Database scripts.
  - `init.sql`: Database initialization script.
  - `index.sql`: Indexing script for the database.

- **ingestion/**: Data ingestion scripts.
  - `bayern.py`: Script to process and ingest data.
  - `data_sources/`: Directory containing Meta4 files.
  - `data_local/`: Directory where downloaded data will be stored.
  - `libs/`: Contains necessary executables like `pg2b3dm.exe`.

---

## API Endpoints

- **GET /**: Root endpoint to check if the backend is running.
- **POST /retrieve_obj**: Accepts a GeoJSON region and returns an OBJ file with the buildings within that region.
- **POST /create-payment-intent**: Creates a Stripe payment intent for processing payments.

---

## Environment Variables

- **DATABASE_URL**: The connection string for the PostGIS database.
- **STRIPE_API_KEY**: Your Stripe secret API key for payment processing.

---

## Payment Processing

The backend integrates with Stripe to handle payments. Ensure you have set your Stripe API keys in the environment variables.

---

## Contributing

Contributions are welcome! Please ensure any changes to the backend code are thoroughly tested.

---

## License

This project is licensed under the MIT License.

<./backend\requirements.txt>
fastapi
uvicorn[standard]
asyncpg
psycopg2-binary
sqlalchemy
geoalchemy2
# Additional dependencies
python-multipart
geojson
anyio
pydantic
packaging
redis
stripe
pyproj
lxml
shapely

<./backend\tileset.conf>
server {
    listen 80;
    server_name localhost;

    root /usr/share/nginx/html;

    # Add the CORS header
    add_header 'Access-Control-Allow-Origin' '*' always;

    # If you want to allow specific headers or methods, add them too
    # add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS' always;
    # add_header 'Access-Control-Allow-Headers' 'Authorization, Content-Type' always;

    location / {
        try_files $uri $uri/ =404;
    }
}


<./backend\app\database.py>
import os
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from app.models import Base

DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql+asyncpg://postgres:barcelona@localhost:8735/easyopendata_database')

engine = create_async_engine(DATABASE_URL, echo=True)
async_session = sessionmaker(
    bind=engine, class_=AsyncSession, expire_on_commit=False
)

# Function to create tables
async def init_db():
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)



<./backend\app\main.py>
# ./backend/main.py

import asyncio
import sys  # For logging configuration
import tempfile
from typing import List
from urllib import request
import uuid
from fastapi import (
    FastAPI,
    Depends,
    HTTPException,
    BackgroundTasks,
)  # Added BackgroundTasks
from pydantic import BaseModel
from sqlalchemy.ext.asyncio import AsyncSession
import stripe
from app.database import async_session, init_db
from app.models import Building, RegionRequest
from app.retrieve_geom import retrieve_obj_file
from sqlalchemy.future import select
from geoalchemy2.functions import (
    ST_AsGeoJSON,
    ST_Intersects,
    ST_GeomFromText,
    ST_SimplifyPreserveTopology,
)
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
import json
import math

# import redis # Not used
# import subprocess # Not used
import os
import logging
from fastapi.staticfiles import StaticFiles

# import shutil # os.unlink is sufficient

# Configure Logging
log_level_name = os.getenv("LOG_LEVEL", "INFO").upper()  # Default to INFO
logging.basicConfig(
    stream=sys.stdout,
    level=getattr(logging, log_level_name, logging.INFO),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

app = FastAPI(title="EasyOpenData Backend API")

# Stripe API Key Initialization (done in on_startup)
# stripe.api_key = os.getenv('STRIPE_SECRET_KEY') # Moved to on_startup


# CORS Configuration
environment = os.getenv("ENVIRONMENT", "production")  # Default to production
frontend_url = os.getenv(
    "FRONTEND_URL", "http://localhost:5173"
)  # Default for dev if not set
vite_base_url_from_env = os.getenv(
    "VITE_BASE_URL"
)  # This is likely the backend's own URL for frontend, or frontend's deployed URL

# Define allowed origins based on environment
if environment == "development":
    # For dev, allow localhost with the typical dev frontend port, and potentially the configured FRONTEND_URL
    # The VITE_BASE_URL in .env is the *backend's* deployed URL, which the frontend uses to call the backend.
    # The FRONTEND_URL in .env is the *frontend's* deployed URL.
    # For CORS on the backend, we need to allow origins from where the frontend is served.
    dev_frontend_port = os.getenv(
        "DEV_FRONTEND_PORT", "5173"
    )  # Get from dev compose or default
    allowed_origins = [
        f"http://localhost:{dev_frontend_port}",
        f"http://127.0.0.1:{dev_frontend_port}",
        # If FRONTEND_URL is set and points to a dev instance, add it
    ]
    if frontend_url and "localhost" not in frontend_url:  # Avoid duplicate localhost
        allowed_origins.append(frontend_url)
    logger.info(f"CORS allowed origins for DEVELOPMENT: {allowed_origins}")
else:  # Production
    allowed_origins = []
    if frontend_url:  # This should be the production frontend URL
        allowed_origins.append(frontend_url)
    # If VITE_BASE_URL is meant to be a frontend origin (unlikely, usually backend's own base), add it.
    # It's more common that FRONTEND_URL is the one we need for CORS.
    # Example: ["https://easyopen.i-am-hellguz.uk"]
    if not allowed_origins:  # Fallback if FRONTEND_URL was not set for prod
        logger.warning(
            "Production FRONTEND_URL not set for CORS, this might cause issues."
        )
        # Add a placeholder or raise an error if critical
    logger.info(f"CORS allowed origins for PRODUCTION: {allowed_origins}")


app.add_middleware(
    CORSMiddleware,
    allow_origins=list(set(allowed_origins)),  # Use set to remove duplicates
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.on_event("startup")
async def on_startup():
    logger.info("Application starting up...")
    await init_db()
    stripe.api_key = os.getenv("STRIPE_SECRET_KEY")
    if not stripe.api_key:
        logger.warning(
            "STRIPE_SECRET_KEY environment variable is not set. Payment functionality will be disabled."
        )
    else:
        logger.info("Stripe API key configured.")

    # Ensure tempfiles directory exists (using the path from docker-compose volume)
    temp_dir = "/data/tempfiles"
    try:
        os.makedirs(temp_dir, exist_ok=True)
        logger.info(f"Temporary files directory '{temp_dir}' is ready.")
    except OSError as e:
        logger.error(f"Could not create or access temp directory '{temp_dir}': {e}")


async def get_db():  # Keep simple name as it's common FastAPI pattern
    async with async_session() as session:
        yield session


@app.get("/")
async def read_root():
    return {"message": "Easy Open Data Backend v1.0"}


async def _background_remove_temp_file(file_path: str):  # Helper for background task
    """Safely removes a file in the background."""
    try:
        if os.path.exists(file_path):
            os.unlink(file_path)
            logger.info(f"Background task: Successfully removed temp file: {file_path}")
        # else:
        # logger.debug(f"Background task: Temp file not found for removal (already deleted?): {file_path}")
    except Exception as e:
        logger.error(
            f"Background task: Error removing temp file {file_path}: {str(e)}",
            exc_info=True,
        )


@app.post("/retrieve_obj")
async def retrieve_obj(
    request: RegionRequest, background_tasks: BackgroundTasks
):  # Added BackgroundTasks
    logger.info(
        f"Received /retrieve_obj request."
    )  # Avoid logging request body for PII/size
    temp_file_path = None
    try:
        random_filename = f"{uuid.uuid4()}.obj"
        # Path corresponds to the volume mount in docker-compose.yml for easyopen_backend
        temp_dir_path = "/data/tempfiles"

        if not os.path.isdir(temp_dir_path):  # Make sure it exists
            logger.error(
                f"Temp directory '{temp_dir_path}' does not exist. Check volume mounts."
            )
            raise HTTPException(
                status_code=500,
                detail="Server-side temporary storage misconfiguration.",
            )

        temp_file_path = os.path.join(temp_dir_path, random_filename)

        await retrieve_obj_file(
            request.region, temp_file_path
        )  # Assumes retrieve_obj_file handles its own DB session
        logger.info(f"OBJ file generated at: {temp_file_path}")

        # Schedule the temp file for deletion AFTER the response is sent
        background_tasks.add_task(_background_remove_temp_file, temp_file_path)

        return FileResponse(
            temp_file_path,
            media_type="application/octet-stream",
            filename=f"easyopendata_export_{uuid.uuid4().hex[:6]}.obj",  # Shorter unique part
        )
    except ValueError as ve:  # Specific error from retrieve_obj_file
        logger.warning(f"Value error during OBJ retrieval: {str(ve)}")
        if temp_file_path and os.path.exists(temp_file_path):
            background_tasks.add_task(
                _background_remove_temp_file, temp_file_path
            )  # Cleanup if created
        raise HTTPException(status_code=400, detail=str(ve))
    except Exception as e:
        logger.error(f"General error in /retrieve_obj: {str(e)}", exc_info=True)
        if temp_file_path and os.path.exists(temp_file_path):
            background_tasks.add_task(
                _background_remove_temp_file, temp_file_path
            )  # Cleanup if created
        raise HTTPException(
            status_code=500,
            detail="An unexpected error occurred while processing your request.",
        )


# Removed original `async def remove_temp_file(file_path: str):` as it's replaced by the background task


def _calculate_order_amount_cents(amount_euros: float) -> int:  # Internal helper
    """Converts amount from euros to cents, ensuring integer."""
    return int(round(amount_euros * 100))


class PaymentIntentRequest(BaseModel):
    amount: float  # Amount in Euros, e.g., 10.50 for €10.50


@app.post("/create-payment-intent")
async def create_payment_intent(data: PaymentIntentRequest):
    if not stripe.api_key:
        logger.error("Stripe API key not set. Cannot create payment intent.")
        raise HTTPException(
            status_code=503, detail="Payment processing is currently unavailable."
        )

    if data.amount <= 0:
        logger.warning(f"Invalid payment amount received: {data.amount}")
        raise HTTPException(
            status_code=400, detail="Payment amount must be a positive value."
        )

    amount_in_cents = _calculate_order_amount_cents(data.amount)
    logger.info(
        f"Creating payment intent for {data.amount:.2f} EUR ({amount_in_cents} cents)."
    )
    try:
        intent = stripe.PaymentIntent.create(
            amount=amount_in_cents,
            currency="eur",
            automatic_payment_methods={"enabled": True},
        )
        return {"clientSecret": intent.client_secret}
    except stripe.error.StripeError as se:
        logger.error(f"Stripe API error: {str(se)}", exc_info=True)
        # Provide a user-friendly message if available from Stripe, otherwise a generic one.
        user_msg = getattr(
            se, "user_message", "A problem occurred with the payment processor."
        )
        raise HTTPException(
            status_code=getattr(se, "http_status", 500), detail=user_msg
        )
    except Exception as e:
        logger.error(
            f"Unexpected error creating payment intent: {str(e)}", exc_info=True
        )
        raise HTTPException(
            status_code=500,
            detail="Could not create payment intent due to an internal server error.",
        )


# The static files for tilesets are now served by a dedicated Nginx container (easyopen_tileset)
# So, the backend no longer needs to mount/serve them directly.
# If you had specific backend logic that interacted with files in `../data/tileset`
# that logic would now use the path `/data/tileset` (as mounted in backend's docker-compose).
# app.mount("/tileset", StaticFiles(directory="../data/tileset"), name="tileset") # This is removed


<./backend\app\models.py>
from sqlalchemy import Column, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from geoalchemy2 import Geometry

Base = declarative_base()

class Building(Base):
    __tablename__ = 'building'

    gml_id = Column(String, primary_key=True)
    name = Column(String)
    geom = Column(Geometry('MULTIPOLYGONZ', srid=4326))  # Adjust geometry type as needed

from pydantic import BaseModel

class RegionRequest(BaseModel):
    region: dict  # Adjust the type if you have a more specific structure


<./backend\app\retrieve_geom.py>
# ./backend/retrieve_obj.py

import asyncio
import json
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from geoalchemy2 import functions as func
from app.database import async_session
from app.models import Building
import os
from pyproj import Transformer

async def retrieve_obj_file(region_geojson, output_path):
    """
    Generates an OBJ file with buildings within the given polygon region.

    Args:
        region_geojson (dict): The GeoJSON representing the input region.
        output_path (str): The file path where the OBJ file will be saved.
    """
    # Parse the input GeoJSON to get the polygon geometry
    features = region_geojson.get('features', [])
    if not features:
        raise ValueError("No features found in the input GeoJSON.")
    
    polygon_feature = features[0]
    polygon_geometry = polygon_feature.get('geometry', {})
    if polygon_geometry.get('type') != 'Polygon':
        raise ValueError("The geometry must be of type 'Polygon'.")

    # Convert GeoJSON geometry to GeoJSON string
    polygon_geojson_str = json.dumps(polygon_geometry)

    # Choose the appropriate projection (e.g., UTM zone 32N for Germany)
    # You may need to adjust the EPSG code based on your location
    source_crs = 'EPSG:4326'  # WGS84 Latitude/Longitude
    target_crs = 'EPSG:25832'  # ETRS89 / UTM zone 32N (adjust as needed)

    # Create a Transformer object for coordinate transformation
    transformer = Transformer.from_crs(source_crs, target_crs, always_xy=True)

    # Clean and validate the user polygon once
    polygon = func.ST_SetSRID(func.ST_GeomFromGeoJSON(polygon_geojson_str), 4326)
    polygon = func.ST_MakeValid(polygon)
    polygon = func.ST_Force3D(polygon)
    polygon = func.ST_Buffer(polygon, 0)
    
    # Start an asynchronous database session
    async with async_session() as session:
        # Query the database for buildings within the polygon
        stmt = select(
            Building.gml_id,
            func.ST_AsGeoJSON(Building.geom).label('geom_geojson')
        ).where(
            func.ST_Intersects(
                func.ST_MakeValid(Building.geom),
                polygon
            )
        )
        
        result = await session.execute(stmt)
        buildings = result.fetchall()
        
        if not buildings:
            print("No buildings found within the given region.")
            return

        # Initialize lists to store OBJ data
        obj_vertices = []
        obj_faces = []
        vertex_offset = 0  # Offset for indexing vertices in faces

        # Process each building geometry
        for building in buildings:
            gml_id = building.gml_id
            geom_geojson_str = building.geom_geojson
            if not geom_geojson_str:
                continue  # Skip if geometry is null

            # Load geometry from GeoJSON
            geom_geojson = json.loads(geom_geojson_str)

            # Handle Polygon and MultiPolygon geometries
            geom_type = geom_geojson.get('type')
            coordinates = geom_geojson.get('coordinates')

            if geom_type == 'Polygon':
                polygons = [coordinates]
            elif geom_type == 'MultiPolygon':
                polygons = coordinates
            else:
                print(f"Skipping unsupported geometry type (ID: {gml_id}, Type: {geom_type})")
                continue

            for polygon in polygons:
                ring_vertex_indices = []

                # Process exterior ring
                exterior_coords = polygon[0]
                exterior_indices = []
                for coord in exterior_coords:
                    lon, lat = coord[:2]
                    z = coord[2] if len(coord) > 2 else 0
                    # Transform coordinates
                    x, y = transformer.transform(lon, lat)
                    obj_vertices.append(f"v {x} {z} {y}")
                    vertex_offset += 1
                    exterior_indices.append(vertex_offset)
                # Add face for exterior ring
                obj_faces.append(f"f {' '.join(map(str, exterior_indices))}")

                # Process interior rings (holes)
                for interior_coords in polygon[1:]:
                    interior_indices = []
                    for coord in interior_coords:
                        lon, lat = coord[:2]
                        z = coord[2] if len(coord) > 2 else 0
                        # Transform coordinates
                        x, y = transformer.transform(lon, lat)
                        obj_vertices.append(f"v {x} {z} {y}")
                        vertex_offset += 1
                        interior_indices.append(vertex_offset)
                    # Add face for interior ring (negative indices to denote holes are not standard in OBJ)
                    # Some software may not support holes directly
                    # So, we can skip adding faces for holes or handle them as separate objects
                    # For now, we'll skip adding faces for holes
                    print(f"Skipping interior ring (hole) in building ID: {gml_id}")

        # Write to OBJ file
        with open(output_path, 'w') as obj_file:
            obj_file.write("# OBJ file generated from buildings\n")
            obj_file.write("\n".join(obj_vertices))
            obj_file.write("\n")
            obj_file.write("\n".join(obj_faces))

        print(f"OBJ file successfully written to {output_path}")
        return


<./backend\db\index.sql>
-- Create the index if it doesn't exist
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 
        FROM pg_indexes 
        WHERE schemaname = 'public' 
        AND tablename = 'building' 
        AND indexname = 'idx_makevalid_geom'
    ) THEN
        CREATE INDEX buildings_geom_idx ON building USING GIST (ST_MakeValid(geom));
    END IF;
END $$;

-- Add the unique constraint if it doesn't exist
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 
        FROM pg_constraint 
        WHERE conname = 'gml_id_unique' 
        AND conrelid = 'public.building'::regclass
    ) THEN
        ALTER TABLE public.building ADD CONSTRAINT gml_id_unique UNIQUE (gml_id);
    END IF;
END $$;

-- ALTER TABLE building
-- ADD CONSTRAINT enforce_geotype_geom CHECK (GeometryType(geom) = 'GEOMETRYCOLLECTION' OR geom IS NULL);


-- Make all geometry entries valid or empty
-- If they aren't all MultiPolygonZ, convert them now
-- Turn off NOTICE output first and restore it after this operation

-- SET client_min_messages = WARNING;

-- UPDATE building
-- SET geom = ST_Force3D(ST_CollectionExtract(ST_MakeValid(geom), 3))
-- WHERE NOT ST_IsValid(geom) OR GeometryType(geom) != 'MULTIPOLYGONZ';

-- RESET client_min_messages;


-- Cluster the table using the index
-- CLUSTER building USING buildings_geom_idx;

-- Analyze to update statistics
ANALYZE building;


<./backend\db\init.sql>
-- Part 1: Database and extension creation (in transaction)
CREATE DATABASE easyopendata_database;
\c easyopendata_database;
CREATE EXTENSION IF NOT EXISTS postgis;

-- Part 2: System settings (must be outside transaction)
\connect easyopendata_database
\echo 'Setting system parameters...'

\set ON_ERROR_STOP off
ALTER SYSTEM SET maintenance_work_mem TO '1GB';
ALTER SYSTEM SET work_mem TO '256MB';
ALTER SYSTEM SET max_parallel_workers_per_gather TO 4;
\set ON_ERROR_STOP on



<./backend\db_backup\backup_script.sh>
#!/bin/bash

BACKUP_DIR=/backups
mkdir -p $BACKUP_DIR
export PGPASSWORD="barcelona"

# Create a new backup
NEW_BACKUP=$BACKUP_DIR/backup_$(date +%Y-%m-%d_%H-%M-%S).dump
pg_dump -U postgres -h easyopen_postgis -F c easyopendata_database > $NEW_BACKUP

# Check if the backup was successful
if [ $? -eq 0 ]; then
    echo "Backup created: $NEW_BACKUP"
else
    echo "Backup failed!" >&2
    exit 1
fi

# Delete old backups, keeping only the last 3
NUM_BACKUPS_TO_KEEP=5
BACKUP_COUNT=$(ls -1 $BACKUP_DIR | wc -l)

if [ $BACKUP_COUNT -gt $NUM_BACKUPS_TO_KEEP ]; then
    echo "Cleaning up old backups..."
    ls -1t $BACKUP_DIR | tail -n +$(($NUM_BACKUPS_TO_KEEP + 1)) | while read OLD_BACKUP; do
        rm -f "$BACKUP_DIR/$OLD_BACKUP"
        echo "Deleted old backup: $BACKUP_DIR/$OLD_BACKUP"
    done
fi

 # To restore, connect shell to easyopen_backup_service container and run
 # pg_restore -U postgres -h easyopen_postgis -d easyopendata_database --clean --if-exists -j 4 /backups/backup_2024-12-08_12-46-08.dump         

<./backend\db_backup\Dockerfile>
FROM postgres:17
RUN apt-get update && apt-get install -y cron && rm -rf /var/lib/apt/lists/*
COPY backup_script.sh /usr/local/bin/backup_script.sh
RUN chmod +x /usr/local/bin/backup_script.sh

# Add cron job
RUN echo "0 0 * * * root /usr/local/bin/backup_script.sh" >> /etc/crontab

CMD ["cron", "-f"]


<./backend\ingestion\bayern.py>
#!/usr/bin/env python3
"""
process_meta4.py

A script to sequentially download GML files from a Meta4 file, transform them by embedding polygons,
ingest them into a PostgreSQL database using a temporary table, convert them to 3D tiles,
append to the main building table, and remove the original files.

Usage:
    python process_meta4.py file.meta4

Requirements:
    - Python 3.x
    - lxml
    - psycopg2
    - ogr2ogr (from GDAL)
    - pg2b3dm_new command available in PATH
    - gltf-pipeline (for Draco compression)
"""

import sys
import os
import subprocess
import hashlib
import logging
import shutil
from urllib.parse import urlparse
from urllib.request import urlopen, Request
from urllib.error import URLError, HTTPError
from lxml import etree
import psycopg2

# Constants
META4_PATH = 'backend/ingestion/data_sources/bayern.meta4'
DATA_DIR = 'backend/ingestion/data_local/bayern'
DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql://postgres:barcelona@localhost:8735/easyopendata_database')
CACHE_DIR = 'data/tileset'
PG2B3DM_PATH = 'backend/ingestion/libs/pg2b3dm.exe'
SQL_INDEX_PATH = 'backend/db/index.sql'
TEMP_TABLE = 'idx_building'  # Temporary table name
MAIN_TABLE = 'building'      # Main building table name
BATCH_N = 20 # number of gml files for which there will be created a separate tileset

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

def parse_meta4(meta4_file):
    """
    Parses the Meta4 XML file and extracts file information.

    Args:
        meta4_file (str): Path to the Meta4 XML file.

    Returns:
        list of dict: List containing information about each file.
    """
    logging.info(f"Parsing Meta4 file: {meta4_file}")
    tree = etree.parse(meta4_file)
    root = tree.getroot()
    ns = {'metalink': 'urn:ietf:params:xml:ns:metalink'}

    files = []
    for file_elem in root.findall('metalink:file', namespaces=ns):
        name = file_elem.get('name')
        size = int(file_elem.find('metalink:size', namespaces=ns).text)
        hash_elem = file_elem.find('metalink:hash', namespaces=ns)
        hash_type = hash_elem.get('type')
        hash_value = hash_elem.text
        urls = [url_elem.text for url_elem in file_elem.findall('metalink:url', namespaces=ns)]
        files.append({
            'name': name,
            'size': size,
            'hash_type': hash_type,
            'hash_value': hash_value,
            'urls': urls
        })
    logging.info(f"Found {len(files)} files in Meta4.")
    return files

def download_file(url, dest_path):
    """
    Downloads a file from a URL to a destination path.

    Args:
        url (str): URL to download from.
        dest_path (str): Destination file path.

    Returns:
        bool: True if download was successful, False otherwise.
    """
    try:
        logging.info(f"Downloading from URL: {url}")
        headers = {'User-Agent': 'Mozilla/5.0'}
        req = Request(url, headers=headers)
        with urlopen(req) as response, open(dest_path, 'wb') as out_file:
            shutil.copyfileobj(response, out_file)
        logging.info(f"Downloaded file to: {dest_path}")
        return True
    except HTTPError as e:
        logging.warning(f"HTTP Error: {e.code} when downloading {url}")
    except URLError as e:
        logging.warning(f"URL Error: {e.reason} when downloading {url}")
    except Exception as e:
        logging.warning(f"Unexpected error when downloading {url}: {e}")
    return False

def verify_file(file_path, expected_size, expected_hash, hash_type='sha-256'):
    """
    Verifies the size and hash of a downloaded file.

    Args:
        file_path (str): Path to the file.
        expected_size (int): Expected file size in bytes.
        expected_hash (str): Expected hash value.
        hash_type (str): Hash algorithm, default 'sha-256'.

    Returns:
        bool: True if verification succeeds, False otherwise.
    """
    logging.info(f"Verifying file: {file_path}")
    # Check size
    actual_size = os.path.getsize(file_path)
    if actual_size != expected_size:
        logging.error(f"Size mismatch for {file_path}: expected {expected_size}, got {actual_size}")
        return False
    # Check hash
    hash_func = hashlib.new(hash_type)
    with open(file_path, 'rb') as f:
        for chunk in iter(lambda: f.read(8192), b''):
            hash_func.update(chunk)
    actual_hash = hash_func.hexdigest()
    if actual_hash.lower() != expected_hash.lower():
        logging.error(f"Hash mismatch for {file_path}: expected {expected_hash}, got {actual_hash}")
        return False
    logging.info(f"Verification passed for {file_path}")
    return True

def get_all_namespaces(gml_tree):
    """
    Extracts all namespaces from the GML tree and assigns a unique prefix to the default namespace.

    Args:
        gml_tree (etree.ElementTree): Parsed GML tree.

    Returns:
        dict: Namespace prefix to URI mapping.
    """
    nsmap = gml_tree.getroot().nsmap.copy()
    # Handle default namespace (None key)
    if None in nsmap:
        nsmap['default'] = nsmap.pop(None)
    # Ensure 'xlink' is included
    if 'xlink' not in nsmap:
        # Attempt to find the xlink namespace
        for prefix, uri in nsmap.items():
            if uri == 'http://www.w3.org/1999/xlink':
                nsmap['xlink'] = uri
                break
        else:
            # If not found, add it manually
            nsmap['xlink'] = 'http://www.w3.org/1999/xlink'
    return nsmap

def transform_gml(input_file, output_file):
    """
    Transforms the input GML file by embedding polygons into surfaceMember elements.

    Args:
        input_file (str): Path to the input GML file.
        output_file (str): Path to the output transformed GML file.
    """
    # Parse the GML file
    logging.info(f"Parsing input GML file: {input_file}")
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(input_file, parser)
    root = tree.getroot()

    # Extract all namespaces
    namespaces = get_all_namespaces(tree)
    # logging.debug("Namespaces detected:")
    # for prefix, uri in namespaces.items():
    #     logging.debug(f"  Prefix: '{prefix}' => URI: '{uri}'")

    # Build a dictionary of gml:id to Polygon elements for quick lookup
    logging.info("Indexing all <gml:Polygon> elements by gml:id...")
    polygon_dict = {}
    for polygon in root.xpath('.//gml:Polygon', namespaces=namespaces):
        polygon_id = polygon.get('{http://www.opengis.net/gml}id')
        if polygon_id:
            polygon_dict[polygon_id] = polygon
    logging.info(f"Indexed {len(polygon_dict)} polygons.")

    # Find all <gml:surfaceMember> elements with xlink:href
    logging.info("Finding all <gml:surfaceMember> elements with xlink:href...")
    surface_members = root.xpath('.//gml:surfaceMember[@xlink:href]', namespaces=namespaces)
    logging.info(f"Found {len(surface_members)} <gml:surfaceMember> elements with xlink:href.")

    for sm in surface_members:
        href = sm.get('{http://www.w3.org/1999/xlink}href')
        if not href:
            continue
        # Extract the referenced polygon ID (remove the '#' prefix)
        polygon_id = href.lstrip('#')
        # logging.debug(f"Processing surfaceMember referencing Polygon ID: {polygon_id}")
        polygon = polygon_dict.get(polygon_id)
        if not polygon:
            logging.warning(f"Polygon with gml:id='{polygon_id}' not found. Skipping.")
            continue
        # Deep copy the polygon element
        polygon_copy = etree.fromstring(etree.tostring(polygon))
        # Remove any existing 'gml:id' to avoid duplicate IDs
        polygon_copy.attrib.pop('{http://www.opengis.net/gml}id', None)
        # Replace the surfaceMember's xlink:href attribute with the actual Polygon
        sm.clear()  # Remove existing children and attributes
        sm.append(polygon_copy)
        # logging.debug(f"Embedded Polygon ID: {polygon_id} into surfaceMember.")

    # Optionally, remove standalone <gml:Polygon> elements that were referenced
    # logging.info("Removing standalone <gml:Polygon> elements that were referenced...")
    # removed_count = 0
    # for polygon_id in polygon_dict.keys():
    #     # Find and remove the standalone polygon
    #     polygons_to_remove = root.xpath(f'.//gml:Polygon[@gml:id="{polygon_id}"]', namespaces=namespaces)
    #     for polygon in polygons_to_remove:
    #         parent = polygon.getparent()
    #         if parent is not None:
    #             parent.remove(polygon)
    #             removed_count += 1
    #             logging.debug(f"Removed standalone Polygon ID: {polygon_id}.")
    # logging.info(f"Removed {removed_count} standalone polygons.")

    # Write the transformed GML to the output file
    logging.info(f"Writing transformed GML to: {output_file}")
    tree.write(output_file, pretty_print=True, xml_declaration=True, encoding='UTF-8')
    logging.info("Transformation complete.")

def ingest_gml_file(gml_file, database_url, table_name):
    """
    Ingests a GML file into a PostgreSQL database using ogr2ogr into a specified table.

    Args:
        gml_file (str): Path to the GML file.
        database_url (str): PostgreSQL connection URL.
        table_name (str): Target table name for ingestion.
    """
    logging.info(f"Ingesting GML file into database table '{table_name}': {gml_file}")
    cmd = [
        'ogr2ogr',
        '-f', 'PostgreSQL',
        '-nln', table_name,              # Specify the target table name
        '-progress',
        '-lco', 'GEOMETRY_NAME=geom',
        '-skipfailures',
        '-nlt', 'MULTIPOLYGONZ',
        '-dim', 'XYZ',
        '-s_srs', 'EPSG:25832',
        '-t_srs', 'EPSG:4326',
        database_url,
        gml_file
    ]
    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    if result.returncode != 0:
        logging.error(f"ogr2ogr failed for {gml_file}: {result.stderr}")
        raise RuntimeError(f"ogr2ogr failed: {result.stderr}")
    logging.info(f"Ingested {gml_file} into table '{table_name}' successfully.")

def execute_sql_file(sql_file_path, database_url):
    """Executes a SQL file in the database."""
    logging.info(f"Executing SQL file: {sql_file_path}")
    url = urlparse(database_url)
    conn = psycopg2.connect(
        dbname=url.path[1:],
        user=url.username,
        password=url.password,
        host=url.hostname,
        port=url.port
    )
    try:
        with conn.cursor() as cur:
            with open(sql_file_path, 'r') as f:
                cur.execute(f.read())
            conn.commit()
        logging.info("SQL file executed successfully")
    except Exception as e:
        logging.error(f"Failed to execute SQL file: {e}")
        conn.rollback()
        raise
    finally:
        conn.close()

def update_geometries(database_url, table_name):
    """
    Updates the geometries in the specified table to put them on ground level.

    Args:
        database_url (str): PostgreSQL connection URL.
        table_name (str): Table to update.
    """
    logging.info(f"Updating geometries to ground level in table '{table_name}'.")
    url = urlparse(database_url)
    conn = psycopg2.connect(
        dbname=url.path[1:],
        user=url.username,
        password=url.password,
        host=url.hostname,
        port=url.port
    )
    try:
        with conn.cursor() as cur:
            cur.execute(f"""
                UPDATE {table_name}
                SET geom = ST_Translate(geom, 0, 0, -ST_ZMin(geom))
                WHERE ST_ZMin(geom) != 0;
            """)
            conn.commit()
        logging.info(f"Geometries in table '{table_name}' updated successfully.")
    except Exception as e:
        logging.error(f"Failed to update geometries in table '{table_name}': {e}")
        conn.rollback()
        raise
    finally:
        conn.close()

def convert_to_3d_tiles(cache_dir, database_url, table_name):
    """
    Converts buildings from the specified table in the database to 3D tiles using pg2b3dm.

    Args:
        cache_dir (str): Output directory for 3D tiles.
        database_url (str): PostgreSQL connection URL.
        table_name (str): Table to convert to 3D tiles.
    """
    logging.info(f"Converting table '{table_name}' to 3D tiles with pg2b3dm.")
    # Parse the database URL for parameters
    url = urlparse(database_url)
    dbname = url.path[1:]
    user = url.username
    host = url.hostname or 'localhost'
    port = url.port
    # Assume password is handled via environment or .pgpass
    cmd = [
        PG2B3DM_PATH,
        '-h', f"{host}:{port}",
        '-U', user,
        '-c', 'geom',
        '-t', table_name,
        '-d', dbname,
        '-o', cache_dir, 
         '--use_implicit_tiling', 'false'  # Uncomment if needed
    ]
    # To handle password, set PGPASSWORD environment variable if available
    env = os.environ.copy()
    if url.password:
        env['PGPASSWORD'] = url.password
    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, env=env)
    if result.returncode != 0:
        logging.error(f"pg2b3dm failed: {result.stderr}")
        raise RuntimeError(f"pg2b3dm failed: {result.stderr}")
    logging.info("3D tiles generated successfully.")

def apply_draco_compression(cache_dir):
    """
    Applies Draco compression to all .glb files in the specified directory.

    Args:
        cache_dir (str): Directory containing .glb files.
    """
    logging.info("Applying Draco compression to glTF files.")
    for root, dirs, files in os.walk(cache_dir):
        for file in files:
            if file.endswith('.glb'):
                gltf_file = os.path.join(root, file)

                # Check if the first line of the file contains "draco"
                try:
                    with open(gltf_file, 'rb') as f:
                        first_line = f.readline().decode('utf-8', errors='ignore')
                        if "draco" in first_line.lower():
                            logging.info(f"File {gltf_file} already contains Draco; skipping compression.")
                            continue
                except Exception as e:
                    logging.error(f"Error reading file {gltf_file}: {e}")
                    continue

                # Proceed with Draco compression
                compressed_file = os.path.join(root, f"{os.path.splitext(file)[0]}_draco.glb")
                cmd = [
                    "gltf-pipeline",
                    '-i', gltf_file,
                    '-o', compressed_file,
                    '--draco.compressionLevel', '7'
                ]
                print(" ".join(cmd))
                result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                if result.returncode != 0:
                    logging.error(f"Draco compression failed for {gltf_file}: {result.stderr}")
                else:
                    os.replace(compressed_file, gltf_file)
                    logging.info(f"Applied Draco compression to {gltf_file}")
                    
def append_temp_to_main(database_url, temp_table, main_table):
    """
    Appends data from the temporary table to the main table by copying all columns.
    If the main table does not have certain columns, they will be created.
    Handles duplicates by ignoring records that violate primary key constraints.

    Args:
        database_url (str): PostgreSQL connection URL.
        temp_table (str): Temporary table name.
        main_table (str): Main table name.
    """
    logging.info(f"Appending data from '{temp_table}' to '{main_table}'.")
    url = urlparse(database_url)
    conn = psycopg2.connect(
        dbname=url.path[1:],
        user=url.username,
        password=url.password,
        host=url.hostname,
        port=url.port
    )

    try:
        with conn.cursor() as cur:
            # Fetch main table columns
            cur.execute(f"""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_name = '{main_table}'
                ORDER BY ordinal_position;
            """)
            main_columns = [row[0] for row in cur.fetchall()]

            # Fetch temp table columns and their data types
            cur.execute(f"""
                SELECT column_name, data_type
                FROM information_schema.columns
                WHERE table_name = '{temp_table}'
                ORDER BY ordinal_position;
            """)
            temp_columns_info = cur.fetchall()
            temp_columns = [row[0] for row in temp_columns_info]

            # Add missing columns to main_table
            for col_name, data_type in temp_columns_info:
                if col_name not in main_columns:
                    logging.info(f"Column '{col_name}' does not exist in '{main_table}'. Adding it.")
                    # Add the column with the same data_type as in temp_table
                    # Note: For complex types or special columns, you may need a more robust mapping.
                    alter_sql = f'ALTER TABLE "{main_table}" ADD COLUMN "{col_name}" {data_type};'
                    cur.execute(alter_sql)
                    main_columns.append(col_name)
                    logging.info(f"Column '{col_name}' added to '{main_table}'.")

            # Now all temp_columns should exist in main_table
            # We will insert all columns from temp_table to main_table
            columns_str = ', '.join([f'"{col}"' for col in temp_columns])

            # Fetch primary key columns from the main table
            cur.execute(f"""
                SELECT a.attname
                FROM pg_index i
                JOIN pg_attribute a ON a.attrelid = i.indrelid
                                     AND a.attnum = ANY(i.indkey)
                WHERE i.indrelid = '{main_table}'::regclass
                  AND i.indisprimary;
            """)
            pk_columns = [row[0] for row in cur.fetchall()]
            if not pk_columns:
                raise ValueError(f"No primary key defined for table '{main_table}'.")

            # Construct the ON CONFLICT clause
            conflict_target = ', '.join([f'"{col}"' for col in pk_columns])
            on_conflict_clause = f"ON CONFLICT ({conflict_target}) DO NOTHING"

            logging.info(f"Using ON CONFLICT clause on columns: {conflict_target}")

            # Execute the INSERT statement with ON CONFLICT
            insert_sql = f"""
                INSERT INTO "{main_table}" ({columns_str})
                SELECT {columns_str} FROM "{temp_table}"
                {on_conflict_clause};
            """
            cur.execute(insert_sql)

            inserted_count = cur.rowcount
            conn.commit()
            logging.info(f"Data appended from '{temp_table}' to '{main_table}' successfully. Inserted {inserted_count} records.")

    except Exception as e:
        logging.error(f"Failed to append data from '{temp_table}' to '{main_table}': {e}")
        conn.rollback()
        raise
    finally:
        conn.close()



def drop_temp_table(database_url, temp_table):
    """
    Drops the temporary table from the database.

    Args:
        database_url (str): PostgreSQL connection URL.
        temp_table (str): Temporary table name.
    """
    logging.info(f"Dropping temporary table '{temp_table}'.")
    url = urlparse(database_url)
    conn = psycopg2.connect(
        dbname=url.path[1:],
        user=url.username,
        password=url.password,
        host=url.hostname,
        port=url.port
    )
    try:
        with conn.cursor() as cur:
            cur.execute(f"DROP TABLE IF EXISTS {temp_table};")
            conn.commit()
        logging.info(f"Temporary table '{temp_table}' dropped successfully.")
    except Exception as e:
        logging.error(f"Failed to drop temporary table '{temp_table}': {e}")
        conn.rollback()
        raise
    finally:
        conn.close()

def remove_file(file_path):
    """
    Removes a file from the filesystem.

    Args:
        file_path (str): Path to the file.
    """
    try:
        os.remove(file_path)
        logging.info(f"Removed file: {file_path}")
    except OSError as e:
        logging.warning(f"Failed to remove file {file_path}: {e}")

import os
import json
import math

def merge_tilesets_into_one(output_path, input_tilesets):
    """
    Merges multiple region-based tilesets into a single tileset.json that references all of them as external.
    If some tilesets do not exist or do not have a region boundingVolume, they are skipped.
    If no valid tilesets remain, creates a minimal tileset with no children.
    
    Args:
        output_path (str): Path to the final merged tileset.json output file.
        input_tilesets (list[str]): Paths to the input tileset.json files to merge.

    Returns:
        None. Writes the merged tileset.json to output_path.
    """
    
    # Load all valid tilesets
    loaded_tilesets = []
    for ts_path in input_tilesets:
        if not os.path.isfile(ts_path):
            # Skip if the file doesn't exist
            continue
        try:
            with open(ts_path, 'r', encoding='utf-8') as f:
                ts = json.load(f)
                loaded_tilesets.append((ts_path, ts))
        except (IOError, json.JSONDecodeError):
            # Skip if the file cannot be read or is not valid JSON
            continue
    
    # Filter down to only those with a region boundingVolume
    all_regions = []
    valid_tilesets = []
    for ts_path, ts in loaded_tilesets:
        root = ts.get("root", {})
        bv = root.get("boundingVolume", {})
        region = bv.get("region")
        
        if region and isinstance(region, list) and len(region) == 6:
            all_regions.append(region)
            valid_tilesets.append((ts_path, ts))
        # If there's no valid region, skip this tileset
    
    if not valid_tilesets:
        # No valid tilesets found, create an empty tileset
        # with a minimal boundingVolume and no children.
        # We'll use a generic region that covers no area.
        # For example, we can pick a degenerate region:
        degenerate_region = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        empty_tileset = {
            "asset": {
                "version": "1.1"
            },
            "geometricError": 0,
            "root": {
                "boundingVolume": {
                    "region": degenerate_region
                },
                "refine": "ADD",
                "geometricError": 0,
                "children": []
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(empty_tileset, f, indent=2)
        print(f"No valid tilesets found. Created an empty merged tileset at {output_path}")
        return
    
    # Compute the encompassing region for all valid tilesets
    west = min(r[0] for r in all_regions)
    south = min(r[1] for r in all_regions)
    east = max(r[2] for r in all_regions)
    north = max(r[3] for r in all_regions)
    minH = min(r[4] for r in all_regions)
    maxH = max(r[5] for r in all_regions)
    merged_region = [west, south, east, north, minH, maxH]

    # Construct children for the merged tileset
    children = []
    output_dir = os.path.dirname(os.path.abspath(output_path))
    for ts_path, ts in valid_tilesets:
        ts_abs = os.path.abspath(ts_path)
        rel_path = os.path.relpath(ts_abs, output_dir)
        
        child = {
            "boundingVolume": ts["root"]["boundingVolume"],
            "geometricError": ts["root"]["geometricError"],
            "refine": ts["root"].get("refine", "ADD").upper(),
            "content": {
                "uri": rel_path
            }
        }
        children.append(child)

    # Determine the maximum geometricError for the parent tileset
    parent_geometric_error = max(ts["root"]["geometricError"] for _, ts in valid_tilesets)
    
    # Create the merged tileset JSON structure
    merged_tileset = { 
        "asset": {
            "version": "1.1"
        },
        "geometricError": parent_geometric_error,
        "root": {
            "boundingVolume": {
                "region": merged_region
            },
            "refine": "ADD",
            "geometricError": parent_geometric_error,
            "children": children
        }
    }

    # Write the merged tileset to disk
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(merged_tileset, f, indent=2)
    print(f"Merged tileset written to {output_path}")

def ensure_main_table_exists(database_url, table_name):
    """
    Ensures that the main table exists in the database. Creates it if it does not exist.

    Args:
        database_url (str): PostgreSQL connection URL.
        table_name (str): Name of the main table.
    """
    logging.info(f"Ensuring main table '{table_name}' exists.")
    url = urlparse(database_url)
    conn = psycopg2.connect(
        dbname=url.path[1:],
        user=url.username,
        password=url.password,
        host=url.hostname,
        port=url.port
    )
    try:
        with conn.cursor() as cur:
            # Check if the table exists
            cur.execute(f"""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables
                    WHERE table_name = '{table_name}'
                );
            """)
            exists = cur.fetchone()[0]

            if not exists:
                logging.info(f"Table '{table_name}' does not exist. Creating it.")
                cur.execute(f"""
                    CREATE TABLE {table_name} (
                        gml_id VARCHAR PRIMARY KEY,
                        geom GEOMETRY(GEOMETRYZ, 4326),
                        attributes JSONB
                    );
                """)
                conn.commit()
                logging.info(f"Table '{table_name}' created successfully.")
            else:
                logging.info(f"Table '{table_name}' already exists.")
    except Exception as e:
        logging.error(f"Failed to ensure table '{table_name}' exists: {e}")
        conn.rollback()
        raise
    finally:
        conn.close()


def main(meta4_file):
    # Ensure DATA_DIR and CACHE_DIR exist
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(CACHE_DIR, exist_ok=True)
    
    # Ensure the main table exists
    ensure_main_table_exists(DATABASE_URL, MAIN_TABLE)
    # execute_sql_file(SQL_INDEX_PATH, DATABASE_URL)

    
    # Parse the Meta4 file
    files = parse_meta4(meta4_file)

    # Drop the temporary table
    drop_temp_table(DATABASE_URL, TEMP_TABLE)

    for ix, file_info in enumerate(files):
        
        if ix < 1700:
           continue
        
        file_name = file_info['name']
        size = file_info['size']
        hash_type = file_info['hash_type']
        hash_value = file_info['hash_value']
        urls = file_info['urls']

        logging.info(f"▶️   FILE {ix+1}/{len(files)}")
        logging.info(f"Processing file: {file_name}")


        temp_tileset_dir = os.path.join(CACHE_DIR, 'sub', str(ix // BATCH_N))  
        main_tileset_dir = CACHE_DIR
           
        os.makedirs(main_tileset_dir, exist_ok=True)   
        os.makedirs(temp_tileset_dir, exist_ok=True)

        # Determine download paths
        download_path = os.path.join(DATA_DIR, file_name)
        transformed_file_name = os.path.splitext(file_name)[0] + '_trs.gml'
        transformed_path = os.path.join(DATA_DIR, transformed_file_name)

        # Download the file from available URLs
        downloaded = False
        for url in urls:
            if download_file(url, download_path):
                # Verify the file
                if verify_file(download_path, size, hash_value, hash_type):
                    downloaded = True
                    break
                else:
                    logging.warning(f"Verification failed for {download_path}. Trying next URL.")
                    remove_file(download_path)
        if not downloaded:
            logging.error(f"Failed to download and verify {file_name} from all URLs. Skipping.")
            continue

        try:
            # Transform the GML file
            transform_gml(download_path, transformed_path)

            # Ingest the transformed GML into the temporary table
            ingest_gml_file(transformed_path, DATABASE_URL, TEMP_TABLE)

            # Update geometries in the temporary table
            update_geometries(DATABASE_URL, TEMP_TABLE)



            if ix % BATCH_N == 0 or ix == len(files) - 1:            
                # Convert the temporary table to 3D tiles
                convert_to_3d_tiles(temp_tileset_dir, DATABASE_URL, TEMP_TABLE)

                # Apply Draco compression to the newly generated tiles
                apply_draco_compression(temp_tileset_dir)

                # Append data from temporary table to main table
                append_temp_to_main(DATABASE_URL, TEMP_TABLE, MAIN_TABLE)

                # Drop the temporary table
                drop_temp_table(DATABASE_URL, TEMP_TABLE)

                batch_count = ix // BATCH_N + 1

                # Collect all input tileset.json paths
                input_tileset_paths = [
                    os.path.join(CACHE_DIR, 'sub', str(b), 'tileset.json')
                    for b in range(batch_count)
                ]

                merged_tileset_path = os.path.join(CACHE_DIR, 'tileset.json')

                logging.info(f"Merging {len(input_tileset_paths)} tilesets into {merged_tileset_path}...")

                try:
                    # Call our custom merging function
                    merge_tilesets_into_one(merged_tileset_path, input_tileset_paths)
                    logging.info("Merged tileset into main tileset successfully.")
                except Exception as e:
                    logging.error(f"Failed to combine merged tilesets: {e}")
                    raise RuntimeError(f"Failed to combine merged tilesets: {e}")

            # npx 3d-tiles-tools combine -i backend/tileset\ -o backend/tileset_combined -f

            if ix == 0:
                # Execute SQL indexing file once before processing
                execute_sql_file(SQL_INDEX_PATH, DATABASE_URL)
                
            # Remove the transformed GML file
            remove_file(transformed_path)
            # Remove the transformed GFS file if it exists
            transformed_gfs_path = os.path.splitext(transformed_path)[0] + ".gfs"
            if os.path.isfile(transformed_gfs_path):
                remove_file(transformed_gfs_path)

            # Optionally, remove the original downloaded GML file
            remove_file(download_path)

            logging.info(f"✅  Completed processing for {file_name}")

        except Exception as e:
            logging.error(f"An error occurred while processing {file_name}: {e}")
            # Optionally, clean up files or continue
            continue

    logging.info("🏁 All files processed.")

if __name__ == '__main__':
    meta4_file = META4_PATH
    if not os.path.isfile(meta4_file):
        logging.error(f"Meta4 file '{meta4_file}' does not exist.")
        sys.exit(1)
    main(meta4_file)


<./backend\ingestion\README.md>
# EasyOpenData Data Ingestion

## Overview

The `ingestion` directory contains scripts and resources for downloading, transforming, and ingesting 3D building data into a PostGIS database. It also includes tools for generating and optimizing 3D tilesets from the database.

---

## Setup Instructions

### Prerequisites

- **Conda** (optional, for managing Python environments)
- **GDAL** and **OGR** libraries
- **Python 3.x**
- **Node.js** and **npm** (for `gltf-pipeline` and other utilities)
- **PostgreSQL with PostGIS extension**

---

### Steps

#### 1. Create a Conda Environment (Optional)

```bash
conda create -n easyopendata_env python=3.10
conda activate easyopendata_env
```

#### 2. Install GDAL and Libraries

```bash
conda install -c conda-forge gdal libgdal
```

#### 3. Install Python Dependencies

```bash
pip install -r ../../requirements.txt
```

#### 4. Install gltf-pipeline

```bash
npm install -g gltf-pipeline
```

#### 5. Set Up PostgreSQL Database

Ensure you have a PostgreSQL instance running with the PostGIS extension installed. Update the `DATABASE_URL` in the script to point to your database.

#### 6. Prepare Required Executables

- Place the `pg2b3dm` executable in the `libs/` directory.
- Ensure `ogr2ogr` is available in your system's `PATH`.

---

## Data Ingestion Script

### `process_meta4.py`

This script sequentially downloads GML files from a `.meta4` file, transforms them by embedding polygons into `surfaceMember` elements, ingests them into a PostgreSQL database using a temporary table, converts the data into 3D tiles, and merges them into a single tileset.

#### Workflow

1. **Parse Meta4**: Extract file metadata (URLs, hashes, etc.).
2. **Download**: Fetch files and verify size and hash integrity.
3. **Transform GML**: Embed referenced polygons into the GML structure.
4. **Ingest**: Load transformed data into a temporary PostgreSQL table.
5. **Update Geometries**: Align geometries to the ground level.
6. **Generate 3D Tiles**: Use `pg2b3dm` to create tilesets.
7. **Optimize with Draco Compression**: Compress `.glb` files to reduce size.
8. **Merge Tilesets**: Combine batch tilesets into a single `tileset.json`.
9. **Append Data**: Append data from the temporary table to the main table.
10. **Clean Up**: Remove processed files and drop temporary tables.

---

### Usage

```bash
python process_meta4.py file.meta4
```

#### Configuration Variables

- **`META4_PATH`**: Path to the `.meta4` file.
- **`DATABASE_URL`**: PostgreSQL connection string.
- **`CACHE_DIR`**: Directory for storing tileset outputs.
- **`DATA_DIR`**: Directory for storing temporary data files.
- **`TEMP_TABLE`**: Temporary table name.
- **`MAIN_TABLE`**: Main table name.

---

## Directory Structure

- **`data_sources/`**: Contains `.meta4` files listing URLs for GML data.
- **`data_local/`**: Directory for downloaded GML files.
- **`libs/`**: Contains utilities like `pg2b3dm`.
- **`tileset/`**: Directory for generated tilesets.

---

## Tools and Dependencies

- **GDAL/OGR**: Used for transforming and ingesting GML files.
- **PostGIS**: PostgreSQL extension for spatial data management.
- **pg2b3dm**: Converts PostGIS data to 3D tiles compatible with CesiumJS.
- **gltf-pipeline**: Optimizes and compresses 3D tiles with Draco compression.

---

## Performance Tips

- **Batch Size**: The `BATCH_N` parameter controls the number of GML files processed per batch. Adjust it based on available memory and performance needs.
- **Indexes**: Ensure database indexes are optimized for spatial operations by executing the provided `index.sql`.

---

## Troubleshooting
 
- **Failed Downloads**: Ensure URLs in the `.meta4` file are valid and reachable.
- **Database Errors**: Check that the database URL and permissions are correctly configured.
- **Script Errors**: Refer to the detailed logs for troubleshooting.


<./backend\ingestion\data_sources\bamberg.meta4>
<?xml version="1.0" encoding="UTF-8"?>
<metalink xmlns="urn:ietf:params:xml:ns:metalink">
  <generator>BVV-MetaLinker</generator>
  <published>2024-11-26T22:13:07Z</published>
  <file name="636_5524.g
................................

<./backend\ingestion\data_sources\bayern.meta4>
<?xml version="1.0" encoding="UTF-8"?>
<metalink xmlns="urn:ietf:params:xml:ns:metalink">
  <generator>BVV-MetaLinker</generator>
  <published>2024-11-19T22:09:01Z</published>
  <file name="792_5432.g
................................

<./backend\ingestion\data_sources\munchen.meta4>
<?xml version="1.0" encoding="UTF-8"?>
<metalink xmlns="urn:ietf:params:xml:ns:metalink">
  <generator>BVV-MetaLinker</generator>
  <published>2024-11-19T22:08:58Z</published>
  <file name="680_5342.g
................................

&&& FILE: ./backend\ingestion\libs\pg2b3dm.exe
&&& ERROR: Could not read file: 'utf-8' codec can't decode byte 0x90 in position 2: invalid start byte

<./frontend\.dockerignore>
node_modules


<./frontend\.gitignore>
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*

node_modules
dist
dist-ssr
*.local
.vite/*

# Editor directories and files
.vscode/*
!.vscode/extensions.json
.idea
.DS_Store
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

yarn.lock

<./frontend\Dockerfile>
FROM node:18-alpine

WORKDIR /app

# Copy package.json and yarn.lock first for dependency installation
COPY package.json ./

# Install dependencies
RUN yarn install 

# Copy the rest of the frontend code
COPY . .

EXPOSE 5173

CMD ["yarn", "dev", "--host", "0.0.0.0"]

<./frontend\Dockerfile.production>
# ./frontend/Dockerfile.production (For Nginx static serving)

# Stage 1: Build the frontend application
FROM node:18-alpine AS builder
LABEL stage="frontend-builder-prod"
WORKDIR /app

# Copy package.json and yarn.lock first for dependency caching
COPY package.json yarn.lock* ./

# Install dependencies
# --production=false ensures devDependencies are available if build scripts need them
RUN yarn install --frozen-lockfile --production=false --network-timeout 600000

# Copy the rest of the frontend code
COPY . .

# These ARGs are passed from `docker-compose.yml` build section
ARG VITE_BASE_URL
ARG VITE_TILESET_URL
ARG VITE_STRIPE_PUBLISHABLE_KEY

# Set them as ENV variables for Vite to pick up during the build process
ENV VITE_BASE_URL=${VITE_BASE_URL}
ENV VITE_TILESET_URL=${VITE_TILESET_URL}
ENV VITE_STRIPE_PUBLISHABLE_KEY=${VITE_STRIPE_PUBLISHABLE_KEY}

# Verify environment variables are set (optional, for debugging build)
RUN echo "VITE_BASE_URL: $VITE_BASE_URL"
RUN echo "VITE_TILESET_URL: $VITE_TILESET_URL"
RUN echo "VITE_STRIPE_PUBLISHABLE_KEY: $VITE_STRIPE_PUBLISHABLE_KEY"

# Build the production-ready static files
RUN yarn build

# Stage 2: Serve the built application using Nginx
FROM nginx:stable-alpine
LABEL stage="frontend-runner-prod"

# Remove the default Nginx static assets and config
RUN rm -rf /usr/share/nginx/html/*
RUN rm -f /etc/nginx/conf.d/default.conf

# Copy the built static files from the build stage to Nginx's html directory
COPY --from=builder /app/dist /usr/share/nginx/html

# Copy a custom nginx configuration.
# This file should be in the same directory as this Dockerfile (./frontend)
COPY nginx.conf /etc/nginx/conf.d/default.conf

# Nginx typically runs on port 80 by default inside the container.
# The `ports` mapping in docker-compose.yml (e.g., "5173:80") handles host exposure.
EXPOSE 80

# Command to run nginx in the foreground
CMD ["nginx", "-g", "daemon off;"]


<./frontend\eslint.config.js>
import js from '@eslint/js'
import globals from 'globals'
import react from 'eslint-plugin-react'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'

export default [
  { ignores: ['dist'] },
  {
    files: ['**/*.{js,jsx}'],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
      parserOptions: {
        ecmaVersion: 'latest',
        ecmaFeatures: { jsx: true },
        sourceType: 'module',
      },
    },
    settings: { react: { version: '18.3' } },
    plugins: {
      react,
      'react-hooks': reactHooks,
      'react-refresh': reactRefresh,
    },
    rules: {
      ...js.configs.recommended.rules,
      ...react.configs.recommended.rules,
      ...react.configs['jsx-runtime'].rules,
      ...reactHooks.configs.recommended.rules,
      'react/jsx-no-target-blank': 'off',
      'react-refresh/only-export-components': [
        'warn',
        { allowConstantExport: true },
      ],
    },
  },
]


<./frontend\index.html>
<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover">

  <title>EasyOpenData</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
  <meta name="theme-color" content="#eeeeee" />

  <link href="https://api.mapbox.com/mapbox-gl-js/v2.6.1/mapbox-gl.css" rel="stylesheet">
  <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/plugins/mapbox-gl-draw/v1.3.0/mapbox-gl-draw.css" type="text/css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
</head>

<body>
  <div id="root"></div>
  <script type="module" src="./src/index.tsx"></script>
</body>

</html>


<./frontend\nginx.conf>
# ./frontend/nginx.conf
# Nginx configuration for serving the production React SPA

server {
    listen 80; # Nginx listens on port 80 inside the container
    server_name _; # Catch-all server name

    # Root directory for your static React build
    root /usr/share/nginx/html;
    index index.html;

    # Serve static files directly
    location / {
        try_files $uri $uri/ /index.html; # Fallback to index.html for SPA routing
    }

    # Cache control for static assets (optional but good practice)
    location ~* \.(?:css|js|jpg|jpeg|gif|png|ico|svg|woff|woff2|ttf|eot|webmanifest|map)$ {
        expires 1y;
        add_header Cache-Control "public, immutable";
        access_log off; # Optional: reduce logging for static files
    }

    # Gzip compression (optional but good practice)
    gzip on;
    gzip_vary on;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types text/plain text/css application/json application/javascript application/xml application/x-font-ttf image/svg+xml;

    # Deny access to hidden files (e.g., .htaccess, .git)
    location ~ /\. {
        deny all;
    }
}

<./frontend\package.json>
{
  "name": "frontend",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "lint": "eslint .",
    "preview": "vite preview"

................................

<./frontend\README.md>
# EasyOpenData Frontend

## Overview

The frontend of EasyOpenData is a React application that provides an interactive map interface for users to select areas of interest, choose data layers, and download spatial data. It integrates with the backend API and handles payment processing via Stripe.

---

## Key Features

- **Interactive Map Interface**: Built with React and MapLibre GL JS for map rendering.
- **Drawing Tools**: Allows users to draw polygons to select regions.
- **3D Visualization**: Uses Deck.gl to render 3D buildings.
- **Search Functionality**: Integrated with Nominatim for location search.
- **Payment Processing**: Secure payments via Stripe.
- **Responsive Design**: Adapts to mobile and desktop screens.

---

## Setup Instructions

### Prerequisites

- **Node.js** and **npm** or **yarn**

### Installation

1. **Install Dependencies**

```bash
cd frontend
yarn install
# or
npm install
```

### Running the Development Server

```bash
yarn dev
# or
npm run dev
```

The application will be available at [http://localhost:5173](http://localhost:5173)

### Build for Production

```bash
yarn build
# or
npm run build
```

The production build will be in the `dist/` directory.

---

## Configuration

### Environment Variables

Create a `.env` file in the `frontend` directory to set environment variables.

- **VITE_BACKEND_URL**: The URL of the backend API.

Example `.env` file:

```
VITE_BACKEND_URL=http://localhost:5400
```

### Map Style

The map uses a custom style defined in `src/basemap.json`. You can customize the map style by modifying this file or replacing it with another style.

---

## Project Structure

- **src/**: Contains the source code of the React application.
  - `App.tsx`: Main application component.
  - `CheckoutForm.tsx`: Handles payment form and Stripe integration.
  - `FloatingPanel.tsx`: UI component for user interactions.
  - `Legals.tsx`: Displays legal documents.
  - `draw-control.ts`: Custom control for drawing polygons.
  - `basemap.json`: Custom map style.

- **public/**: Contains static assets.

---

## Key Components

- **Map Integration**: Uses MapLibre GL JS for map rendering and Mapbox Draw for drawing tools.
- **3D Rendering**: Deck.gl is used to render 3D buildings.
- **Payment Integration**: Stripe is integrated using `@stripe/react-stripe-js` and `@stripe/stripe-js`.
- **Legal Documents**: Legal documents are displayed using `ReactMarkdown`.

---

## Customization

- **Map Style**: Modify `src/basemap.json` to change the map appearance.
- **API Endpoints**: Ensure `VITE_BACKEND_URL` points to the correct backend API.
- **Stripe Keys**: Update the publishable key in `FloatingPanel.tsx` and ensure the backend has the correct secret key.

---

## Legal Documents

The `docs/` directory contains Markdown files for legal documents such as:

- `agb.md`: Terms and Conditions
- `datenschutz.md`: Privacy Policy
- `impressum.md`: Imprint
- `widerruf.md`: Cancellation Policy

These documents are displayed in the application using the `Legals.tsx` component.

---

## Contributing

Contributions are welcome! Please ensure any changes to the frontend code are thoroughly tested.

---

## License

This project is licensed under the MIT License.

<./frontend\tsconfig.json>
{
    "compilerOptions": {
      "target": "ESNext",
      "types": ["vite/client"],
      "jsx": "react",
      "moduleResolution": "node",
      "allowSyntheticDefaultImports": true
    }
  }
................................

<./frontend\vite.config.js>
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vite.dev/config/
export default defineConfig({
  server: {
    watch: {
      usePolling: true, // For Docker environments
    },
  },
  plugins: [react()],
})


<./frontend\public\basemap.json>
{
  "version": 8,
  "name": "Positron",
  "metadata": {
    "mapbox:autocomposite": false,
    "mapbox:groups": {
      "101da9f13b64a08fa4b6ac1168e89e5f": {
        "collapsed": false,
        "name"
................................

<./frontend\public\vite.svg>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="31.88" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 257"><defs><linearGradient id="IconifyId1813088fe1fbc01fb466" x1="-.828%" x2="57.636%" y1="7.652%" y2="78.411%"><stop offset="0%" stop-color="#41D1FF"></stop><stop offset="100%" stop-color="#BD34FE"></stop></linearGradient><linearGradient id="IconifyId1813088fe1fbc01fb467" x1="43.376%" x2="50.316%" y1="2.242%" y2="89.03%"><stop offset="0%" stop-color="#FFEA83"></stop><stop offset="8.333%" stop-color="#FFDD35"></stop><stop offset="100%" stop-color="#FFA800"></stop></linearGradient></defs><path fill="url(#IconifyId1813088fe1fbc01fb466)" d="M255.153 37.938L134.897 252.976c-2.483 4.44-8.862 4.466-11.382.048L.875 37.958c-2.746-4.814 1.371-10.646 6.827-9.67l120.385 21.517a6.537 6.537 0 0 0 2.322-.004l117.867-21.483c5.438-.991 9.574 4.796 6.877 9.62Z"></path><path fill="url(#IconifyId1813088fe1fbc01fb467)" d="M185.432.063L96.44 17.501a3.268 3.268 0 0 0-2.634 3.014l-5.474 92.456a3.268 3.268 0 0 0 3.997 3.378l24.777-5.718c2.318-.535 4.413 1.507 3.936 3.838l-7.361 36.047c-.495 2.426 1.782 4.5 4.151 3.78l15.304-4.649c2.372-.72 4.652 1.36 4.15 3.788l-11.698 56.621c-.732 3.542 3.979 5.473 5.943 2.437l1.313-2.028l72.516-144.72c1.215-2.423-.88-5.186-3.54-4.672l-25.505 4.922c-2.396.462-4.435-1.77-3.759-4.114l16.646-57.705c.677-2.35-1.37-4.583-3.769-4.113Z"></path></svg>

<./frontend\src\app.tsx>
// App.tsx
import React, { useCallback, useState, useRef, useEffect } from "react";
import { createRoot } from "react-dom/client";
import {
  Map,
  NavigationControl,
  Popup,
  useControl,
} from "react-map-gl/maplibre";
import { Tile3DLayer, MapViewState, AmbientLight, DirectionalLight, LightingEffect } from "deck.gl";
import { MapboxOverlay as DeckOverlay } from "@deck.gl/mapbox";
import "maplibre-gl/dist/maplibre-gl.css";
import type { Tileset3D } from "@loaders.gl/tiles";
import MapboxDraw from "@mapbox/mapbox-gl-draw";
import "@mapbox/mapbox-gl-draw/dist/mapbox-gl-draw.css";
import FloatingPanel from "./FloatingPanel";
import Logo from "./Logo";
import * as turf from "@turf/turf";
import "bootstrap/dist/css/bootstrap.min.css";

import DrawControl from './draw-control';
import LegalDocuments from "./Legals";

import './styles.css'
import './colors.css'

const BASE_URL = import.meta.env.VITE_BASE_URL;
const TILESET_URL = import.meta.env.VITE_TILESET_URL;


const INITIAL_VIEW_STATE: MapViewState = {
  latitude: 49.8917,
  longitude: 10.8863,
  pitch: 45,
  maxPitch: 60,
  bearing: 0,
  minZoom: 2,
  maxZoom: 30,
  zoom: 17,
};

const MAP_STYLE = "/basemap.json";

function DeckGLOverlay(props: any) {
  const overlay = useControl(() => new DeckOverlay(props));
  overlay.setProps(props);
  return null;
}

function Root() {
  const [selected, setSelected] = useState<any>(null);
  const [viewState, setViewState] = useState<MapViewState>(INITIAL_VIEW_STATE);
  const [features, setFeatures] = useState<Record<string, any>>({});
  const [isLod2Visible, setIsLod2Visible] = useState(true);
  const [polygonArea, setPolygonArea] = useState<number | null>(null);
  const mapRef = useRef<any>(null); // Reference to the map instance

  const drawRef = useRef<MapboxDraw | null>(null); // Reference to the MapboxDraw instance

  const rootStyles = getComputedStyle(document.documentElement);
  const POLYGON_COLOR = rootStyles.getPropertyValue('--bs-secondary').trim();
  const POLYGON_SELECTED_COLOR = rootStyles.getPropertyValue('--bs-secondary-selected').trim();

  // Initialize MapboxDraw and add it to the map
  const handleMapLoad = useCallback(() => {
    const map = mapRef.current.getMap();

    // Initialize MapboxDraw if not already initialized
    if (!drawRef.current) {
      drawRef.current = new MapboxDraw({
        displayControlsDefault: false,
        controls: {
          polygon: false,
          trash: false,
        },
        styles: [
          {
            id: 'gl-draw-polygon-fill',
            type: 'fill',
            paint: {
              'fill-color': POLYGON_COLOR,
              'fill-opacity': 0.5, // Adjust transparency if needed
            },
          },
          // Selected Polygon Fill
          {
            id: 'gl-draw-polygon-fill-active',
            type: 'fill',
            filter: ['all', ['==', '$type', 'Polygon'], ['==', 'active', 'true']],
            paint: {
              'fill-color': POLYGON_SELECTED_COLOR,
              'fill-opacity': 0.5,
            },
          },
          // Default Polygon Stroke
          {
            id: 'gl-draw-polygon-stroke',
            type: 'line',
            paint: {
              'line-color': POLYGON_SELECTED_COLOR,
              'line-width': 4,
            },
          },
          // Selected Polygon Stroke
          {
            id: 'gl-draw-polygon-stroke-active',
            type: 'line',
            filter: ['all', ['==', '$type', 'Polygon'], ['==', 'active', 'true']],
            paint: {
              'line-color': POLYGON_SELECTED_COLOR,
              'line-width': 6,
            },
          },
          {
            id: 'gl-draw-point',
            type: 'circle',
            filter: ['all', ['!=', 'meta', 'midpoint']],
            paint: {
              'circle-radius': 12,
              'circle-color': POLYGON_SELECTED_COLOR,
            },
          },
          {
            id: 'gl-draw-point-active',
            type: 'circle',
            filter: ['all', ['!=', 'meta', 'midpoint'], ['==', 'active', 'true']],
            paint: {
              'circle-radius': 8,
              'circle-color': POLYGON_COLOR,
            },
          },
          // Midpoints
          {
            id: 'gl-draw-midpoint',
            type: 'circle',
            filter: ['all', ['==', 'meta', 'midpoint']],
            paint: {
              'circle-radius': 8,
              'circle-color': POLYGON_SELECTED_COLOR,
            },
          },
        ]
      });
      map.addControl(drawRef.current);
    }

  // Bind event listeners for onUpdate and onDelete
  map.on('draw.create', onUpdate); // Bind onUpdate callback
  map.on('draw.update', onUpdate); // Bind onUpdate callback
  map.on('draw.delete', onDelete); // Bind onDelete callback

}, []);
  // const onTilesetLoad = (tileset: Tileset3D) => {
  //   const { cartographicCenter, zoom } = tileset;
  //   setViewState((prev) => ({
  //     ...prev,
  //     longitude: cartographicCenter[0],
  //     latitude: cartographicCenter[1],
  //     zoom,
  //   }));
  // };


  const onUpdate = useCallback((e) => {
    setFeatures((currFeatures) => {
      const newFeatures = { ...currFeatures };
      for (const f of e.features) {
        newFeatures[f.id] = f;
      }
      return newFeatures;
    });

    // Calculate polygon area
    if (e.features && e.features.length > 0) {
      const polygon = e.features[0];
      const area = turf.area(polygon) / 1e6; // Convert from m² to km²
      setPolygonArea(area);
    }
  }, []);

  const onDelete = useCallback((e) => {
    setFeatures((currFeatures) => {
      const newFeatures = { ...currFeatures };
      for (const f of e.features) {
        delete newFeatures[f.id];
      }
      return newFeatures;
    });
    setPolygonArea(null);
  }, []);

  const handleDrawPolygon = () => {
    if (drawRef.current) {
      drawRef.current.deleteAll();
      drawRef.current.changeMode("draw_polygon");
    }
  };

  const handleRemovePolygon = () => {
    if (drawRef.current) {
      drawRef.current.deleteAll();
    }
  };

  const handleFetchObjFile = async () => {
    console.info("getFetchObjFile")
    if (drawRef.current) {
      const data = drawRef.current.getAll();
      if (data.features.length > 0) {
        try {
          const response = await fetch(BASE_URL + "/retrieve_obj", {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
            },
            body: JSON.stringify({ region: data }),
          });

          if (response.ok) {
            const blob = await response.blob();
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement("a");
            a.style.display = "none";
            a.href = url;
            // Use the filename from the Content-Disposition header if available
            const contentDisposition = response.headers.get("Content-Disposition");
            const filenameMatch =
              contentDisposition && contentDisposition.match(/filename="?(.+)"?/i);
            a.download = filenameMatch
              ? filenameMatch[1]
              : `object_file.obj`;
            document.body.appendChild(a);
            a.click();
            window.URL.revokeObjectURL(url);
          } else {
            console.error("Failed to fetch obj file");
          }
        } catch (error) {
          console.error("Error fetching obj file:", error);
        }
      } else {
        console.error("No polygon drawn");
      }
    }
  };

  const handleZoomChange = (event: any) => {
    const newZoom = event.viewState.zoom;
    setViewState(event.viewState);

    // Toggle visibility based on zoom level
    if (newZoom < 1) {
      setIsLod2Visible(false);
    } else {
      setIsLod2Visible(true);
    }
  };

  // Create ambient light
  const ambientLight = new AmbientLight({
    color: [240, 255, 255],
    intensity: 1.0
  });

  // Create directional light
  const directionalLight1 = new DirectionalLight({
    color: [220, 255, 255],
    intensity: 0.6,
    direction: [-1, -3, -1]
  });

  // Create directional light
  const directionalLight2 = new DirectionalLight({
    color: [255, 220, 255],
    intensity: 1,
    direction: [1, -3, 1]
  });

  // Create lighting effect
  const lightingEffect = new LightingEffect({ ambientLight, directionalLight1, directionalLight2 });


  const layers = [
    new Tile3DLayer({
      id: "tile-3d-layer",
      data: TILESET_URL,
      // pickable: true,
      // autoHighlight: false,
      // onClick: (info, event) => console.log("Clicked:", info, event),
      // getPickingInfo: (pickParams) => console.log("PickInfo", pickParams),
      // onTilesetLoad,
      visible: isLod2Visible,
      // For ScenegraphLayer (b3dm or i3dm format)
      //_lighting: 'pbr',
      //effects: [lightingEffect],
      loadOptions: {
        tileset: {
          maxRequests: 16,
          updateTransforms: false,
          maximumMemoryUsage: 512
          //maximumScreenSpaceError: 16, // Adjust this value as needed
          //viewDistanceScale: 1.5 // Adjust this value as needed
        }
      },
      // Additional sublayer props for fine-grained control
      _subLayerProps: {
        scenegraph: {
          getColor: (d) => [255, 255, 255, 150], // Blue color for scenegraph models (alternative method)
          //effects: [lightingEffect]
        }
      }
    }),
  ];

  const handleSearch = async (query: string) => {
    const response = await fetch(
      `https://nominatim.openstreetmap.org/search?format=json&q=${encodeURIComponent(query)}&countrycodes=de`
    );
    const data = await response.json();
    return data;
  };

  const handleSelectResult = (result: any) => {
    // Fly to the selected location
    const map = mapRef.current.getMap();

    map.flyTo({
      center: [parseFloat(result.lon), parseFloat(result.lat)],
      zoom: 14
    });
  };

  return (
    <div style={{ position: "fixed", width: "100%", height: "100%" }}>
      <Map
        initialViewState={viewState}
        mapStyle={MAP_STYLE}
        onLoad={handleMapLoad} // Ensure map is passed here
        onMove={handleZoomChange}
        ref={mapRef}
        style={{ width: "100%", height: "100%" }}
        hash={true}
      >
        {selected && (
          <Popup
            key={selected.properties.name}
            anchor="bottom"
            style={{ zIndex: 10 }}
            longitude={selected.geometry.coordinates[0]}
            latitude={selected.geometry.coordinates[1]}
          >
            {selected.properties.name} ({selected.properties.abbrev})
          </Popup>
        )}
        <DeckGLOverlay layers={layers}   //effects={[lightingEffect]} // Apply the custom lighting effect globally
        />
        {/* <DrawControl
          ref={drawRef}
          onCreate={onUpdate}
          onUpdate={onUpdate}
          onDelete={onDelete}
        /> */}
        {/* <div
          style={{
            position: 'absolute',
            bottom: '240px', // Adjust as needed to ensure a 20px gap above Auswahl panel
            left: '20px',
            zIndex: 2000,
            pointerEvents: 'none',
          }}
        >
          <div style={{ pointerEvents: 'auto' }}>
            <NavigationControl />
          </div>
        </div> */}
        {/* <DrawControl
          position="top-right"
          displayControlsDefault={false}
          controls={{
            polygon: false,
            trash: false
          }}
          defaultMode="draw_polygon"
          onCreate={onUpdate}
          onUpdate={onUpdate}
          onDelete={onDelete}
        /> */}
      </Map>
      <FloatingPanel
        onDrawPolygon={handleDrawPolygon}
        onRemovePolygon={handleRemovePolygon}
        onFetchObjFile={handleFetchObjFile}
        polygonArea={polygonArea}
        onSearch={handleSearch}
        onSelectResult={handleSelectResult}
      />

      <div className="top-bar-container">
        <div className="top-bar-section legals">
          <LegalDocuments />
        </div>
        <div className="top-bar-section logo">
          <Logo />
        </div>
      </div>

    </div>
  );
}

interface DrawControlProps {
  onCreate: (e: any) => void;
  onUpdate: (e: any) => void;
  onDelete: (e: any) => void;
}

export default Root;


<./frontend\src\CheckoutForm.tsx>
import React, { useState } from "react";
import { useStripe, useElements, CardElement } from "@stripe/react-stripe-js";

interface CheckoutFormProps {
  price: number;
  onFetchObjFile: () => void;
}

interface CustomerData {
  email: string;
  name: string;
  address: {
    line1: string;
    postal_code: string;
    city: string;
    country: string;
  };
}
const CheckoutForm: React.FC<CheckoutFormProps> = ({ price, onFetchObjFile }) => {
  const stripe = useStripe();
  const elements = useElements();
  const [loading, setLoading] = useState(false);
  const [isExpanded, setIsExpanded] = useState(false);
  const [customerData, setCustomerData] = useState<CustomerData>({
    email: "",
    name: "",
    address: {
      line1: "",
      postal_code: "",
      city: "",
      country: "DE",
    },
  });

  // If price is 0, render only the download button
  if (price === 0) {
    return (
      <>
      <div className="text-center mb-3">
      Grundstücke unter 0.01 km² können kostenfrei heruntergeladen werden
      </div>
      <button 
        onClick={onFetchObjFile}
        className="btn btn-secondary btn mt-2"
      >
        .obj Herunterladen
      </button>
      </>
    );
  }

  const handleFocus = () => {
    setIsExpanded(true);
  };
  const handleFocusOut = () => {
    setIsExpanded(false);
  };

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!stripe || !elements) return;

    if (!customerData.email || !customerData.name || !customerData.address.line1 || 
        !customerData.address.postal_code || !customerData.address.city) {
      document.getElementById("payment-message")!.textContent = "Please fill in all required fields.";
      return;
    }

    setLoading(true);

    try {
      const response = await fetch(import.meta.env.VITE_BASE_URL + "/create-payment-intent", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ 
          amount: Math.round(price * 100),
          customer: customerData
        }),
      });

      if (!response.ok) throw new Error("Failed to create PaymentIntent.");

      const { clientSecret } = await response.json();

      const result = await stripe.confirmCardPayment(clientSecret, {
        payment_method: {
          billing_details: {
            name: customerData.name,
            email: customerData.email,
            address: {
              line1: customerData.address.line1,
              postal_code: customerData.address.postal_code,
              city: customerData.address.city,
              country: customerData.address.country,
            },
          },
          card: elements.getElement(CardElement)!,
        },
      });

      if (result.error) {
        document.getElementById("payment-message")!.textContent = result.error.message;
      } else if (result.paymentIntent?.status === "succeeded") {
        document.getElementById("payment-message")!.textContent = "Success! Your download will start soon.";
        onFetchObjFile();
      }
    } catch (error) {
      console.error("Payment error:", error);
    } finally {
      setLoading(false);
    }
  };
  return (
    <form onSubmit={handleSubmit} className="d-flex flex-column gap-2">
      
      {/* Secure Payment Badge */}
      <div className="d-flex align-items-center gap-1 text-secondary small">
        <span className="bi bi-lock-fill"></span>
        Secure payment via Stripe
      </div>
  
      {/* Price Details */}
      <div className="text-secondary small">
        <p>
          <strong>Order Total:</strong> €{price.toFixed(2)}
        </p>
        <p>No additional fees. You’ll only be charged this amount.</p>
      </div>
  
      {isExpanded && (
        <div className="mt-2 animate__animated animate__fadeIn">
          {/* Customer Details */}
          <input
            type="email"
            placeholder="Email *"
            required
            value={customerData.email}
            onChange={(e) =>
              setCustomerData({ ...customerData, email: e.target.value })
            }
            className="form-control form-control-sm mb-2"
          />
          <input
            type="text"
            placeholder="Full Name *"
            required
            value={customerData.name}
            onChange={(e) =>
              setCustomerData({ ...customerData, name: e.target.value })
            }
            className="form-control form-control-sm mb-2"
          />
          <input
            type="text"
            placeholder="Street Address *"
            required
            value={customerData.address.line1}
            onChange={(e) =>
              setCustomerData({
                ...customerData,
                address: { ...customerData.address, line1: e.target.value },
              })
            }
            className="form-control form-control-sm mb-2"
          />
          <div className="d-flex gap-2 mb-2">
            <input
              type="text"
              placeholder="Postal Code *"
              required
              value={customerData.address.postal_code}
              onChange={(e) =>
                setCustomerData({
                  ...customerData,
                  address: {
                    ...customerData.address,
                    postal_code: e.target.value,
                  },
                })
              }
              className="form-control form-control-sm"
            />
            <input
              type="text"
              placeholder="City *"
              required
              value={customerData.address.city}
              onChange={(e) =>
                setCustomerData({
                  ...customerData,
                  address: { ...customerData.address, city: e.target.value },
                })
              }
              className="form-control form-control-sm"
            />
          </div>
        </div>
      )}
  
      {/* Card Element */}
      <CardElement
      
      onFocus={handleFocus}
      onFocusOut={handleFocusOut}
        options={{
          style: {
            base: {
              fontSize: "14px",
            },
          },
        }}
      />
      <div id="payment-message" className="text-danger small"></div>
  
      {/* Link to Stripe Security Info */}
      <div className="small mt-7">
        <a
          href="https://stripe.com/docs/security"
          target="_blank"
          rel="noopener noreferrer"
          className="text-secondary"
        >
          Learn more about how your payment information is secured.
        </a>
      </div>
  
      <button
        type="submit"
        disabled={!stripe || loading}
        className="btn btn-secondary btn-sm mt-2"
      >
        {loading ? "Processing..." : `Pay €${price.toFixed(2)}`}
      </button>
    </form>
  );
  
};
export default CheckoutForm;

<./frontend\src\colors.css>
:root {
    --bs-primary: #2a7a92; /* Your desired primary color */
    --bs-secondary: #3e66bb;
    --bs-secondary-selected: #244ca3;
    --bs-success: #28a745;
    --bs-danger: #f38093;
    --bs-warning: #ffc107;
    --bs-info: #17a2b8;
    --bs-light: #f8f9fa;
    --bs-dark: #343a40;
  }
  
/*   
  .btn-primary {
    background-color: var(--bs-primary) !important;
    border-color: var(--bs-primary) !important;
  }
  .btn-secondary {
    background-color: var(--bs-secondary) !important;
    border-color: var(--bs-secondary) !important;
  }
  
  .btn-danger {
    background-color: var(--bs-danger) !important;
    border-color: var(--bs-danger) !important;
  }
   */

   
/*------------------------------------
- COLOR primary
------------------------------------*/
.alert-primary {
  color: #0e2a32;
  background-color: #a3d5e4;
  border-color: #93cee0;
}

.alert-primary hr {
  border-top-color: #7fc5da;
}

.alert-primary .alert-link {
  color: #03090b;
}

.badge-primary {
  color: #fff;
  background-color: #2a7a92;
}

.badge-primary[href]:hover, .badge-primary[href]:focus {
  color: #fff;
  background-color: #1e586a;
}

.bg-primary {
  background-color: #2a7a92 !important;
}

a.bg-primary:hover, a.bg-primary:focus,
button.bg-primary:hover,
button.bg-primary:focus {
  background-color: #1e586a !important;
}

.border-primary {
  border-color: #2a7a92 !important;
}

.btn-primary {
  color: #fff;
  background-color: #2a7a92;
  border-color: #2a7a92;
}

.btn-primary:hover {
  color: #fff;
  background-color: #226276;
  border-color: #1e586a;
}

.btn-primary:focus, .btn-primary.focus {
  box-shadow: 0 0 0 0.2rem rgba(42, 122, 146, 0.5);
}

.btn-primary.disabled, .btn-primary:disabled {
  color: #fff;
  background-color: #2a7a92;
  border-color: #2a7a92;
}

.btn-primary:not(:disabled):not(.disabled):active, .btn-primary:not(:disabled):not(.disabled).active, .show > .btn-primary.dropdown-toggle {
  color: #fff;
  background-color: #1e586a;
  border-color: #1b4e5e;
}

.btn-primary:not(:disabled):not(.disabled):active:focus, .btn-primary:not(:disabled):not(.disabled).active:focus, .show > .btn-primary.dropdown-toggle:focus {
  box-shadow: 0 0 0 0.2rem rgba(42, 122, 146, 0.5);
}

.btn-outline-primary {
  color: #2a7a92;
  background-color: transparent;
  border-color: #2a7a92;
}

.btn-outline-primary:hover {
  color: #fff;
  background-color: #2a7a92;
  border-color: #2a7a92;
}

.btn-outline-primary:focus, .btn-outline-primary.focus {
  box-shadow: 0 0 0 0.2rem rgba(42, 122, 146, 0.5);
}

.btn-outline-primary.disabled, .btn-outline-primary:disabled {
  color: #2a7a92;
  background-color: transparent;
}

.btn-outline-primary:not(:disabled):not(.disabled):active, .btn-outline-primary:not(:disabled):not(.disabled).active, .show > .btn-outline-primary.dropdown-toggle {
  color: #fff;
  background-color: #2a7a92;
  border-color: #2a7a92;
}

.btn-outline-primary:not(:disabled):not(.disabled):active:focus, .btn-outline-primary:not(:disabled):not(.disabled).active:focus, .show > .btn-outline-primary.dropdown-toggle:focus {
  box-shadow: 0 0 0 0.2rem rgba(42, 122, 146, 0.5);
}

.list-group-item-primary {
  color: #0e2a32;
  background-color: #93cee0;
}

.list-group-item-primary.list-group-item-action:hover, .list-group-item-primary.list-group-item-action:focus {
  color: #0e2a32;
  background-color: #7fc5da;
}

.list-group-item-primary.list-group-item-action.active {
  color: #fff;
  background-color: #0e2a32;
  border-color: #0e2a32;
}

.table-primary,
.table-primary > th,
.table-primary > td {
  background-color: #93cee0;
}

.table-hover .table-primary:hover {
  background-color: #7fc5da;
}

.table-hover .table-primary:hover > td,
.table-hover .table-primary:hover > th {
  background-color: #7fc5da;
}

.text-primary {
  color: #2a7a92 !important;
}

a.text-primary:hover, a.text-primary:focus {
  color: #1e586a !important;
}

   
/*------------------------------------
- COLOR secondary
------------------------------------*/
.alert-secondary {
  color: #1f335f;
  background-color: #d4ddf0;
  border-color: #c4d1eb;
}

.alert-secondary hr {
  border-top-color: #b1c2e5;
}

.alert-secondary .alert-link {
  color: #121e38;
}

.badge-secondary {
  color: #fff;
  background-color: #3e66bb;
}

.badge-secondary[href]:hover, .badge-secondary[href]:focus {
  color: #fff;
  background-color: #315194;
}

.bg-secondary {
  background-color: #3e66bb !important;
}

a.bg-secondary:hover, a.bg-secondary:focus,
button.bg-secondary:hover,
button.bg-secondary:focus {
  background-color: #315194 !important;
}

.border-secondary {
  border-color: #3e66bb !important;
}

.btn-secondary {
  color: #fff;
  background-color: #3e66bb;
  border-color: #3e66bb;
}

.btn-secondary:hover {
  color: #fff;
  background-color: #3557a0;
  border-color: #315194;
}

.btn-secondary:focus, .btn-secondary.focus {
  box-shadow: 0 0 0 0.2rem rgba(62, 102, 187, 0.5);
}

.btn-secondary.disabled, .btn-secondary:disabled {
  color: #fff;
  background-color: #3e66bb;
  border-color: #3e66bb;
}

.btn-secondary:not(:disabled):not(.disabled):active, .btn-secondary:not(:disabled):not(.disabled).active, .show > .btn-secondary.dropdown-toggle {
  color: #fff;
  background-color: #315194;
  border-color: #2d4a89;
}

.btn-secondary:not(:disabled):not(.disabled):active:focus, .btn-secondary:not(:disabled):not(.disabled).active:focus, .show > .btn-secondary.dropdown-toggle:focus {
  box-shadow: 0 0 0 0.2rem rgba(62, 102, 187, 0.5);
}

.btn-outline-secondary {
  color: #3e66bb;
  background-color: transparent;
  border-color: #3e66bb;
}

.btn-outline-secondary:hover {
  color: #fff;
  background-color: #3e66bb;
  border-color: #3e66bb;
}

.btn-outline-secondary:focus, .btn-outline-secondary.focus {
  box-shadow: 0 0 0 0.2rem rgba(62, 102, 187, 0.5);
}

.btn-outline-secondary.disabled, .btn-outline-secondary:disabled {
  color: #3e66bb;
  background-color: transparent;
}

.btn-outline-secondary:not(:disabled):not(.disabled):active, .btn-outline-secondary:not(:disabled):not(.disabled).active, .show > .btn-outline-secondary.dropdown-toggle {
  color: #fff;
  background-color: #3e66bb;
  border-color: #3e66bb;
}

.btn-outline-secondary:not(:disabled):not(.disabled):active:focus, .btn-outline-secondary:not(:disabled):not(.disabled).active:focus, .show > .btn-outline-secondary.dropdown-toggle:focus {
  box-shadow: 0 0 0 0.2rem rgba(62, 102, 187, 0.5);
}

.list-group-item-secondary {
  color: #1f335f;
  background-color: #c4d1eb;
}

.list-group-item-secondary.list-group-item-action:hover, .list-group-item-secondary.list-group-item-action:focus {
  color: #1f335f;
  background-color: #b1c2e5;
}

.list-group-item-secondary.list-group-item-action.active {
  color: #fff;
  background-color: #1f335f;
  border-color: #1f335f;
}

.table-secondary,
.table-secondary > th,
.table-secondary > td {
  background-color: #c4d1eb;
}

.table-hover .table-secondary:hover {
  background-color: #b1c2e5;
}

.table-hover .table-secondary:hover > td,
.table-hover .table-secondary:hover > th {
  background-color: #b1c2e5;
}

.text-secondary {
  color: #3e66bb !important;
}

a.text-secondary:hover, a.text-secondary:focus {
  color: #315194 !important;
}

/*------------------------------------
- COLOR danger
------------------------------------*/
.alert-danger {
  color: #d31b39;
  background-color: #10512f128;
  border-color: #10211d119;
}

.alert-danger hr {
  border-top-color: #100107105;
}

.alert-danger .alert-link {
  color: #a6152d;
}

.badge-danger {
  color: #212529;
  background-color: #ee7b8e;
}

.badge-danger[href]:hover, .badge-danger[href]:focus {
  color: #212529;
  background-color: #e84d67;
}

.bg-danger {
  background-color: #ee7b8e !important;
}

a.bg-danger:hover, a.bg-danger:focus,
button.bg-danger:hover,
button.bg-danger:focus {
  background-color: #e84d67 !important;
}

.border-danger {
  border-color: #ee7b8e !important;
}

.btn-danger {
  color: #212529;
  background-color: #ee7b8e;
  border-color: #ee7b8e;
}

.btn-danger:hover {
  color: #212529;
  background-color: #e95b72;
  border-color: #e84d67;
}

.btn-danger:focus, .btn-danger.focus {
  box-shadow: 0 0 0 0.2rem rgba(238, 123, 142, 0.5);
}

.btn-danger.disabled, .btn-danger:disabled {
  color: #212529;
  background-color: #ee7b8e;
  border-color: #ee7b8e;
}

.btn-danger:not(:disabled):not(.disabled):active, .btn-danger:not(:disabled):not(.disabled).active, .show > .btn-danger.dropdown-toggle {
  color: #212529;
  background-color: #e84d67;
  border-color: #e6405b;
}

.btn-danger:not(:disabled):not(.disabled):active:focus, .btn-danger:not(:disabled):not(.disabled).active:focus, .show > .btn-danger.dropdown-toggle:focus {
  box-shadow: 0 0 0 0.2rem rgba(238, 123, 142, 0.5);
}

.btn-outline-danger {
  color: #ee7b8e;
  background-color: transparent;
  border-color: #ee7b8e;
}

.btn-outline-danger:hover {
  color: #212529;
  background-color: #ee7b8e;
  border-color: #ee7b8e;
}

.btn-outline-danger:focus, .btn-outline-danger.focus {
  box-shadow: 0 0 0 0.2rem rgba(238, 123, 142, 0.5);
}

.btn-outline-danger.disabled, .btn-outline-danger:disabled {
  color: #ee7b8e;
  background-color: transparent;
}

.btn-outline-danger:not(:disabled):not(.disabled):active, .btn-outline-danger:not(:disabled):not(.disabled).active, .show > .btn-outline-danger.dropdown-toggle {
  color: #212529;
  background-color: #ee7b8e;
  border-color: #ee7b8e;
}

.btn-outline-danger:not(:disabled):not(.disabled):active:focus, .btn-outline-danger:not(:disabled):not(.disabled).active:focus, .show > .btn-outline-danger.dropdown-toggle:focus {
  box-shadow: 0 0 0 0.2rem rgba(238, 123, 142, 0.5);
}

.list-group-item-danger {
  color: #d31b39;
  background-color: #10211d119;
}

.list-group-item-danger.list-group-item-action:hover, .list-group-item-danger.list-group-item-action:focus {
  color: #d31b39;
  background-color: #100107105;
}

.list-group-item-danger.list-group-item-action.active {
  color: #212529;
  background-color: #d31b39;
  border-color: #d31b39;
}

.table-danger,
.table-danger > th,
.table-danger > td {
  background-color: #10211d119;
}

.table-hover .table-danger:hover {
  background-color: #100107105;
}

.table-hover .table-danger:hover > td,
.table-hover .table-danger:hover > th {
  background-color: #100107105;
}

.text-danger {
  color: #ee7b8e !important;
}

a.text-danger:hover, a.text-danger:focus {
  color: #e84d67 !important;
}


<./frontend\src\draw-control.ts>
import MapboxDraw from '@mapbox/mapbox-gl-draw';
import {useControl} from 'react-map-gl';

import type {MapRef, ControlPosition} from 'react-map-gl';

type DrawControlProps = ConstructorParameters<typeof MapboxDraw>[0] & {
  position?: ControlPosition;

  onCreate?: (evt: {features: object[]}) => void;
  onUpdate?: (evt: {features: object[]; action: string}) => void;
  onDelete?: (evt: {features: object[]}) => void;
};

export default function DrawControl(props: DrawControlProps) {
  useControl<MapboxDraw>(
    () => new MapboxDraw(props),
    ({map}: {map: MapRef}) => {
      map.on('draw.create', props.onCreate);
      map.on('draw.update', props.onUpdate);
      map.on('draw.delete', props.onDelete);
    },
    {
      position: props.position
    }
  );

  return null;
}

DrawControl.defaultProps = {
  onCreate: () => {},
  onUpdate: () => {},
  onDelete: () => {}
};

<./frontend\src\FloatingPanel.tsx>
import React, { useState, useEffect } from "react";
import { loadStripe } from "@stripe/stripe-js";
import { Elements } from "@stripe/react-stripe-js";
import CheckoutForm from "./CheckoutForm";
import "bootstrap/dist/css/bootstrap.min.css";
// Import FontAwesome
import { FontAwesomeIcon } from '@fortawesome/react-fontawesome';
import { faChevronUp, faChevronDown } from '@fortawesome/free-solid-svg-icons';

// Initialize Stripe with your publishable key
const stripePromise = loadStripe(import.meta.env.VITE_STRIPE_PUBLISHABLE_KEY);

interface FloatingPanelProps {
  onDrawPolygon: () => void;
  onRemovePolygon: () => void;
  onFetchObjFile: () => void;
  polygonArea: number | null; // in square kilometers
  onSearch: (query: string) => Promise<any>;
  onSelectResult: (result: any) => void;
}

const FloatingPanel: React.FC<FloatingPanelProps> = ({
  onDrawPolygon,
  onRemovePolygon,
  onFetchObjFile,
  polygonArea,
  onSearch,
  onSelectResult,
}) => {
  const [price, setPrice] = useState<number>(0);

  const [searchQuery, setSearchQuery] = useState("");
  const [searchResults, setSearchResults] = useState<any[]>([]);
  const [isSearching, setIsSearching] = useState(false);

  // New state for expanded tab
  const [expandedTab, setExpandedTab] = useState<string | null>(null);

  const [isMobile, setIsMobile] = useState<boolean>(
    window.innerWidth <= 768
  );

  useEffect(() => {
    if (polygonArea !== null) {
      if (polygonArea < 0.01) {
        setPrice(0);
      } else {
        setPrice(50 * polygonArea);
      }
    }
  }, [polygonArea]);

  // Handle window resize to update isMobile state
  useEffect(() => {
    const handleResize = () => {
      setIsMobile(window.innerWidth <= 768);
    };

    window.addEventListener("resize", handleResize);
    return () => window.removeEventListener("resize", handleResize);
  }, []);

  // Handle search functionality
  const handleSearch = async (e: React.ChangeEvent<HTMLInputElement>) => {
    const query = e.target.value;
    setSearchQuery(query);

    if (query.length > 2) {
      setIsSearching(true);
      try {
        const results = await onSearch(query);
        setSearchResults(results);
      } catch (error) {
        console.error("Search error:", error);
        setSearchResults([]);
      }
      setIsSearching(false);
    } else {
      setSearchResults([]);
    }
  };

  // Extracted content functions for reuse
  const renderAuswahlContent = () => (
    <>
      {/* Search Input */}
      <div className="w-100 mb-3 position-relative">
        <input
          type="text"
          className="form-control form-control-sm"
          placeholder="Standort finden..."
          value={searchQuery}
          onChange={handleSearch}
        />

        {/* Search Results */}
        {searchResults.length > 0 && (
          <div
            className="position-absolute bg-white shadow-sm rounded mt-1 w-100 overflow-auto"
            style={{
              maxHeight: "150px",
              zIndex: 1060,
            }}
          >
            {searchResults.map((result, index) => (
              <div
                key={index}
                className="p-2 hover-bg-light cursor-pointer"
                onClick={() => {
                  onSelectResult(result);
                  setSearchResults([]);
                  setSearchQuery("");
                }}
                style={{ cursor: "pointer" }}
              >
                {result.display_name || result.name}
              </div>
            ))}
          </div>
        )}

        {isSearching && (
          <div className="text-center mt-2">
            <small>Suche...</small>
          </div>
        )}
      </div>

      {/* Polygon Area and Price Display */}
      {polygonArea !== null ? (
        <div className="text-center mb-3">
          <strong>Gebietfläche:</strong> {polygonArea.toFixed(3)} km²
        </div>
      ) : (
        <p className="text-center mb-3">Zeichnen Sie das Gebiet ein</p>
      )}

      {/* Action Buttons */}
      <div className="btn-group w-100" role="group">
        <button
          type="button"
          className="btn btn-secondary btn mt-2"
          onClick={onDrawPolygon}
        >
          Polygon zeichnen
        </button>
        <button
          type="button"
          className="btn btn-danger btn mt-2"
          onClick={onRemovePolygon}
        >
          Entfernen
        </button>
      </div>
    </>
  );

  const renderHerunterladenContent = () => (
    <>
      <Elements stripe={stripePromise}>
        <CheckoutForm price={price} onFetchObjFile={onFetchObjFile} />
      </Elements>
    </>
  );

  // Mobile Layout
  if (isMobile) {
    return (
      <div
        className="position-fixed"
        style={{
          bottom: "30px",
          left: "0",
          right: "0",
          zIndex: 1050,
          pointerEvents: "none",
        }}
      >
        {/* Auswahl Tab */}
        <div
          style={{
            pointerEvents: "auto",
            marginBottom: expandedTab === 'Auswahl' ? '0' : '10px',
          }}
        >
          <div
            className={`bg-white shadow p-3 d-flex justify-content-between align-items-center ${
              expandedTab === 'Auswahl' ? 'rounded-top' : 'rounded'
            }`}
            style={{
              margin: "0 20px",
              cursor: "pointer",
            }}
            onClick={() => {
              setExpandedTab(expandedTab === 'Auswahl' ? null : 'Auswahl');
            }}
          >
            <h5 className="mb-0">Auswahl</h5>
            {/* Expand/Collapse Icon */}
            <FontAwesomeIcon
              icon={expandedTab === 'Auswahl' ? faChevronUp : faChevronDown}
            />
          </div>
          {expandedTab === 'Auswahl' && (
            <div
              className="bg-white shadow p-3 rounded-bottom"
              style={{
                margin: "0 20px 20px",
              }}
            >
              {renderAuswahlContent()}
            </div>
          )}
        </div>

        {/* Herunterladen Tab */}
        <div
          style={{
            pointerEvents: "auto",
            marginBottom: expandedTab === 'Herunterladen' ? '0' : '10px',
          }}
        >
          <div
            className={`bg-white shadow p-3 d-flex justify-content-between align-items-center ${
              expandedTab === 'Herunterladen' ? 'rounded-top' : 'rounded'
            }`}
            style={{
              margin: "0 20px",
              cursor: "pointer",
            }}
            onClick={() => {
              setExpandedTab(expandedTab === 'Herunterladen' ? null : 'Herunterladen');
            }}
          >
            <h5 className="mb-0">Herunterladen</h5>
            {/* Expand/Collapse Icon */}
            <FontAwesomeIcon
              icon={expandedTab === 'Herunterladen' ? faChevronUp : faChevronDown}
            />
          </div>
          {expandedTab === 'Herunterladen' && (
            <div
              className="bg-white shadow p-3 rounded-bottom"
              style={{
                margin: "0 20px 20px",
              }}
            >
              {renderHerunterladenContent()}
            </div>
          )}
        </div>
      </div>
    );
  }

  // Desktop Layout remains unchanged
  return (
    <div
      className="d-flex align-items-end"
      style={{
        position: "absolute",
        bottom: "20px",
        left: "20px",
        right: "20px",
        gap: "20px",
        zIndex: 1050,
        pointerEvents: "none",
      }}
    >
      <div
        className="d-flex gap-3"
        style={{
          marginRight: "auto",
          pointerEvents: "none",
        }}
      >
        {/* Auswahl Panel */}
        <div
          className="bg-white rounded shadow p-3 d-flex flex-column align-items-center justify-content-center"
          style={{
            width: "300px",
            minHeight: "200px",
            pointerEvents: "auto",
          }}
        >
          <h5 className="mb-3 text-center">Auswahl</h5>
          {renderAuswahlContent()}
        </div>
      </div>

      {/* Herunterladen Panel */}
      <div
        className="bg-white rounded shadow p-3 d-flex flex-column "
        style={{
          width: "300px",
          minHeight: "50px",
          marginLeft: "auto",
          transition: "height 0.3s ease-in-out",
          pointerEvents: "auto",
        }}
      >
        <h5 className="mb-3 text-center">Herunterladen</h5>
        {renderHerunterladenContent()}
      </div>
    </div>
  );
};

export default FloatingPanel;


<./frontend\src\index.tsx>
import React from 'react';
import ReactDOM from 'react-dom/client';
import App from './App';

ReactDOM.createRoot(document.getElementById('root')!).render(<App />);


<./frontend\src\Legals.tsx>
import React, { useState, useEffect } from 'react';
import ReactMarkdown from 'react-markdown';
import remarkGfm from 'remark-gfm';

const LegalDocumentPanel: React.FC<{
  documentType: string;
  isOpen: boolean;
  setOpenDocument: (doc: string | null) => void;
}> = ({ documentType, isOpen, setOpenDocument }) => {
  const [content, setContent] = useState('');

  useEffect(() => {
    if (isOpen && !content) {
      const fetchContent = async () => {
        try {
          const response = await fetch(`/docs/${documentType}.md`);
          const text = await response.text();
          setContent(text);
        } catch (error) {
          console.error(`Error loading ${documentType}:`, error);
          setContent('Failed to load content.');
        }
      };
      fetchContent();
    }
  }, [isOpen, content, documentType]);

  const handleClick = () => {
    setOpenDocument(isOpen ? null : documentType);
  };

  return ( 
    <>
      <button
        onClick={handleClick}
        className="btn btn-sm btn-light"
        style={{
          display: 'inline-block',
          padding: '0.25rem 0.5rem',           
           fontSize: "0.8rem",

        }}
      >
        {documentType.charAt(0).toUpperCase() + documentType.slice(1)}
      </button>

      {isOpen && (
        <div
          className="bg-white rounded shadow p-3"
          style={{
            position: 'absolute',
            top: '50px',
            left: '10px',
            maxWidth: '600px',
            width: '80vw',
            maxHeight: '50vh',
            overflowY: 'auto',
            zIndex: 1070,
          }}
        >
          <button
            className="btn btn-sm btn-close float-end"
            onClick={() => setOpenDocument(null)}
          />
          <h5 className="mb-3">
            {documentType.charAt(0).toUpperCase() + documentType.slice(1)}
          </h5>
          <ReactMarkdown remarkPlugins={[remarkGfm]}>{content}</ReactMarkdown>
        </div>
      )}
    </>
  );
};

const LegalDocuments: React.FC = () => {
  const [openDocument, setOpenDocument] = useState<string | null>(null);

  return (
    <div
      style={{
        display: 'flex',
        gap: '10px',
        pointerEvents: "auto"
      }}
    >
      <LegalDocumentPanel
        documentType="quellen"
        isOpen={openDocument === 'quellen'}
        setOpenDocument={setOpenDocument}
      />
      <LegalDocumentPanel
        documentType="impressum"
        isOpen={openDocument === 'impressum'}
        setOpenDocument={setOpenDocument}
      />
      <LegalDocumentPanel
        documentType="datenschutz"
        isOpen={openDocument === 'datenschutz'}
        setOpenDocument={setOpenDocument}
      />
      <LegalDocumentPanel
        documentType="AGB & Widerruf"
        isOpen={openDocument === 'AGB & Widerruf'}
        setOpenDocument={setOpenDocument}
      />
    </div>
  );
};


export default LegalDocuments;


<./frontend\src\Logo.tsx>
import React, { useState, useEffect } from "react";
import "bootstrap/dist/css/bootstrap.min.css";
import "./styles.css";

const Logo: React.FC = () => {
  const [isExpanded, setIsExpanded] = useState(true); // Control visibility
  const [timeoutId, setTimeoutId] = useState<number | null>(null);

  const toggleDescription = () => {
    setIsExpanded((prev) => !prev);

    // Clear the timeout if the user manually toggles the button
    if (timeoutId) {
      clearTimeout(timeoutId);
      setTimeoutId(null);
    }

    // Reset auto-hide timer if expanded
    if (!isExpanded) {
      const id = window.setTimeout(() => setIsExpanded(false), 30000);
      setTimeoutId(id);
    }
  };

  useEffect(() => {
    // Auto-hide description after 30 seconds
    const id = window.setTimeout(() => setIsExpanded(false), 30000);
    setTimeoutId(id);

    return () => {
      if (id) clearTimeout(id);
    };
  }, []);

  return (
    <div
      className="d-inline-block px-4 py-2 rounded shadow bg-light"
      style={{
        color: "black",
        pointerEvents: "auto",
        fontWeight: 500,
        width: "450px",
        maxWidth: "80%",
        position: "relative",
      }}
    >
      <div
        className="space-grotesk-regular text-center"
        style={{
          fontSize: "2rem",
        }}
      >
        EasyOpenData
      </div>

      {isExpanded && (
        <div
          className="text-left mb-3 ibm-plex-sans-light"
          style={{
            position: "absolute", // Make it float independently
            top: "3rem",          // Adjust the vertical positioning
            left: 0,
            width: "100%",        // Ensure it spans the logo's width
            fontSize: "1rem",
            fontFamily: "IBM Plex Sans, sans-serif",
            fontWeight: 300, // Matches ibm-plex-sans-light
            marginTop: "0.5rem",
            lineHeight: "1.5",
            backgroundColor: "#f8f9fa",
            padding: "0.5rem",
            borderRadius: "5px",
            boxShadow: "0px 4px 6px rgba(0, 0, 0, 0.1)",
          }}

        >
          <strong>
            Zeichnen Sie ein Polygon auf der Karte ein und laden präzise Gebäudegeometrie als .obj-Dateien herunter.
            Nach Zahlung erhalten Sie direkt Ihren Download-Link.<br />
            Kosten: Polygonfläche × 50 €/km²<br />
          </strong>
        </div>
      )}

      <button
        onClick={toggleDescription}
        style={{
          position: "absolute",
          top: "0.2rem",
          right: "0.2rem",
          background: "none",
          border: "none",
          cursor: "pointer",
          fontSize: "1rem",
          fontWeight: "bold",
        }}
      >
        {isExpanded ? "–" : "+"}
      </button>
    </div>
  );
};

export default Logo;


<./frontend\src\styles.css>
/* @import url('https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap'); */
@import url('https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&family=Space+Grotesk:wght@300..700&display=swap');


html,
body {
  height: 100%;
  overflow: hidden;  /* Prevent scrolling */
}

/* Override all fonts */
* {
  font-family: "IBM Plex Sans", sans-serif;
  font-optical-sizing: auto;
  font-weight: 300;
  font-style: normal;
}

.ibm-plex-sans-thin {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 100;
  font-style: normal;
}

.ibm-plex-sans-extralight {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 200;
  font-style: normal;
}

.ibm-plex-sans-light {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 300;
  font-style: normal;
}

.ibm-plex-sans-regular {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 400;
  font-style: normal;
}

.ibm-plex-sans-medium {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 500;
  font-style: normal;
}

.ibm-plex-sans-semibold {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 600;
  font-style: normal;
}

.ibm-plex-sans-bold {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 700;
  font-style: normal;
}

.ibm-plex-sans-thin-italic {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 100;
  font-style: italic;
}

.ibm-plex-sans-extralight-italic {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 200;
  font-style: italic;
}

.ibm-plex-sans-light-italic {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 300;
  font-style: italic;
}

.ibm-plex-sans-regular-italic {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 400;
  font-style: italic;
}

.ibm-plex-sans-medium-italic {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 500;
  font-style: italic;
}

.ibm-plex-sans-semibold-italic {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 600;
  font-style: italic;
}

.ibm-plex-sans-bold-italic {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 700;
  font-style: italic;
}

.space-grotesk-light {
  font-family: "Space Grotesk", sans-serif;
  font-optical-sizing: auto;
  font-weight: 300;
  font-style: normal;
}

.space-grotesk-regular {
  font-family: "Space Grotesk", sans-serif;
  font-optical-sizing: auto;
  font-weight: 400;
  font-style: normal;
}

.space-grotesk-bold {
  font-family: "Space Grotesk", sans-serif;
  font-optical-sizing: auto;
  font-weight: 500;
  font-style: normal;
}

.space-grotesk-extrabold {
  font-family: "Space Grotesk", sans-serif;
  font-optical-sizing: auto;
  font-weight: 700;
  font-style: normal;
}

.top-bar-container {
  position: absolute;
  top: 10px;
  left: 10px;
  right: 10px;
  display: flex;
  flex-wrap: wrap;
  justify-content: center;
  /* Default mobile layout: everything centered */
  align-items: center;
  gap: 10px;
  z-index: 1060;
  pointer-events: none;
}

.top-bar-section {
  flex: 1 1 auto;
  display: flex;
  justify-content: center;
  pointer-events: auto;
  align-self: flex-start; /* Ensure alignment to the top of the container */
}


/* On wider screens, switch to a spaced layout:
   - Legals on the left
   - Logo on the right
*/
@media (min-width: 1000px) {
  .top-bar-container {
    justify-content: space-between;
  }

  .legals {
    justify-content: flex-start;
  }

  .logo {
    justify-content: flex-end;
  }
}

