&&& FILE: ./.gitignore
&&& CONTENT:
# Ignore local data files
data_local/

# Python cache
__pycache__/
*.pyc

# Environment variables
.env

# Other ignores
*.so

*.gml
*.gfs

#venv
venv/
.venv/

&&& FILE: ./commands.md
&&& CONTENT:
### Generate Backend
Please create a backend  repo for my project. I will paste README of it in the end of this message.
I want to use FastAPI+PostGIS for it. I plan to start with 3d buildings, so paste parcels and terrain only as place holders. I plan to create 17 scripts in folder (/data_ingest) (one per bundesland) for uploading their specific .gml files (stored in /data_local/bundesland_name/file1.gml, file2.gml,..)

**___README___**

I want you to reply with ONE code block structured like this:

&&& FILE: path/to/file.ext
&&& CONTENT:
content
of
the
file

&&& FILE: path/to/another/file.ext
&&& CONTENT:
content
of
another
file

&&& FILE: ./README.md
&&& CONTENT:
# EasyOpenData
## Open Data Extractor for German Spatial Datasets

## Overview
This project aims to provide an easy-to-use platform for accessing and downloading spatial data from German open data sources, covering all Bundesländer. The system standardizes the diverse formats in which these datasets are available and presents them to users via an intuitive web interface.

Users can interact with a map, select areas of interest (via polygons or rectangles), and choose the data layers they wish to download. Available data types include:
- Parcels
- Terrain
- 3D Buildings (LOD1/LOD2)

The platform processes the selected data and provides it in the user's desired format after payment, either as a direct download or via email.

---

## Key Features
- **Map-Based User Interface**: Allows users to interact with spatial data visually by selecting areas on a map.
- **Multi-Format Support**: Data is standardized into a unified format, enabling seamless querying and export in multiple formats (e.g., GeoJSON, glTF).
- **Dynamic Data Processing**: Only fetches and processes data for the area selected by the user, ensuring efficiency.
- **Real-Time Visualization**: Users can preview data layers on the map as they select their areas of interest.
- **Scalable Backend**: Built for performance, capable of handling large datasets from multiple regions.

---

## Technologies Used

### **Backend**

- **PostGIS**: Spatial database for storing and querying geographic data efficiently.
- **Python FastAPI**: Backend framework for building APIs to handle user requests and interact with PostGIS.
- **GDAL**: Used for data conversion between formats like CityGML, GeoJSON, and glTF.
- **Py3Dtiles/Trimesh**: For converting 3D data to glTF format dynamically.
- **Docker**: Containerized deployment for portability and scalability.

### **Frontend**

- **React + Leaflet**: Interactive map interface for 2D visualization and area selection.
- **CesiumJS**: 3D visualization for previewing datasets like LOD1/LOD2 buildings.

### **Infrastructure**

- **PostgreSQL + PostGIS**: Backend database for spatial data storage and indexing.
- **NGINX**: For serving static files and proxying API requests.
- **Cloud Storage**: For storing processed datasets ready for download (e.g., AWS S3, Google Cloud Storage).

---

## Workflow

1. **Data Ingestion**:
   - Import spatial data from various Bundesländer open data portals.
   - Convert datasets into a unified format (e.g., CityGML or PostGIS-compatible geometry).

2. **Data Storage**:
   - Store standardized data in a PostGIS database with spatial indexing for efficient querying.

3. **User Interaction**:
   - Users draw polygons/rectangles on the map to define areas of interest.
   - Backend processes the request, fetching relevant data using spatial queries.

4. **Data Processing**:
   - Convert queried data to web-friendly formats:
     - GeoJSON for 2D previews.
     - glTF for 3D previews.
   - Generate downloadable files in formats requested by the user.

5. **Data Delivery**:
   - Users download processed data directly from the browser or receive it via email after payment.

---

## Project Goals

1. **Standardization**: Harmonize data formats across all Bundesländer for consistent access and processing.
2. **Ease of Use**: Create a user-friendly interface for both novice and advanced users.
3. **Scalability**: Design a backend architecture capable of handling large datasets and high user demand.
4. **Flexibility**: Enable the export of data in various formats (e.g., GeoJSON, glTF, Shapefile).

---

## Example Usage

### **Frontend**

1. User opens the webpage and views a map interface.
2. Selects an area of interest by drawing a polygon or rectangle.
3. Chooses the desired data layers (e.g., 3D buildings).
4. Proceeds to payment and downloads the processed data.

### **Backend**

1. Receives the user’s area and data type requests via API.
2. Queries PostGIS for intersecting objects within the selected area.
3. Dynamically converts data into the requested format (e.g., glTF).
4. Returns the processed data for download.

---

## Setup

### **Backend**

1. Install PostgreSQL and PostGIS:

   ```bash
   sudo apt install postgresql postgis
   ```

2. Clone the repository and install dependencies:

   ```bash
   git clone https://github.com/your-repo/open-data-extractor.git
   cd open-data-extractor/backend
   pip install -r requirements.txt
   ```

3. Import sample data into PostGIS:

   ```bash
   ogr2ogr -f "PostgreSQL" PG:"dbname=your_db user=your_user password=your_pass" sample_data.gml
   ```

4. Run the FastAPI server:

   ```bash
   uvicorn main:app --reload
   ```

### **Frontend**

1. Install Node.js and dependencies:

   ```bash
   cd open-data-extractor/frontend
   npm install
   ```

2. Run the React development server:

   ```bash
   npm start
   ```

---

## Future Roadmap

- Add support for additional data layers (e.g., terrain, parcels).
- Implement caching for frequently requested data.
- Introduce user accounts for managing downloads and licenses.
- Explore additional formats for exporting data (e.g., OBJ, IFC).
- Expand coverage beyond Germany to other European countries.

---

## Contributing

Contributions are welcome! Please fork the repository and submit a pull request with your changes. Ensure all code adheres to the style guide and includes proper documentation.

---

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.

---

## Contact

For questions, feedback, or support, contact us at:

- **Email**: support@opendataextractor.com

&&& FILE: ./backend\.env
&&& CONTENT:
DATABASE_URL=postgresql+asyncpg://postgres:barcelona@localhost:5432/easyopendata_database


&&& FILE: ./backend\database.py
&&& CONTENT:
import os
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from models import Base  # Import Base from models.py

DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql+asyncpg://user:password@localhost:5432/your_db')

engine = create_async_engine(DATABASE_URL, echo=True)
async_session = sessionmaker(
    bind=engine, class_=AsyncSession, expire_on_commit=False
)

# Function to create tables
async def init_db():
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)



&&& FILE: ./backend\main.py
&&& CONTENT:
from fastapi import FastAPI, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from database import async_session, init_db
from models import Building
from typing import List
from sqlalchemy.future import select
from geoalchemy2.functions import ST_AsGeoJSON, ST_Intersects, ST_GeomFromGeoJSON
import json

app = FastAPI()

@app.on_event("startup")
async def on_startup():
    await init_db()

async def get_db():
    async with async_session() as session:
        yield session

@app.get("/")
async def read_root():
    return {"message": "Hello World"}

@app.post("/buildings")
async def get_buildings(geometry: dict, db: AsyncSession = Depends(get_db)):
    """
    Get buildings within the given boundary.

    :param geometry: GeoJSON geometry defining the boundary.
    """
    try:
        # Convert input geometry (GeoJSON) to a string
        geometry_geojson = json.dumps(geometry)

        # Create a geometry object from the GeoJSON
        geom = ST_GeomFromGeoJSON(geometry_geojson)

        # Build the query
        query = select(
            Building.id,
            ST_AsGeoJSON(Building.geom).label('geometry')
        ).where(
            ST_Intersects(Building.geom, geom)
        )

        # Execute the query
        result = await db.execute(query)
        buildings = result.fetchall()

        # Construct GeoJSON response
        features = []
        for building in buildings:
            feature = {
                "type": "Feature",
                "geometry": json.loads(building.geometry),
                "properties": {
                    "id": building.id
                }
            }
            features.append(feature)

        geojson = {
            "type": "FeatureCollection",
            "features": features
        }

        return geojson

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))



&&& FILE: ./backend\models.py
&&& CONTENT:
from sqlalchemy import Column, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from geoalchemy2 import Geometry

Base = declarative_base()

class Building(Base):
    __tablename__ = 'buildings'

    id = Column(Integer, primary_key=True)
    name = Column(String)
    geom = Column(Geometry('GEOMETRYZ', srid=4326))  # Adjust geometry type as needed



&&& FILE: ./backend\requirements.txt
&&& CONTENT:
fastapi
uvicorn[standard]
asyncpg
psycopg2-binary
sqlalchemy
geoalchemy2
# gdal
# Additional dependencies
python-multipart  # For handling form data if needed


&&& FILE: ./backend\setup_database.md
&&& CONTENT:
# Database Setup Instructions for Windows

Follow these steps to create the PostgreSQL database with the PostGIS extension on Windows:

---

## 1. Install PostgreSQL and PostGIS

### **Download the Installer**

1. Visit the PostgreSQL official website: [PostgreSQL Downloads](https://www.postgresql.org/download/windows/).
2. Click on **"Download the installer"** to go to the EnterpriseDB download page.
3. Download the latest version of the PostgreSQL installer for Windows.

### **Run the Installer**

1. Run the downloaded `.exe` installer.
2. Follow the installation wizard steps:
   - **Installation Directory**: Choose your preferred installation directory.
   - **Select Components**: Ensure that **"PostGIS Bundle"** is checked to install PostGIS along with PostgreSQL.
   - **Password Setup**: Set a password for the default `postgres` superuser. Remember this password.
   - **Port Number**: Default is `5432`. You can change it if needed.
   - **Locale**: Choose the default locale or your preferred setting.
3. Complete the installation and wait for it to finish.

---

## 2. Create a New PostgreSQL Database and User

### **Open pgAdmin**

1. After installation, open **pgAdmin 4** from the Start Menu.
2. When prompted, enter the password you set for the `postgres` user during installation.

### **Create a New Database**

1. In pgAdmin, expand the server tree to see **"Databases"**.
2. Right-click on **"Databases"** and select **"Create"** > **"Database..."**.
3. In the **"Database"** dialog:
   - **Database Name**: Enter `your_db`.
   - **Owner**: Select `postgres` or your preferred user.
4. Click **"Save"**.

### **Create a New User (Role)**

1. Expand the **"Login/Group Roles"** under your server.
2. Right-click on **"Login/Group Roles"** and select **"Create"** > **"Login/Group Role..."**.
3. In the **"Properties"** tab:
   - **Role Name**: Enter `your_user`.
4. In the **"Definition"** tab:
   - **Password**: Enter `your_password`.
   - **Confirm Password**: Re-enter the password.
5. In the **"Privileges"** tab:
   - Set **"Can login?"** to **"Yes"**.
6. Click **"Save"**.

### **Grant Privileges to the User**

1. In pgAdmin, navigate to **"Databases"** > **"your_db"** > **"Schemas"** > **"public"**.
2. Right-click on **"public"** schema and select **"Properties"**.
3. Go to the **"Privileges"** tab.
4. Click on the **"Add"** icon (a plus sign).
5. In the new row:
   - **Role**: Select `your_user`.
   - **Privileges**: Check all the boxes (or at least **"Usage"** and **"Create"**).
6. Click **"Save"**.

### **Enable the PostGIS Extension**

1. Right-click on **"your_db"** and select **"Query Tool"**.
2. In the Query Editor, run the following SQL command:

   ```sql
   CREATE EXTENSION postgis;
   ```

3. Click the **"Execute/Refresh"** button (lightning bolt icon) to run the query.
4. You should see a message indicating that the extension was created successfully.

---

## 3. Update Your Database Configuration

### **Modify the `.env` File**

In your project directory, locate the `.env` file inside the `./backend` folder and update the `DATABASE_URL`:

```
DATABASE_URL=postgresql+asyncpg://your_user:your_password@localhost:5432/your_db
```

---

## 4. Run the Application to Create Tables

The `init_db()` function in `database.py` will automatically create the necessary tables when the application starts.

### **Open Command Prompt or PowerShell**

Navigate to your project's backend directory:

```cmd
cd path\to\your\project\backend
```

### **Create a Virtual Environment (Optional but Recommended)**

```cmd
python -m venv venv
venv\Scripts\activate
```

### **Install Dependencies**

Ensure all the required Python packages are installed:

```cmd
pip install -r requirements.txt
```

### **Start the FastAPI Application**

```cmd
uvicorn main:app --reload
```

---

## 5. Verify the Tables Have Been Created

### **Using pgAdmin**

1. In pgAdmin, right-click on **"Tables"** under **"your_db"** > **"Schemas"** > **"public"**, and select **"Refresh"**.
2. Expand the **"Tables"** section.
3. You should see the `buildings` table listed.

---

## 6. Ingest Data into the Database

Use the provided data ingestion scripts to populate the database with building data.

### **Install GDAL for Windows**

1. Download the GDAL Windows binaries from [GIS Internals](https://www.gisinternals.com/query.html?content=filelist&file=release-1930-x64-gdal-3-4-1-mapserver-7-6-4.zip).
2. Extract the contents to a directory (e.g., `C:\Program Files\GDAL`).
3. Add the GDAL bin directory to your system PATH:
   - Open **Control Panel** > **System** > **Advanced system settings**.
   - Click on **"Environment Variables"**.
   - Under **"System Variables"**, find and edit the **"Path"** variable.
   - Add the path to the GDAL `bin` directory (e.g., `C:\Program Files\GDAL\bin`).
4. Set the `GDAL_DATA` environment variable:
   - In **"System Variables"**, click **"New"**.
   - **Variable name**: `GDAL_DATA`
   - **Variable value**: `C:\Program Files\GDAL\gdal-data`

### **Place Your GML Files**

Add your `.gml` files into the appropriate directories under `data_local\{bundesland_name}\`.

### **Run the Ingestion Script**

```cmd
python data_ingest\ingest_baden_wuerttemberg.py
```

Replace `ingest_baden_wuerttemberg.py` with the script corresponding to your Bundesland.

**Note**: You may need to install `osgeo` dependencies for GDAL to work with Python scripts.

---

## 7. Test the Endpoint

You can now test the `/buildings` endpoint to retrieve buildings within a given boundary.

### **Example Request Using cURL**

```cmd
curl -X POST "http://localhost:8000/buildings" -H "Content-Type: application/json" -d "{\"type\":\"Polygon\",\"coordinates\":[[[9.0,48.0],[9.1,48.0],[9.1,48.1],[9.0,48.1],[9.0,48.0]]]}"
```

### **Example Request Using Python**

```python
import requests

url = "http://localhost:8000/buildings"
geometry = {
    "type": "Polygon",
    "coordinates": [
        [
            [9.0, 48.0],
            [9.1, 48.0],
            [9.1, 48.1],
            [9.0, 48.1],
            [9.0, 48.0]
        ]
    ]
}

response = requests.post(url, json=geometry)
print(response.json())
```

---

## Notes

- Ensure that the SRID (Spatial Reference System Identifier) matches between your data and the database. The default SRID in the model is `4326`.
- If you encounter any issues, check the application logs for errors and verify your database connection settings.
- Make sure that the `psycopg2` package is installed properly. On Windows, you might need to install `psycopg2-binary`.

---

## Troubleshooting

### **Common Issues**

- **GDAL Not Found**: Ensure that GDAL is correctly installed and added to your system PATH.
- **Database Connection Errors**: Double-check your `DATABASE_URL` in the `.env` file.
- **Permission Denied**: Run Command Prompt or PowerShell as an administrator if you encounter permission issues.
- **Port Conflicts**: Ensure that port `5432` (PostgreSQL) and `8000` (FastAPI default) are not being used by other applications.

---

## Additional Resources

- [PostgreSQL Documentation](https://www.postgresql.org/docs/)
- [PostGIS Documentation](https://postgis.net/documentation/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [GDAL Documentation](https://gdal.org/)



&&& FILE: ./backend\data_ingest\ingest_baden_wuerttemberg.py
&&& CONTENT:
import os
import glob
import subprocess

DATA_DIR = 'data_local/baden_wuerttemberg'
DATABASE_URL = os.getenv('DATABASE_URL', 'PG:dbname=your_db user=your_user password=your_pass')

def ingest_gml_files():
    gml_files = glob.glob(os.path.join(DATA_DIR, '*.gml'))
    for gml_file in gml_files:
        cmd = [
            'ogr2ogr',
            '-f', 'PostgreSQL',
            '-overwrite',
            '-progress',
            DATABASE_URL,
            gml_file
        ]
        subprocess.run(cmd)

if __name__ == '__main__':
    ingest_gml_files()



&&& FILE: ./backend\data_ingest\ingest_bayern.py
&&& CONTENT:
import os
import glob
import subprocess

DATA_DIR = 'backend/data_local/bayern'
DATABASE_URL = os.getenv('DATABASE_URL', 'PG:dbname=easyopendata_database user=postgres password=barcelona')

def ingest_gml_files():
    gml_files = glob.glob(os.path.join(DATA_DIR, '*.gml'))
    for gml_file in gml_files:
        cmd = [
            'ogr2ogr',
            '-f', 'PostgreSQL',
            '-overwrite',
            '-progress',
            DATABASE_URL,
            gml_file
        ]
        subprocess.run(cmd)

if __name__ == '__main__':
    ingest_gml_files()




&&& FILE: ./backend\data_local\README.md
&&& CONTENT:
# Data Local Directory

Place your local GML data files here under their respective Bundesland directories.

Example:

- data_local/baden_wuerttemberg/file1.gml
- data_local/bayern/file2.gml

Ensure the directory structure matches the Bundesland names used in the data_ingest scripts.



&&& FILE: ./utils\small-portion.py
&&& CONTENT:
import json

# Path to the large GeoJSON file
input_file = r"output_dir\650_5478_repr.geojson"

# Path for the output smaller GeoJSON file
output_file = "650_5478_repr_small.geojson"

# Desired size of the output file in bytes (50 MB)
desired_size = 1 * 1024 * 1024

def extract_small_geojson(input_file, output_file, max_size):
    try:
        with open(input_file, 'r', encoding='utf-8') as infile:
            # Initialize variables to construct the new GeoJSON
            small_data = {
                "type": "FeatureCollection",
                "features": []
            }
            
            # Read the opening part of the GeoJSON file
            line = infile.readline()
            while line.strip() != '"features": [':
                line = infile.readline()

            # Read features one by one
            current_size = 0
            for line in infile:
                if line.strip() == ']':
                    break  # End of features list

                # Remove trailing comma if present
                feature_str = line.rstrip(',\n')
                
                # Parse feature JSON
                feature = json.loads(feature_str)
                
                # Add feature to the new collection
                small_data["features"].append(feature)
                
                # Update current size
                current_size += len(feature_str.encode('utf-8'))
                
                # Stop if the current size exceeds or reaches the desired size
                if current_size >= max_size:
                    break

            # Write closing brackets for the JSON structure
            with open(output_file, 'w', encoding='utf-8') as outfile:
                json.dump(small_data, outfile, ensure_ascii=False, indent=2)
            
            print(f"Extracted {len(small_data['features'])} features into {output_file}")

    except Exception as e:
        print(f"An error occurred: {e}")

# Run the function to extract a smaller GeoJSON file
extract_small_geojson(input_file, output_file, desired_size)

