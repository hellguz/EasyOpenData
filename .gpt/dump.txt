&&& FILE: ./.env
&&& CONTENT:
VITE_BASE_URL = https://easyopen-server.i-am-hellguz.uk
VITE_TILESET_URL= https://easyopen-tiles.i-am-hellguz.uk/tileset.json

&&& FILE: ./.gitignore
&&& CONTENT:
# Ignore local data files
data_local/

# Python cache
__pycache__/
*.pyc

# Environment variables
.env

# Other ignores
*.so

# tileset
backend/tileset*/
!backend/tileset/README.md

*.gml
*.gfs
*.obj

#venv
venv/
.venv/
.conda

node_modules/

postgres_data/*
postgres_backups/*

data/

&&& FILE: ./docker-compose.yml
&&& CONTENT:
services:
  easyopen_postgis:
    container_name: easyopen_postgis
    image: postgis/postgis:17-3.5
    restart: always
    ports:
      - 8735:5432
    environment:
      POSTGRES_PASSWORD: barcelona
    volumes:
      - ./data/postgres_data:/var/lib/postgresql/data
      - ./backend/db/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 10s
      timeout: 5s
      retries: 20
    networks:
      - easyopen_network

  easyopen_backend:
    container_name: easyopen_backend
    build:
      context: ./backend
      dockerfile: Dockerfile
    command: uvicorn app.main:app --host 0.0.0.0 --port 5400 --reload
    volumes:
      - ./backend:/app
      - ./data/tileset:/data/tileset
      - ./data/tileset_combined:/data/tileset_combined
      - ./data/tempfiles:/data/tempfiles
    ports:
      - 5400:5400
    environment:
      DATABASE_URL: postgresql+asyncpg://postgres:barcelona@easyopen_postgis:5432/easyopendata_database
    depends_on:
      easyopen_postgis:
        condition: service_healthy
    networks:
      - easyopen_network

  easyopen_frontend:
    container_name: easyopen_frontend
    build:
      context: ./frontend
      dockerfile: Dockerfile
    volumes:
      - ./frontend:/app
      - /app/node_modules
    ports:
      - 5173:5173
    environment:
      VITE_BACKEND_URL: https://easyopen-server.i-am-hellguz.uk
    networks:
      - easyopen_network
      

  easyopen_tileset:
    container_name: easyopen_tileset
    image: nginx:alpine
    restart: always
    ports:
      - 5576:80
    volumes:
      - ./data/tileset:/usr/share/nginx/html:ro
      - ./backend/tileset.conf:/etc/nginx/conf.d/default.conf:ro
    networks:
      - easyopen_network

  easyopen_backup_service:
    build: ./backend/db_backup # path to your Dockerfile with cron and pg_dump
    container_name: easyopen_backup_service
    depends_on:
      - easyopen_postgis
    volumes:
      - ./data/postgres_backups:/backups
    environment:
      POSTGRES_PASSWORD: barcelona
    networks:
      - easyopen_network

  easyopen_pgadmin:
    container_name: easyopen_pgadmin
    image: dpage/pgadmin4
    restart: always
    ports:
      - 5050:80
    environment:
      PGADMIN_DEFAULT_EMAIL: hellguz@gmail.com
      PGADMIN_DEFAULT_PASSWORD: barcelona
    depends_on:
      - easyopen_postgis
    networks:
      - easyopen_network

  easyopen_cloudflared:
    container_name: easyopen_cloudflared
    image: cloudflare/cloudflared:latest
    depends_on:
      - easyopen_backend
    restart: always
    command: 'tunnel --no-autoupdate run --token eyJhIjoiMjViZjczZmU2MmZlMzQwN2Y3MTI4NGU4MjZlZGQ0MjEiLCJ0IjoiYjAwODUzY2QtYmE1MC00MzA0LTg3NzktMWQ1Y2RjMzI4YWIxIiwicyI6IlltUTNOMlpqWTJVdE1tVmpZaTAwWmpoaUxUZzRPVGt0WkRVMlpUUmhZVE5sTkdWaiJ9'
    networks:
      - easyopen_network

volumes:
  postgres_data:

networks:
  easyopen_network:
    driver: bridge

&&& FILE: ./README.md
&&& CONTENT:
# EasyOpenData

## Open Data Extractor for German Spatial Datasets

### Overview

EasyOpenData is a platform that provides an easy-to-use interface for accessing and downloading spatial data from Germ
................................

&&& FILE: ./backend\Dockerfile
&&& CONTENT:
# ./backend/Dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir -r /app/requirements.txt

COPY . /app

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]


&&& FILE: ./backend\README.md
&&& CONTENT:
# EasyOpenData Backend

## Overview

The backend of EasyOpenData is a FastAPI application that serves as the core of the platform, handling data retrieval, processing, and API endpoints for the fronte
................................

&&& FILE: ./backend\requirements.txt
&&& CONTENT:
fastapi
uvicorn[standard]
asyncpg
psycopg2-binary
sqlalchemy
geoalchemy2
# Additional dependencies
python-multipart
geojson
anyio
pydantic
packaging
redis
stripe
pyproj
lxml
shapely

&&& FILE: ./backend\tileset.conf
&&& CONTENT:
server {
    listen 80;
    server_name localhost;

    root /usr/share/nginx/html;

    # Add the CORS header
    add_header 'Access-Control-Allow-Origin' '*' always;

    # If you want to allow specific headers or methods, add them too
    # add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS' always;
    # add_header 'Access-Control-Allow-Headers' 'Authorization, Content-Type' always;

    location / {
        try_files $uri $uri/ =404;
    }
}


&&& FILE: ./backend\app\database.py
&&& CONTENT:
import os
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from app.models import Base

DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql+asyncpg://postgres:barcelona@localhost:8735/easyopendata_database')

engine = create_async_engine(DATABASE_URL, echo=True)
async_session = sessionmaker(
    bind=engine, class_=AsyncSession, expire_on_commit=False
)

# Function to create tables
async def init_db():
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)



&&& FILE: ./backend\app\main.py
&&& CONTENT:
# ./backend/main.py

import asyncio
import tempfile
from typing import List
from urllib import request
import uuid
from fastapi import FastAPI, Depends, HTTPException
from pydantic import BaseModel
from sqlalchemy.ext.asyncio import AsyncSession
import stripe
from app.database import async_session, init_db
from app.models import Building, RegionRequest
from app.retrieve_geom import retrieve_obj_file
from sqlalchemy.future import select
from geoalchemy2.functions import ST_AsGeoJSON, ST_Intersects, ST_GeomFromText, ST_SimplifyPreserveTopology
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
import json
import math
import redis
import subprocess
import os
import logging
from fastapi.staticfiles import StaticFiles

import shutil

# Configure Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()


app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", os.getenv("VITE_BASE_URL")], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def on_startup():
    await init_db()

async def get_db():
    async with async_session() as session:
        yield session

@app.get("/")
async def read_root():
    return {"message": "Easy Open Data v1.0"}

@app.post("/retrieve_obj")
async def retrieve_obj(request: RegionRequest):
    print(f"Received region: {request.region}")

    try:
        # Generate a unique filename using UUID
        random_filename = f"{uuid.uuid4()}.obj"
        temp_path = os.path.join("/data/tempfiles", random_filename)
        await retrieve_obj_file(request.region, temp_path)
        return FileResponse(
            temp_path,
            media_type="application/octet-stream",
            filename=f"object.txt")
    except Exception as e:
        logger.error(f"Error in retrieve_obj: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


async def remove_temp_file(file_path: str):
    await asyncio.sleep(0)  # Ensure this runs after the response is sent
    if os.path.exists(file_path):
        os.unlink(file_path)

        
stripe.api_key = 'REMOVED_STRIPE_KEY'


def calculate_order_amount(amount: float):
    # Replace this constant with a calculation of the order's amount
    # Calculate the order total on the server to prevent
    # people from directly manipulating the amount on the client
    return int(amount*100)

class PaymentIntentRequest(BaseModel): 
    amount: float

@app.post("/create-payment-intent")
async def create_payment_intent(data: PaymentIntentRequest):
    try:
        intent = stripe.PaymentIntent.create(
            amount=calculate_order_amount(data.amount),  # Amount in cents
            currency="eur",
            automatic_payment_methods={"enabled": True},
        )
        return {"clientSecret": intent.client_secret}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
app.mount("/tileset", StaticFiles(directory="../data/tileset"), name="tileset")


&&& FILE: ./backend\app\models.py
&&& CONTENT:
from sqlalchemy import Column, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from geoalchemy2 import Geometry

Base = declarative_base()

class Building(Base):
    __tablename__ = 'building'

    gml_id = Column(String, primary_key=True)
    name = Column(String)
    geom = Column(Geometry('MULTIPOLYGONZ', srid=4326))  # Adjust geometry type as needed

from pydantic import BaseModel

class RegionRequest(BaseModel):
    region: dict  # Adjust the type if you have a more specific structure


&&& FILE: ./backend\app\retrieve_geom.py
&&& CONTENT:
# ./backend/retrieve_obj.py

import asyncio
import json
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from geoalchemy2 import functions as func
from app.database import async_session
from app.models import Building
import os
from pyproj import Transformer

async def retrieve_obj_file(region_geojson, output_path):
    """
    Generates an OBJ file with buildings within the given polygon region.

    Args:
        region_geojson (dict): The GeoJSON representing the input region.
        output_path (str): The file path where the OBJ file will be saved.
    """
    # Parse the input GeoJSON to get the polygon geometry
    features = region_geojson.get('features', [])
    if not features:
        raise ValueError("No features found in the input GeoJSON.")
    
    polygon_feature = features[0]
    polygon_geometry = polygon_feature.get('geometry', {})
    if polygon_geometry.get('type') != 'Polygon':
        raise ValueError("The geometry must be of type 'Polygon'.")

    # Convert GeoJSON geometry to GeoJSON string
    polygon_geojson_str = json.dumps(polygon_geometry)

    # Choose the appropriate projection (e.g., UTM zone 32N for Germany)
    # You may need to adjust the EPSG code based on your location
    source_crs = 'EPSG:4326'  # WGS84 Latitude/Longitude
    target_crs = 'EPSG:25832'  # ETRS89 / UTM zone 32N (adjust as needed)

    # Create a Transformer object for coordinate transformation
    transformer = Transformer.from_crs(source_crs, target_crs, always_xy=True)

    # Clean and validate the user polygon once
    polygon = func.ST_SetSRID(func.ST_GeomFromGeoJSON(polygon_geojson_str), 4326)
    polygon = func.ST_MakeValid(polygon)
    polygon = func.ST_Force3D(polygon)
    polygon = func.ST_Buffer(polygon, 0)
    
    # Start an asynchronous database session
    async with async_session() as session:
        # Query the database for buildings within the polygon
        stmt = select(
            Building.gml_id,
            func.ST_AsGeoJSON(Building.geom).label('geom_geojson')
        ).where(
            func.ST_Intersects(
                func.ST_MakeValid(Building.geom),
                polygon
            )
        )
        
        result = await session.execute(stmt)
        buildings = result.fetchall()
        
        if not buildings:
            print("No buildings found within the given region.")
            return

        # Initialize lists to store OBJ data
        obj_vertices = []
        obj_faces = []
        vertex_offset = 0  # Offset for indexing vertices in faces

        # Process each building geometry
        for building in buildings:
            gml_id = building.gml_id
            geom_geojson_str = building.geom_geojson
            if not geom_geojson_str:
                continue  # Skip if geometry is null

            # Load geometry from GeoJSON
            geom_geojson = json.loads(geom_geojson_str)

            # Handle Polygon and MultiPolygon geometries
            geom_type = geom_geojson.get('type')
            coordinates = geom_geojson.get('coordinates')

            if geom_type == 'Polygon':
                polygons = [coordinates]
            elif geom_type == 'MultiPolygon':
                polygons = coordinates
            else:
                print(f"Skipping unsupported geometry type (ID: {gml_id}, Type: {geom_type})")
                continue

            for polygon in polygons:
                ring_vertex_indices = []

                # Process exterior ring
                exterior_coords = polygon[0]
                exterior_indices = []
                for coord in exterior_coords:
                    lon, lat = coord[:2]
                    z = coord[2] if len(coord) > 2 else 0
                    # Transform coordinates
                    x, y = transformer.transform(lon, lat)
                    obj_vertices.append(f"v {x} {z} {y}")
                    vertex_offset += 1
                    exterior_indices.append(vertex_offset)
                # Add face for exterior ring
                obj_faces.append(f"f {' '.join(map(str, exterior_indices))}")

                # Process interior rings (holes)
                for interior_coords in polygon[1:]:
                    interior_indices = []
                    for coord in interior_coords:
                        lon, lat = coord[:2]
                        z = coord[2] if len(coord) > 2 else 0
                        # Transform coordinates
                        x, y = transformer.transform(lon, lat)
                        obj_vertices.append(f"v {x} {z} {y}")
                        vertex_offset += 1
                        interior_indices.append(vertex_offset)
                    # Add face for interior ring (negative indices to denote holes are not standard in OBJ)
                    # Some software may not support holes directly
                    # So, we can skip adding faces for holes or handle them as separate objects
                    # For now, we'll skip adding faces for holes
                    print(f"Skipping interior ring (hole) in building ID: {gml_id}")

        # Write to OBJ file
        with open(output_path, 'w') as obj_file:
            obj_file.write("# OBJ file generated from buildings\n")
            obj_file.write("\n".join(obj_vertices))
            obj_file.write("\n")
            obj_file.write("\n".join(obj_faces))

        print(f"OBJ file successfully written to {output_path}")
        return


&&& FILE: ./backend\db\index.sql
&&& CONTENT:
-- Create the index if it doesn't exist
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 
        FROM pg_indexes 
        WHERE schemaname = 'public' 
        AND tablename = 'building' 
        AND indexname = 'idx_makevalid_geom'
    ) THEN
        CREATE INDEX buildings_geom_idx ON building USING GIST (ST_MakeValid(geom));
    END IF;
END $$;

-- Add the unique constraint if it doesn't exist
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 
        FROM pg_constraint 
        WHERE conname = 'gml_id_unique' 
        AND conrelid = 'public.building'::regclass
    ) THEN
        ALTER TABLE public.building ADD CONSTRAINT gml_id_unique UNIQUE (gml_id);
    END IF;
END $$;

-- ALTER TABLE building
-- ADD CONSTRAINT enforce_geotype_geom CHECK (GeometryType(geom) = 'GEOMETRYCOLLECTION' OR geom IS NULL);


-- Make all geometry entries valid or empty
-- If they aren't all MultiPolygonZ, convert them now
-- Turn off NOTICE output first and restore it after this operation

-- SET client_min_messages = WARNING;

-- UPDATE building
-- SET geom = ST_Force3D(ST_CollectionExtract(ST_MakeValid(geom), 3))
-- WHERE NOT ST_IsValid(geom) OR GeometryType(geom) != 'MULTIPOLYGONZ';

-- RESET client_min_messages;


-- Cluster the table using the index
-- CLUSTER building USING buildings_geom_idx;

-- Analyze to update statistics
ANALYZE building;


&&& FILE: ./backend\db\init.sql
&&& CONTENT:
-- Part 1: Database and extension creation (in transaction)
CREATE DATABASE easyopendata_database;
\c easyopendata_database;
CREATE EXTENSION IF NOT EXISTS postgis;

-- Part 2: System settings (must be outside transaction)
\connect easyopendata_database
\echo 'Setting system parameters...'

\set ON_ERROR_STOP off
ALTER SYSTEM SET maintenance_work_mem TO '1GB';
ALTER SYSTEM SET work_mem TO '256MB';
ALTER SYSTEM SET max_parallel_workers_per_gather TO 4;
\set ON_ERROR_STOP on



&&& FILE: ./backend\db_backup\backup_script.sh
&&& CONTENT:
#!/bin/bash

BACKUP_DIR=/backups
mkdir -p $BACKUP_DIR
export PGPASSWORD="barcelona"

# Create a new backup
NEW_BACKUP=$BACKUP_DIR/backup_$(date +%Y-%m-%d_%H-%M-%S).dump
pg_dump -U postgres -h easyopen_postgis -F c easyopendata_database > $NEW_BACKUP

# Check if the backup was successful
if [ $? -eq 0 ]; then
    echo "Backup created: $NEW_BACKUP"
else
    echo "Backup failed!" >&2
    exit 1
fi

# Delete old backups, keeping only the last 3
NUM_BACKUPS_TO_KEEP=5
BACKUP_COUNT=$(ls -1 $BACKUP_DIR | wc -l)

if [ $BACKUP_COUNT -gt $NUM_BACKUPS_TO_KEEP ]; then
    echo "Cleaning up old backups..."
    ls -1t $BACKUP_DIR | tail -n +$(($NUM_BACKUPS_TO_KEEP + 1)) | while read OLD_BACKUP; do
        rm -f "$BACKUP_DIR/$OLD_BACKUP"
        echo "Deleted old backup: $BACKUP_DIR/$OLD_BACKUP"
    done
fi

 # To restore, connect shell to easyopen_backup_service container and run
 # pg_restore -U postgres -h easyopen_postgis -d easyopendata_database --clean --if-exists -j 4 /backups/backup_2024-12-08_12-46-08.dump         

&&& FILE: ./backend\db_backup\Dockerfile
&&& CONTENT:
FROM postgres:17
RUN apt-get update && apt-get install -y cron && rm -rf /var/lib/apt/lists/*
COPY backup_script.sh /usr/local/bin/backup_script.sh
RUN chmod +x /usr/local/bin/backup_script.sh

# Add cron job
RUN echo "0 0 * * * root /usr/local/bin/backup_script.sh" >> /etc/crontab

CMD ["cron", "-f"]


&&& FILE: ./backend\ingestion\bayern.py
&&& CONTENT:
#!/usr/bin/env python3
"""
process_meta4.py

A script to sequentially download GML files from a Meta4 file, transform them by embedding polygons,
ingest them into a PostgreSQL database using a temporary table, convert them to 3D tiles,
append to the main building table, and remove the original files.

Usage:
    python process_meta4.py file.meta4

Requirements:
    - Python 3.x
    - lxml
    - psycopg2
    - ogr2ogr (from GDAL)
    - pg2b3dm_new command available in PATH
    - gltf-pipeline (for Draco compression)
"""

import sys
import os
import subprocess
import hashlib
import logging
import shutil
from urllib.parse import urlparse
from urllib.request import urlopen, Request
from urllib.error import URLError, HTTPError
from lxml import etree
import psycopg2

# Constants
META4_PATH = 'backend/ingestion/data_sources/bayern.meta4'
DATA_DIR = 'backend/ingestion/data_local/bayern'
DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql://postgres:barcelona@localhost:8735/easyopendata_database')
CACHE_DIR = 'data/tileset'
PG2B3DM_PATH = 'backend/ingestion/libs/pg2b3dm.exe'
SQL_INDEX_PATH = 'backend/db/index.sql'
TEMP_TABLE = 'idx_building'  # Temporary table name
MAIN_TABLE = 'building'      # Main building table name
BATCH_N = 20 # number of gml files for which there will be created a separate tileset

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

def parse_meta4(meta4_file):
    """
    Parses the Meta4 XML file and extracts file information.

    Args:
        meta4_file (str): Path to the Meta4 XML file.

    Returns:
        list of dict: List containing information about each file.
    """
    logging.info(f"Parsing Meta4 file: {meta4_file}")
    tree = etree.parse(meta4_file)
    root = tree.getroot()
    ns = {'metalink': 'urn:ietf:params:xml:ns:metalink'}

    files = []
    for file_elem in root.findall('metalink:file', namespaces=ns):
        name = file_elem.get('name')
        size = int(file_elem.find('metalink:size', namespaces=ns).text)
        hash_elem = file_elem.find('metalink:hash', namespaces=ns)
        hash_type = hash_elem.get('type')
        hash_value = hash_elem.text
        urls = [url_elem.text for url_elem in file_elem.findall('metalink:url', namespaces=ns)]
        files.append({
            'name': name,
            'size': size,
            'hash_type': hash_type,
            'hash_value': hash_value,
            'urls': urls
        })
    logging.info(f"Found {len(files)} files in Meta4.")
    return files

def download_file(url, dest_path):
    """
    Downloads a file from a URL to a destination path.

    Args:
        url (str): URL to download from.
        dest_path (str): Destination file path.

    Returns:
        bool: True if download was successful, False otherwise.
    """
    try:
        logging.info(f"Downloading from URL: {url}")
        headers = {'User-Agent': 'Mozilla/5.0'}
        req = Request(url, headers=headers)
        with urlopen(req) as response, open(dest_path, 'wb') as out_file:
            shutil.copyfileobj(response, out_file)
        logging.info(f"Downloaded file to: {dest_path}")
        return True
    except HTTPError as e:
        logging.warning(f"HTTP Error: {e.code} when downloading {url}")
    except URLError as e:
        logging.warning(f"URL Error: {e.reason} when downloading {url}")
    except Exception as e:
        logging.warning(f"Unexpected error when downloading {url}: {e}")
    return False

def verify_file(file_path, expected_size, expected_hash, hash_type='sha-256'):
    """
    Verifies the size and hash of a downloaded file.

    Args:
        file_path (str): Path to the file.
        expected_size (int): Expected file size in bytes.
        expected_hash (str): Expected hash value.
        hash_type (str): Hash algorithm, default 'sha-256'.

    Returns:
        bool: True if verification succeeds, False otherwise.
    """
    logging.info(f"Verifying file: {file_path}")
    # Check size
    actual_size = os.path.getsize(file_path)
    if actual_size != expected_size:
        logging.error(f"Size mismatch for {file_path}: expected {expected_size}, got {actual_size}")
        return False
    # Check hash
    hash_func = hashlib.new(hash_type)
    with open(file_path, 'rb') as f:
        for chunk in iter(lambda: f.read(8192), b''):
            hash_func.update(chunk)
    actual_hash = hash_func.hexdigest()
    if actual_hash.lower() != expected_hash.lower():
        logging.error(f"Hash mismatch for {file_path}: expected {expected_hash}, got {actual_hash}")
        return False
    logging.info(f"Verification passed for {file_path}")
    return True

def get_all_namespaces(gml_tree):
    """
    Extracts all namespaces from the GML tree and assigns a unique prefix to the default namespace.

    Args:
        gml_tree (etree.ElementTree): Parsed GML tree.

    Returns:
        dict: Namespace prefix to URI mapping.
    """
    nsmap = gml_tree.getroot().nsmap.copy()
    # Handle default namespace (None key)
    if None in nsmap:
        nsmap['default'] = nsmap.pop(None)
    # Ensure 'xlink' is included
    if 'xlink' not in nsmap:
        # Attempt to find the xlink namespace
        for prefix, uri in nsmap.items():
            if uri == 'http://www.w3.org/1999/xlink':
                nsmap['xlink'] = uri
                break
        else:
            # If not found, add it manually
            nsmap['xlink'] = 'http://www.w3.org/1999/xlink'
    return nsmap

def transform_gml(input_file, output_file):
    """
    Transforms the input GML file by embedding polygons into surfaceMember elements.

    Args:
        input_file (str): Path to the input GML file.
        output_file (str): Path to the output transformed GML file.
    """
    # Parse the GML file
    logging.info(f"Parsing input GML file: {input_file}")
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(input_file, parser)
    root = tree.getroot()

    # Extract all namespaces
    namespaces = get_all_namespaces(tree)
    # logging.debug("Namespaces detected:")
    # for prefix, uri in namespaces.items():
    #     logging.debug(f"  Prefix: '{prefix}' => URI: '{uri}'")

    # Build a dictionary of gml:id to Polygon elements for quick lookup
    logging.info("Indexing all <gml:Polygon> elements by gml:id...")
    polygon_dict = {}
    for polygon in root.xpath('.//gml:Polygon', namespaces=namespaces):
        polygon_id = polygon.get('{http://www.opengis.net/gml}id')
        if polygon_id:
            polygon_dict[polygon_id] = polygon
    logging.info(f"Indexed {len(polygon_dict)} polygons.")

    # Find all <gml:surfaceMember> elements with xlink:href
    logging.info("Finding all <gml:surfaceMember> elements with xlink:href...")
    surface_members = root.xpath('.//gml:surfaceMember[@xlink:href]', namespaces=namespaces)
    logging.info(f"Found {len(surface_members)} <gml:surfaceMember> elements with xlink:href.")

    for sm in surface_members:
        href = sm.get('{http://www.w3.org/1999/xlink}href')
        if not href:
            continue
        # Extract the referenced polygon ID (remove the '#' prefix)
        polygon_id = href.lstrip('#')
        # logging.debug(f"Processing surfaceMember referencing Polygon ID: {polygon_id}")
        polygon = polygon_dict.get(polygon_id)
        if not polygon:
            logging.warning(f"Polygon with gml:id='{polygon_id}' not found. Skipping.")
            continue
        # Deep copy the polygon element
        polygon_copy = etree.fromstring(etree.tostring(polygon))
        # Remove any existing 'gml:id' to avoid duplicate IDs
        polygon_copy.attrib.pop('{http://www.opengis.net/gml}id', None)
        # Replace the surfaceMember's xlink:href attribute with the actual Polygon
        sm.clear()  # Remove existing children and attributes
        sm.append(polygon_copy)
        # logging.debug(f"Embedded Polygon ID: {polygon_id} into surfaceMember.")

    # Optionally, remove standalone <gml:Polygon> elements that were referenced
    # logging.info("Removing standalone <gml:Polygon> elements that were referenced...")
    # removed_count = 0
    # for polygon_id in polygon_dict.keys():
    #     # Find and remove the standalone polygon
    #     polygons_to_remove = root.xpath(f'.//gml:Polygon[@gml:id="{polygon_id}"]', namespaces=namespaces)
    #     for polygon in polygons_to_remove:
    #         parent = polygon.getparent()
    #         if parent is not None:
    #             parent.remove(polygon)
    #             removed_count += 1
    #             logging.debug(f"Removed standalone Polygon ID: {polygon_id}.")
    # logging.info(f"Removed {removed_count} standalone polygons.")

    # Write the transformed GML to the output file
    logging.info(f"Writing transformed GML to: {output_file}")
    tree.write(output_file, pretty_print=True, xml_declaration=True, encoding='UTF-8')
    logging.info("Transformation complete.")

def ingest_gml_file(gml_file, database_url, table_name):
    """
    Ingests a GML file into a PostgreSQL database using ogr2ogr into a specified table.

    Args:
        gml_file (str): Path to the GML file.
        database_url (str): PostgreSQL connection URL.
        table_name (str): Target table name for ingestion.
    """
    logging.info(f"Ingesting GML file into database table '{table_name}': {gml_file}")
    cmd = [
        'ogr2ogr',
        '-f', 'PostgreSQL',
        '-nln', table_name,              # Specify the target table name
        '-progress',
        '-lco', 'GEOMETRY_NAME=geom',
        '-skipfailures',
        '-nlt', 'MULTIPOLYGONZ',
        '-dim', 'XYZ',
        '-s_srs', 'EPSG:25832',
        '-t_srs', 'EPSG:4326',
        database_url,
        gml_file
    ]
    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    if result.returncode != 0:
        logging.error(f"ogr2ogr failed for {gml_file}: {result.stderr}")
        raise RuntimeError(f"ogr2ogr failed: {result.stderr}")
    logging.info(f"Ingested {gml_file} into table '{table_name}' successfully.")

def execute_sql_file(sql_file_path, database_url):
    """Executes a SQL file in the database."""
    logging.info(f"Executing SQL file: {sql_file_path}")
    url = urlparse(database_url)
    conn = psycopg2.connect(
        dbname=url.path[1:],
        user=url.username,
        password=url.password,
        host=url.hostname,
        port=url.port
    )
    try:
        with conn.cursor() as cur:
            with open(sql_file_path, 'r') as f:
                cur.execute(f.read())
            conn.commit()
        logging.info("SQL file executed successfully")
    except Exception as e:
        logging.error(f"Failed to execute SQL file: {e}")
        conn.rollback()
        raise
    finally:
        conn.close()

def update_geometries(database_url, table_name):
    """
    Updates the geometries in the specified table to put them on ground level.

    Args:
        database_url (str): PostgreSQL connection URL.
        table_name (str): Table to update.
    """
    logging.info(f"Updating geometries to ground level in table '{table_name}'.")
    url = urlparse(database_url)
    conn = psycopg2.connect(
        dbname=url.path[1:],
        user=url.username,
        password=url.password,
        host=url.hostname,
        port=url.port
    )
    try:
        with conn.cursor() as cur:
            cur.execute(f"""
                UPDATE {table_name}
                SET geom = ST_Translate(geom, 0, 0, -ST_ZMin(geom))
                WHERE ST_ZMin(geom) != 0;
            """)
            conn.commit()
        logging.info(f"Geometries in table '{table_name}' updated successfully.")
    except Exception as e:
        logging.error(f"Failed to update geometries in table '{table_name}': {e}")
        conn.rollback()
        raise
    finally:
        conn.close()

def convert_to_3d_tiles(cache_dir, database_url, table_name):
    """
    Converts buildings from the specified table in the database to 3D tiles using pg2b3dm.

    Args:
        cache_dir (str): Output directory for 3D tiles.
        database_url (str): PostgreSQL connection URL.
        table_name (str): Table to convert to 3D tiles.
    """
    logging.info(f"Converting table '{table_name}' to 3D tiles with pg2b3dm.")
    # Parse the database URL for parameters
    url = urlparse(database_url)
    dbname = url.path[1:]
    user = url.username
    host = url.hostname or 'localhost'
    port = url.port
    # Assume password is handled via environment or .pgpass
    cmd = [
        PG2B3DM_PATH,
        '-h', f"{host}:{port}",
        '-U', user,
        '-c', 'geom',
        '-t', table_name,
        '-d', dbname,
        '-o', cache_dir, 
         '--use_implicit_tiling', 'false'  # Uncomment if needed
    ]
    # To handle password, set PGPASSWORD environment variable if available
    env = os.environ.copy()
    if url.password:
        env['PGPASSWORD'] = url.password
    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, env=env)
    if result.returncode != 0:
        logging.error(f"pg2b3dm failed: {result.stderr}")
        raise RuntimeError(f"pg2b3dm failed: {result.stderr}")
    logging.info("3D tiles generated successfully.")

def apply_draco_compression(cache_dir):
    """
    Applies Draco compression to all .glb files in the specified directory.

    Args:
        cache_dir (str): Directory containing .glb files.
    """
    logging.info("Applying Draco compression to glTF files.")
    for root, dirs, files in os.walk(cache_dir):
        for file in files:
            if file.endswith('.glb'):
                gltf_file = os.path.join(root, file)

                # Check if the first line of the file contains "draco"
                try:
                    with open(gltf_file, 'rb') as f:
                        first_line = f.readline().decode('utf-8', errors='ignore')
                        if "draco" in first_line.lower():
                            logging.info(f"File {gltf_file} already contains Draco; skipping compression.")
                            continue
                except Exception as e:
                    logging.error(f"Error reading file {gltf_file}: {e}")
                    continue

                # Proceed with Draco compression
                compressed_file = os.path.join(root, f"{os.path.splitext(file)[0]}_draco.glb")
                cmd = [
                    "gltf-pipeline",
                    '-i', gltf_file,
                    '-o', compressed_file,
                    '--draco.compressionLevel', '7'
                ]
                print(" ".join(cmd))
                result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                if result.returncode != 0:
                    logging.error(f"Draco compression failed for {gltf_file}: {result.stderr}")
                else:
                    os.replace(compressed_file, gltf_file)
                    logging.info(f"Applied Draco compression to {gltf_file}")
                    
def append_temp_to_main(database_url, temp_table, main_table):
    """
    Appends data from the temporary table to the main table by copying all columns.
    If the main table does not have certain columns, they will be created.
    Handles duplicates by ignoring records that violate primary key constraints.

    Args:
        database_url (str): PostgreSQL connection URL.
        temp_table (str): Temporary table name.
        main_table (str): Main table name.
    """
    logging.info(f"Appending data from '{temp_table}' to '{main_table}'.")
    url = urlparse(database_url)
    conn = psycopg2.connect(
        dbname=url.path[1:],
        user=url.username,
        password=url.password,
        host=url.hostname,
        port=url.port
    )

    try:
        with conn.cursor() as cur:
            # Fetch main table columns
            cur.execute(f"""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_name = '{main_table}'
                ORDER BY ordinal_position;
            """)
            main_columns = [row[0] for row in cur.fetchall()]

            # Fetch temp table columns and their data types
            cur.execute(f"""
                SELECT column_name, data_type
                FROM information_schema.columns
                WHERE table_name = '{temp_table}'
                ORDER BY ordinal_position;
            """)
            temp_columns_info = cur.fetchall()
            temp_columns = [row[0] for row in temp_columns_info]

            # Add missing columns to main_table
            for col_name, data_type in temp_columns_info:
                if col_name not in main_columns:
                    logging.info(f"Column '{col_name}' does not exist in '{main_table}'. Adding it.")
                    # Add the column with the same data_type as in temp_table
                    # Note: For complex types or special columns, you may need a more robust mapping.
                    alter_sql = f'ALTER TABLE "{main_table}" ADD COLUMN "{col_name}" {data_type};'
                    cur.execute(alter_sql)
                    main_columns.append(col_name)
                    logging.info(f"Column '{col_name}' added to '{main_table}'.")

            # Now all temp_columns should exist in main_table
            # We will insert all columns from temp_table to main_table
            columns_str = ', '.join([f'"{col}"' for col in temp_columns])

            # Fetch primary key columns from the main table
            cur.execute(f"""
                SELECT a.attname
                FROM pg_index i
                JOIN pg_attribute a ON a.attrelid = i.indrelid
                                     AND a.attnum = ANY(i.indkey)
                WHERE i.indrelid = '{main_table}'::regclass
                  AND i.indisprimary;
            """)
            pk_columns = [row[0] for row in cur.fetchall()]
            if not pk_columns:
                raise ValueError(f"No primary key defined for table '{main_table}'.")

            # Construct the ON CONFLICT clause
            conflict_target = ', '.join([f'"{col}"' for col in pk_columns])
            on_conflict_clause = f"ON CONFLICT ({conflict_target}) DO NOTHING"

            logging.info(f"Using ON CONFLICT clause on columns: {conflict_target}")

            # Execute the INSERT statement with ON CONFLICT
            insert_sql = f"""
                INSERT INTO "{main_table}" ({columns_str})
                SELECT {columns_str} FROM "{temp_table}"
                {on_conflict_clause};
            """
            cur.execute(insert_sql)

            inserted_count = cur.rowcount
            conn.commit()
            logging.info(f"Data appended from '{temp_table}' to '{main_table}' successfully. Inserted {inserted_count} records.")

    except Exception as e:
        logging.error(f"Failed to append data from '{temp_table}' to '{main_table}': {e}")
        conn.rollback()
        raise
    finally:
        conn.close()



def drop_temp_table(database_url, temp_table):
    """
    Drops the temporary table from the database.

    Args:
        database_url (str): PostgreSQL connection URL.
        temp_table (str): Temporary table name.
    """
    logging.info(f"Dropping temporary table '{temp_table}'.")
    url = urlparse(database_url)
    conn = psycopg2.connect(
        dbname=url.path[1:],
        user=url.username,
        password=url.password,
        host=url.hostname,
        port=url.port
    )
    try:
        with conn.cursor() as cur:
            cur.execute(f"DROP TABLE IF EXISTS {temp_table};")
            conn.commit()
        logging.info(f"Temporary table '{temp_table}' dropped successfully.")
    except Exception as e:
        logging.error(f"Failed to drop temporary table '{temp_table}': {e}")
        conn.rollback()
        raise
    finally:
        conn.close()

def remove_file(file_path):
    """
    Removes a file from the filesystem.

    Args:
        file_path (str): Path to the file.
    """
    try:
        os.remove(file_path)
        logging.info(f"Removed file: {file_path}")
    except OSError as e:
        logging.warning(f"Failed to remove file {file_path}: {e}")

import os
import json
import math

def merge_tilesets_into_one(output_path, input_tilesets):
    """
    Merges multiple region-based tilesets into a single tileset.json that references all of them as external.
    If some tilesets do not exist or do not have a region boundingVolume, they are skipped.
    If no valid tilesets remain, creates a minimal tileset with no children.
    
    Args:
        output_path (str): Path to the final merged tileset.json output file.
        input_tilesets (list[str]): Paths to the input tileset.json files to merge.

    Returns:
        None. Writes the merged tileset.json to output_path.
    """
    
    # Load all valid tilesets
    loaded_tilesets = []
    for ts_path in input_tilesets:
        if not os.path.isfile(ts_path):
            # Skip if the file doesn't exist
            continue
        try:
            with open(ts_path, 'r', encoding='utf-8') as f:
                ts = json.load(f)
                loaded_tilesets.append((ts_path, ts))
        except (IOError, json.JSONDecodeError):
            # Skip if the file cannot be read or is not valid JSON
            continue
    
    # Filter down to only those with a region boundingVolume
    all_regions = []
    valid_tilesets = []
    for ts_path, ts in loaded_tilesets:
        root = ts.get("root", {})
        bv = root.get("boundingVolume", {})
        region = bv.get("region")
        
        if region and isinstance(region, list) and len(region) == 6:
            all_regions.append(region)
            valid_tilesets.append((ts_path, ts))
        # If there's no valid region, skip this tileset
    
    if not valid_tilesets:
        # No valid tilesets found, create an empty tileset
        # with a minimal boundingVolume and no children.
        # We'll use a generic region that covers no area.
        # For example, we can pick a degenerate region:
        degenerate_region = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        empty_tileset = {
            "asset": {
                "version": "1.1"
            },
            "geometricError": 0,
            "root": {
                "boundingVolume": {
                    "region": degenerate_region
                },
                "refine": "ADD",
                "geometricError": 0,
                "children": []
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(empty_tileset, f, indent=2)
        print(f"No valid tilesets found. Created an empty merged tileset at {output_path}")
        return
    
    # Compute the encompassing region for all valid tilesets
    west = min(r[0] for r in all_regions)
    south = min(r[1] for r in all_regions)
    east = max(r[2] for r in all_regions)
    north = max(r[3] for r in all_regions)
    minH = min(r[4] for r in all_regions)
    maxH = max(r[5] for r in all_regions)
    merged_region = [west, south, east, north, minH, maxH]

    # Construct children for the merged tileset
    children = []
    output_dir = os.path.dirname(os.path.abspath(output_path))
    for ts_path, ts in valid_tilesets:
        ts_abs = os.path.abspath(ts_path)
        rel_path = os.path.relpath(ts_abs, output_dir)
        
        child = {
            "boundingVolume": ts["root"]["boundingVolume"],
            "geometricError": ts["root"]["geometricError"],
            "refine": ts["root"].get("refine", "ADD").upper(),
            "content": {
                "uri": rel_path
            }
        }
        children.append(child)

    # Determine the maximum geometricError for the parent tileset
    parent_geometric_error = max(ts["root"]["geometricError"] for _, ts in valid_tilesets)
    
    # Create the merged tileset JSON structure
    merged_tileset = { 
        "asset": {
            "version": "1.1"
        },
        "geometricError": parent_geometric_error,
        "root": {
            "boundingVolume": {
                "region": merged_region
            },
            "refine": "ADD",
            "geometricError": parent_geometric_error,
            "children": children
        }
    }

    # Write the merged tileset to disk
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(merged_tileset, f, indent=2)
    print(f"Merged tileset written to {output_path}")

def ensure_main_table_exists(database_url, table_name):
    """
    Ensures that the main table exists in the database. Creates it if it does not exist.

    Args:
        database_url (str): PostgreSQL connection URL.
        table_name (str): Name of the main table.
    """
    logging.info(f"Ensuring main table '{table_name}' exists.")
    url = urlparse(database_url)
    conn = psycopg2.connect(
        dbname=url.path[1:],
        user=url.username,
        password=url.password,
        host=url.hostname,
        port=url.port
    )
    try:
        with conn.cursor() as cur:
            # Check if the table exists
            cur.execute(f"""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables
                    WHERE table_name = '{table_name}'
                );
            """)
            exists = cur.fetchone()[0]

            if not exists:
                logging.info(f"Table '{table_name}' does not exist. Creating it.")
                cur.execute(f"""
                    CREATE TABLE {table_name} (
                        gml_id VARCHAR PRIMARY KEY,
                        geom GEOMETRY(GEOMETRYZ, 4326),
                        attributes JSONB
                    );
                """)
                conn.commit()
                logging.info(f"Table '{table_name}' created successfully.")
            else:
                logging.info(f"Table '{table_name}' already exists.")
    except Exception as e:
        logging.error(f"Failed to ensure table '{table_name}' exists: {e}")
        conn.rollback()
        raise
    finally:
        conn.close()


def main(meta4_file):
    # Ensure DATA_DIR and CACHE_DIR exist
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(CACHE_DIR, exist_ok=True)
    
    # Ensure the main table exists
    ensure_main_table_exists(DATABASE_URL, MAIN_TABLE)
    # execute_sql_file(SQL_INDEX_PATH, DATABASE_URL)

    
    # Parse the Meta4 file
    files = parse_meta4(meta4_file)

    # Drop the temporary table
    drop_temp_table(DATABASE_URL, TEMP_TABLE)

    for ix, file_info in enumerate(files):
        
        if ix < 1700:
           continue
        
        file_name = file_info['name']
        size = file_info['size']
        hash_type = file_info['hash_type']
        hash_value = file_info['hash_value']
        urls = file_info['urls']

        logging.info(f"▶️   FILE {ix+1}/{len(files)}")
        logging.info(f"Processing file: {file_name}")


        temp_tileset_dir = os.path.join(CACHE_DIR, 'sub', str(ix // BATCH_N))  
        main_tileset_dir = CACHE_DIR
           
        os.makedirs(main_tileset_dir, exist_ok=True)   
        os.makedirs(temp_tileset_dir, exist_ok=True)

        # Determine download paths
        download_path = os.path.join(DATA_DIR, file_name)
        transformed_file_name = os.path.splitext(file_name)[0] + '_trs.gml'
        transformed_path = os.path.join(DATA_DIR, transformed_file_name)

        # Download the file from available URLs
        downloaded = False
        for url in urls:
            if download_file(url, download_path):
                # Verify the file
                if verify_file(download_path, size, hash_value, hash_type):
                    downloaded = True
                    break
                else:
                    logging.warning(f"Verification failed for {download_path}. Trying next URL.")
                    remove_file(download_path)
        if not downloaded:
            logging.error(f"Failed to download and verify {file_name} from all URLs. Skipping.")
            continue

        try:
            # Transform the GML file
            transform_gml(download_path, transformed_path)

            # Ingest the transformed GML into the temporary table
            ingest_gml_file(transformed_path, DATABASE_URL, TEMP_TABLE)

            # Update geometries in the temporary table
            update_geometries(DATABASE_URL, TEMP_TABLE)



            if ix % BATCH_N == 0 or ix == len(files) - 1:            
                # Convert the temporary table to 3D tiles
                convert_to_3d_tiles(temp_tileset_dir, DATABASE_URL, TEMP_TABLE)

                # Apply Draco compression to the newly generated tiles
                apply_draco_compression(temp_tileset_dir)

                # Append data from temporary table to main table
                append_temp_to_main(DATABASE_URL, TEMP_TABLE, MAIN_TABLE)

                # Drop the temporary table
                drop_temp_table(DATABASE_URL, TEMP_TABLE)

                batch_count = ix // BATCH_N + 1

                # Collect all input tileset.json paths
                input_tileset_paths = [
                    os.path.join(CACHE_DIR, 'sub', str(b), 'tileset.json')
                    for b in range(batch_count)
                ]

                merged_tileset_path = os.path.join(CACHE_DIR, 'tileset.json')

                logging.info(f"Merging {len(input_tileset_paths)} tilesets into {merged_tileset_path}...")

                try:
                    # Call our custom merging function
                    merge_tilesets_into_one(merged_tileset_path, input_tileset_paths)
                    logging.info("Merged tileset into main tileset successfully.")
                except Exception as e:
                    logging.error(f"Failed to combine merged tilesets: {e}")
                    raise RuntimeError(f"Failed to combine merged tilesets: {e}")

            # npx 3d-tiles-tools combine -i backend/tileset\ -o backend/tileset_combined -f

            if ix == 0:
                # Execute SQL indexing file once before processing
                execute_sql_file(SQL_INDEX_PATH, DATABASE_URL)
                
            # Remove the transformed GML file
            remove_file(transformed_path)
            # Remove the transformed GFS file if it exists
            transformed_gfs_path = os.path.splitext(transformed_path)[0] + ".gfs"
            if os.path.isfile(transformed_gfs_path):
                remove_file(transformed_gfs_path)

            # Optionally, remove the original downloaded GML file
            remove_file(download_path)

            logging.info(f"✅  Completed processing for {file_name}")

        except Exception as e:
            logging.error(f"An error occurred while processing {file_name}: {e}")
            # Optionally, clean up files or continue
            continue

    logging.info("🏁 All files processed.")

if __name__ == '__main__':
    meta4_file = META4_PATH
    if not os.path.isfile(meta4_file):
        logging.error(f"Meta4 file '{meta4_file}' does not exist.")
        sys.exit(1)
    main(meta4_file)


&&& FILE: ./backend\ingestion\README.md
&&& CONTENT:
# EasyOpenData Data Ingestion

## Overview

The `ingestion` directory contains scripts and resources for downloading, transforming, and ingesting 3D building data into a PostGIS database. It also incl
................................

&&& FILE: ./backend\ingestion\data_sources\bamberg.meta4
&&& CONTENT:
<?xml version="1.0" encoding="UTF-8"?>
<metalink xmlns="urn:ietf:params:xml:ns:metalink">
  <generator>BVV-MetaLinker</generator>
  <published>2024-11-26T22:13:07Z</published>
  <file name="636_5524.g
................................

&&& FILE: ./backend\ingestion\data_sources\bayern.meta4
&&& CONTENT:
<?xml version="1.0" encoding="UTF-8"?>
<metalink xmlns="urn:ietf:params:xml:ns:metalink">
  <generator>BVV-MetaLinker</generator>
  <published>2024-11-19T22:09:01Z</published>
  <file name="792_5432.g
................................

&&& FILE: ./backend\ingestion\data_sources\munchen.meta4
&&& CONTENT:
<?xml version="1.0" encoding="UTF-8"?>
<metalink xmlns="urn:ietf:params:xml:ns:metalink">
  <generator>BVV-MetaLinker</generator>
  <published>2024-11-19T22:08:58Z</published>
  <file name="680_5342.g
................................

&&& FILE: ./backend\ingestion\libs\pg2b3dm.exe
&&& ERROR: Could not read file: 'utf-8' codec can't decode byte 0x90 in position 2: invalid start byte

&&& FILE: ./frontend\.dockerignore
&&& CONTENT:
node_modules


&&& FILE: ./frontend\.gitignore
&&& CONTENT:
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*

node_modules
dist
dist-ssr
*.local
.vite/*

# Editor directories and files
.vscode/*
!.vscode/extensions.json
.idea
.DS_Store
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

yarn.lock

&&& FILE: ./frontend\Dockerfile
&&& CONTENT:
FROM node:18-alpine

WORKDIR /app

# Copy package.json and yarn.lock first for dependency installation
COPY package.json ./

# Install dependencies
RUN yarn install 

# Copy the rest of the frontend code
COPY . .

EXPOSE 5173

CMD ["yarn", "dev", "--host", "0.0.0.0"]

&&& FILE: ./frontend\Dockerfile.production
&&& CONTENT:
# Use a lightweight Node.js base image
FROM node:18-alpine AS builder

WORKDIR /app

# Copy package.json and yarn.lock first for dependency installation
COPY package.json yarn.lock ./

# Install dependencies
RUN yarn install --frozen-lockfile

# Copy the rest of the frontend code
COPY . .

# Build the production-ready static files
RUN yarn build

# Use a lightweight web server for serving the static files
FROM nginx:stable-alpine

# Copy the production build from the builder stage
COPY --from=builder /app/dist /usr/share/nginx/html

# Copy a custom nginx configuration to configure port 5173
COPY nginx.conf /etc/nginx/conf.d/default.conf

# Expose the custom port
EXPOSE 5173

# Command to run nginx
CMD ["nginx", "-g", "daemon off;"]


&&& FILE: ./frontend\eslint.config.js
&&& CONTENT:
import js from '@eslint/js'
import globals from 'globals'
import react from 'eslint-plugin-react'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'

export default [
  { ignores: ['dist'] },
  {
    files: ['**/*.{js,jsx}'],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
      parserOptions: {
        ecmaVersion: 'latest',
        ecmaFeatures: { jsx: true },
        sourceType: 'module',
      },
    },
    settings: { react: { version: '18.3' } },
    plugins: {
      react,
      'react-hooks': reactHooks,
      'react-refresh': reactRefresh,
    },
    rules: {
      ...js.configs.recommended.rules,
      ...react.configs.recommended.rules,
      ...react.configs['jsx-runtime'].rules,
      ...reactHooks.configs.recommended.rules,
      'react/jsx-no-target-blank': 'off',
      'react-refresh/only-export-components': [
        'warn',
        { allowConstantExport: true },
      ],
    },
  },
]


&&& FILE: ./frontend\index.html
&&& CONTENT:
<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover">

  <title>EasyOpenData</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
  <meta name="theme-color" content="#eeeeee" />

  <link href="https://api.mapbox.com/mapbox-gl-js/v2.6.1/mapbox-gl.css" rel="stylesheet">
  <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/plugins/mapbox-gl-draw/v1.3.0/mapbox-gl-draw.css" type="text/css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
</head>

<body>
  <div id="root"></div>
  <script type="module" src="./src/index.tsx"></script>
</body>

</html>


&&& FILE: ./frontend\nginx.conf
&&& CONTENT:
server {
    listen 5173; # Custom port

    server_name 0.0.0.0; # Ensure it listens on all network interfaces

    root /usr/share/nginx/html; # Serve the static files

    index index.html;

    location / {
        try_files $uri /index.html; # Enable SPA routing (if needed)
    }

    error_page 404 /index.html; # Handle 404 errors by serving the main app
}


&&& FILE: ./frontend\package.json
&&& CONTENT:
{
  "name": "frontend",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "lint": "eslint .",
    "preview": "vite preview"

................................

&&& FILE: ./frontend\README.md
&&& CONTENT:
# EasyOpenData Frontend

## Overview

The frontend of EasyOpenData is a React application that provides an interactive map interface for users to select areas of interest, choose data layers, and down
................................

&&& FILE: ./frontend\tsconfig.json
&&& CONTENT:
{
    "compilerOptions": {
      "target": "es2020",
      "jsx": "react",
      "moduleResolution": "node",
      "allowSyntheticDefaultImports": true
    }
  }
................................

&&& FILE: ./frontend\vite.config.js
&&& CONTENT:
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vite.dev/config/
export default defineConfig({
  server: {
    watch: {
      usePolling: true, // For Docker environments
    },
  },
  plugins: [react()],
})


&&& FILE: ./frontend\yarn.lock
&&& CONTENT:
# THIS IS AN AUTOGENERATED FILE. DO NOT EDIT THIS FILE DIRECTLY.
# yarn lockfile v1


"@ampproject/remapping@^2.2.0":
  version "2.3.0"
  resolved "https://registry.npmjs.org/@ampproject/remapping/-/r
................................

&&& FILE: ./frontend\public\basemap.json
&&& CONTENT:
{
  "version": 8,
  "name": "Positron",
  "metadata": {
    "mapbox:autocomposite": false,
    "mapbox:groups": {
      "101da9f13b64a08fa4b6ac1168e89e5f": {
        "collapsed": false,
        "name"
................................

&&& FILE: ./frontend\public\vite.svg
&&& CONTENT:
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="31.88" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 257"><defs><linearGradient id="IconifyId1813088fe1fbc01fb466" x1="-.828%" x2="57.636%" y1="7.652%" y2="78.411%"><stop offset="0%" stop-color="#41D1FF"></stop><stop offset="100%" stop-color="#BD34FE"></stop></linearGradient><linearGradient id="IconifyId1813088fe1fbc01fb467" x1="43.376%" x2="50.316%" y1="2.242%" y2="89.03%"><stop offset="0%" stop-color="#FFEA83"></stop><stop offset="8.333%" stop-color="#FFDD35"></stop><stop offset="100%" stop-color="#FFA800"></stop></linearGradient></defs><path fill="url(#IconifyId1813088fe1fbc01fb466)" d="M255.153 37.938L134.897 252.976c-2.483 4.44-8.862 4.466-11.382.048L.875 37.958c-2.746-4.814 1.371-10.646 6.827-9.67l120.385 21.517a6.537 6.537 0 0 0 2.322-.004l117.867-21.483c5.438-.991 9.574 4.796 6.877 9.62Z"></path><path fill="url(#IconifyId1813088fe1fbc01fb467)" d="M185.432.063L96.44 17.501a3.268 3.268 0 0 0-2.634 3.014l-5.474 92.456a3.268 3.268 0 0 0 3.997 3.378l24.777-5.718c2.318-.535 4.413 1.507 3.936 3.838l-7.361 36.047c-.495 2.426 1.782 4.5 4.151 3.78l15.304-4.649c2.372-.72 4.652 1.36 4.15 3.788l-11.698 56.621c-.732 3.542 3.979 5.473 5.943 2.437l1.313-2.028l72.516-144.72c1.215-2.423-.88-5.186-3.54-4.672l-25.505 4.922c-2.396.462-4.435-1.77-3.759-4.114l16.646-57.705c.677-2.35-1.37-4.583-3.769-4.113Z"></path></svg>

&&& FILE: ./frontend\src\App.tsx
&&& CONTENT:
// App.tsx
import React, { useCallback, useState, useRef, useEffect } from "react";
import { createRoot } from "react-dom/client";
import {
  Map,
  NavigationControl,
  Popup,
  useControl,
} from "react-map-gl/maplibre";
import { Tile3DLayer, MapViewState, AmbientLight, DirectionalLight, LightingEffect } from "deck.gl";
import { MapboxOverlay as DeckOverlay } from "@deck.gl/mapbox";
import "maplibre-gl/dist/maplibre-gl.css";
import type { Tileset3D } from "@loaders.gl/tiles";
import MapboxDraw from "@mapbox/mapbox-gl-draw";
import "@mapbox/mapbox-gl-draw/dist/mapbox-gl-draw.css";
import FloatingPanel from "./FloatingPanel";
import Logo from "./Logo";
import * as turf from "@turf/turf";
import "bootstrap/dist/css/bootstrap.min.css";

import DrawControl from './draw-control';
import LegalDocuments from "./Legals";

import './styles.css'
import './colors.css'

const BACKEND_URL = import.meta.env.VITE_BASE_URL;
const TILESET_URL = import.meta.env.VITE_TILESET_URL;


const INITIAL_VIEW_STATE: MapViewState = {
  latitude: 49.8917,
  longitude: 10.8863,
  pitch: 45,
  maxPitch: 60,
  bearing: 0,
  minZoom: 2,
  maxZoom: 30,
  zoom: 17,
};

const MAP_STYLE = "/basemap.json";

function DeckGLOverlay(props: any) {
  const overlay = useControl(() => new DeckOverlay(props));
  overlay.setProps(props);
  return null;
}

function Root() {
  const [selected, setSelected] = useState<any>(null);
  const [viewState, setViewState] = useState<MapViewState>(INITIAL_VIEW_STATE);
  const [features, setFeatures] = useState<Record<string, any>>({});
  const [isLod2Visible, setIsLod2Visible] = useState(true);
  const [polygonArea, setPolygonArea] = useState<number | null>(null);
  const mapRef = useRef<any>(null); // Reference to the map instance

  const drawRef = useRef<MapboxDraw | null>(null); // Reference to the MapboxDraw instance

  const rootStyles = getComputedStyle(document.documentElement);
  const POLYGON_COLOR = rootStyles.getPropertyValue('--bs-secondary').trim();
  const POLYGON_SELECTED_COLOR = rootStyles.getPropertyValue('--bs-secondary-selected').trim();

  // Initialize MapboxDraw and add it to the map
  const handleMapLoad = useCallback(() => {
    const map = mapRef.current.getMap();

    // Initialize MapboxDraw if not already initialized
    if (!drawRef.current) {
      drawRef.current = new MapboxDraw({
        displayControlsDefault: false,
        controls: {
          polygon: false,
          trash: false,
        },
        styles: [
          {
            id: 'gl-draw-polygon-fill',
            type: 'fill',
            paint: {
              'fill-color': POLYGON_COLOR,
              'fill-opacity': 0.5, // Adjust transparency if needed
            },
          },
          // Selected Polygon Fill
          {
            id: 'gl-draw-polygon-fill-active',
            type: 'fill',
            filter: ['all', ['==', '$type', 'Polygon'], ['==', 'active', 'true']],
            paint: {
              'fill-color': POLYGON_SELECTED_COLOR,
              'fill-opacity': 0.5,
            },
          },
          // Default Polygon Stroke
          {
            id: 'gl-draw-polygon-stroke',
            type: 'line',
            paint: {
              'line-color': POLYGON_SELECTED_COLOR,
              'line-width': 4,
            },
          },
          // Selected Polygon Stroke
          {
            id: 'gl-draw-polygon-stroke-active',
            type: 'line',
            filter: ['all', ['==', '$type', 'Polygon'], ['==', 'active', 'true']],
            paint: {
              'line-color': POLYGON_SELECTED_COLOR,
              'line-width': 6,
            },
          },
          {
            id: 'gl-draw-point',
            type: 'circle',
            filter: ['all', ['!=', 'meta', 'midpoint']],
            paint: {
              'circle-radius': 12,
              'circle-color': POLYGON_SELECTED_COLOR,
            },
          },
          {
            id: 'gl-draw-point-active',
            type: 'circle',
            filter: ['all', ['!=', 'meta', 'midpoint'], ['==', 'active', 'true']],
            paint: {
              'circle-radius': 8,
              'circle-color': POLYGON_COLOR,
            },
          },
          // Midpoints
          {
            id: 'gl-draw-midpoint',
            type: 'circle',
            filter: ['all', ['==', 'meta', 'midpoint']],
            paint: {
              'circle-radius': 8,
              'circle-color': POLYGON_SELECTED_COLOR,
            },
          },
        ]
      });
      map.addControl(drawRef.current);
    }

  // Bind event listeners for onUpdate and onDelete
  map.on('draw.create', onUpdate); // Bind onUpdate callback
  map.on('draw.update', onUpdate); // Bind onUpdate callback
  map.on('draw.delete', onDelete); // Bind onDelete callback

}, []);
  // const onTilesetLoad = (tileset: Tileset3D) => {
  //   const { cartographicCenter, zoom } = tileset;
  //   setViewState((prev) => ({
  //     ...prev,
  //     longitude: cartographicCenter[0],
  //     latitude: cartographicCenter[1],
  //     zoom,
  //   }));
  // };


  const onUpdate = useCallback((e) => {
    setFeatures((currFeatures) => {
      const newFeatures = { ...currFeatures };
      for (const f of e.features) {
        newFeatures[f.id] = f;
      }
      return newFeatures;
    });

    // Calculate polygon area
    if (e.features && e.features.length > 0) {
      const polygon = e.features[0];
      const area = turf.area(polygon) / 1e6; // Convert from m² to km²
      setPolygonArea(area);
    }
  }, []);

  const onDelete = useCallback((e) => {
    setFeatures((currFeatures) => {
      const newFeatures = { ...currFeatures };
      for (const f of e.features) {
        delete newFeatures[f.id];
      }
      return newFeatures;
    });
    setPolygonArea(null);
  }, []);

  const handleDrawPolygon = () => {
    if (drawRef.current) {
      drawRef.current.deleteAll();
      drawRef.current.changeMode("draw_polygon");
    }
  };

  const handleRemovePolygon = () => {
    if (drawRef.current) {
      drawRef.current.deleteAll();
    }
  };

  const handleFetchObjFile = async () => {
    console.info("getFetchObjFile")
    if (drawRef.current) {
      const data = drawRef.current.getAll();
      if (data.features.length > 0) {
        try {
          const response = await fetch(BACKEND_URL + "/retrieve_obj", {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
            },
            body: JSON.stringify({ region: data }),
          });

          if (response.ok) {
            const blob = await response.blob();
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement("a");
            a.style.display = "none";
            a.href = url;
            // Use the filename from the Content-Disposition header if available
            const contentDisposition = response.headers.get("Content-Disposition");
            const filenameMatch =
              contentDisposition && contentDisposition.match(/filename="?(.+)"?/i);
            a.download = filenameMatch
              ? filenameMatch[1]
              : `object_file.obj`;
            document.body.appendChild(a);
            a.click();
            window.URL.revokeObjectURL(url);
          } else {
            console.error("Failed to fetch obj file");
          }
        } catch (error) {
          console.error("Error fetching obj file:", error);
        }
      } else {
        console.error("No polygon drawn");
      }
    }
  };

  const handleZoomChange = (event: any) => {
    const newZoom = event.viewState.zoom;
    setViewState(event.viewState);

    // Toggle visibility based on zoom level
    if (newZoom < 1) {
      setIsLod2Visible(false);
    } else {
      setIsLod2Visible(true);
    }
  };

  // Create ambient light
  const ambientLight = new AmbientLight({
    color: [240, 255, 255],
    intensity: 1.0
  });

  // Create directional light
  const directionalLight1 = new DirectionalLight({
    color: [220, 255, 255],
    intensity: 0.6,
    direction: [-1, -3, -1]
  });

  // Create directional light
  const directionalLight2 = new DirectionalLight({
    color: [255, 220, 255],
    intensity: 1,
    direction: [1, -3, 1]
  });

  // Create lighting effect
  const lightingEffect = new LightingEffect({ ambientLight, directionalLight1, directionalLight2 });


  const layers = [
    new Tile3DLayer({
      id: "tile-3d-layer",
      data: TILESET_URL,
      // pickable: true,
      // autoHighlight: false,
      // onClick: (info, event) => console.log("Clicked:", info, event),
      // getPickingInfo: (pickParams) => console.log("PickInfo", pickParams),
      // onTilesetLoad,
      visible: isLod2Visible,
      // For ScenegraphLayer (b3dm or i3dm format)
      //_lighting: 'pbr',
      //effects: [lightingEffect],
      loadOptions: {
        tileset: {
          maxRequests: 16,
          updateTransforms: false,
          maximumMemoryUsage: 512
          //maximumScreenSpaceError: 16, // Adjust this value as needed
          //viewDistanceScale: 1.5 // Adjust this value as needed
        }
      },
      // Additional sublayer props for fine-grained control
      _subLayerProps: {
        scenegraph: {
          getColor: (d) => [255, 255, 255, 150], // Blue color for scenegraph models (alternative method)
          //effects: [lightingEffect]
        }
      }
    }),
  ];

  const handleSearch = async (query: string) => {
    const response = await fetch(
      `https://nominatim.openstreetmap.org/search?format=json&q=${encodeURIComponent(query)}&countrycodes=de`
    );
    const data = await response.json();
    return data;
  };

  const handleSelectResult = (result: any) => {
    // Fly to the selected location
    const map = mapRef.current.getMap();

    map.flyTo({
      center: [parseFloat(result.lon), parseFloat(result.lat)],
      zoom: 14
    });
  };

  return (
    <div style={{ position: "fixed", width: "100%", height: "100%" }}>
      <Map
        initialViewState={viewState}
        mapStyle={MAP_STYLE}
        onLoad={handleMapLoad} // Ensure map is passed here
        onMove={handleZoomChange}
        ref={mapRef}
        style={{ width: "100%", height: "100%" }}
        hash={true}
      >
        {selected && (
          <Popup
            key={selected.properties.name}
            anchor="bottom"
            style={{ zIndex: 10 }}
            longitude={selected.geometry.coordinates[0]}
            latitude={selected.geometry.coordinates[1]}
          >
            {selected.properties.name} ({selected.properties.abbrev})
          </Popup>
        )}
        <DeckGLOverlay layers={layers}   //effects={[lightingEffect]} // Apply the custom lighting effect globally
        />
        {/* <DrawControl
          ref={drawRef}
          onCreate={onUpdate}
          onUpdate={onUpdate}
          onDelete={onDelete}
        /> */}
        {/* <div
          style={{
            position: 'absolute',
            bottom: '240px', // Adjust as needed to ensure a 20px gap above Auswahl panel
            left: '20px',
            zIndex: 2000,
            pointerEvents: 'none',
          }}
        >
          <div style={{ pointerEvents: 'auto' }}>
            <NavigationControl />
          </div>
        </div> */}
        {/* <DrawControl
          position="top-right"
          displayControlsDefault={false}
          controls={{
            polygon: false,
            trash: false
          }}
          defaultMode="draw_polygon"
          onCreate={onUpdate}
          onUpdate={onUpdate}
          onDelete={onDelete}
        /> */}
      </Map>
      <FloatingPanel
        onDrawPolygon={handleDrawPolygon}
        onRemovePolygon={handleRemovePolygon}
        onFetchObjFile={handleFetchObjFile}
        polygonArea={polygonArea}
        onSearch={handleSearch}
        onSelectResult={handleSelectResult}
      />

      <div className="top-bar-container">
        <div className="top-bar-section legals">
          <LegalDocuments />
        </div>
        <div className="top-bar-section logo">
          <Logo />
        </div>
      </div>

    </div>
  );
}

interface DrawControlProps {
  onCreate: (e: any) => void;
  onUpdate: (e: any) => void;
  onDelete: (e: any) => void;
}

export default Root;


&&& FILE: ./frontend\src\CheckoutForm.tsx
&&& CONTENT:
import React, { useState } from "react";
import { useStripe, useElements, CardElement } from "@stripe/react-stripe-js";

interface CheckoutFormProps {
  price: number;
  onFetchObjFile: () => void;
}

interface CustomerData {
  email: string;
  name: string;
  address: {
    line1: string;
    postal_code: string;
    city: string;
    country: string;
  };
}
const CheckoutForm: React.FC<CheckoutFormProps> = ({ price, onFetchObjFile }) => {
  const stripe = useStripe();
  const elements = useElements();
  const [loading, setLoading] = useState(false);
  const [isExpanded, setIsExpanded] = useState(false);
  const [customerData, setCustomerData] = useState<CustomerData>({
    email: "",
    name: "",
    address: {
      line1: "",
      postal_code: "",
      city: "",
      country: "DE",
    },
  });

  // If price is 0, render only the download button
  if (price === 0) {
    return (
      <>
      <div className="text-center mb-3">
      Grundstücke unter 0.01 km² können kostenfrei heruntergeladen werden
      </div>
      <button 
        onClick={onFetchObjFile}
        className="btn btn-secondary btn mt-2"
      >
        .obj Herunterladen
      </button>
      </>
    );
  }

  const handleFocus = () => {
    setIsExpanded(true);
  };
  const handleFocusOut = () => {
    setIsExpanded(false);
  };

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!stripe || !elements) return;

    if (!customerData.email || !customerData.name || !customerData.address.line1 || 
        !customerData.address.postal_code || !customerData.address.city) {
      document.getElementById("payment-message")!.textContent = "Please fill in all required fields.";
      return;
    }

    setLoading(true);

    try {
      const response = await fetch(import.meta.env.VITE_BACKEND_URL + "/create-payment-intent", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ 
          amount: Math.round(price * 100),
          customer: customerData
        }),
      });

      if (!response.ok) throw new Error("Failed to create PaymentIntent.");

      const { clientSecret } = await response.json();

      const result = await stripe.confirmCardPayment(clientSecret, {
        payment_method: {
          billing_details: {
            name: customerData.name,
            email: customerData.email,
            address: {
              line1: customerData.address.line1,
              postal_code: customerData.address.postal_code,
              city: customerData.address.city,
              country: customerData.address.country,
            },
          },
          card: elements.getElement(CardElement)!,
        },
      });

      if (result.error) {
        document.getElementById("payment-message")!.textContent = result.error.message;
      } else if (result.paymentIntent?.status === "succeeded") {
        document.getElementById("payment-message")!.textContent = "Success! Your download will start soon.";
        onFetchObjFile();
      }
    } catch (error) {
      console.error("Payment error:", error);
    } finally {
      setLoading(false);
    }
  };
  return (
    <form onSubmit={handleSubmit} className="d-flex flex-column gap-2">
      
      {/* Secure Payment Badge */}
      <div className="d-flex align-items-center gap-1 text-secondary small">
        <span className="bi bi-lock-fill"></span>
        Secure payment via Stripe
      </div>
  
      {/* Price Details */}
      <div className="text-secondary small">
        <p>
          <strong>Order Total:</strong> €{price.toFixed(2)}
        </p>
        <p>No additional fees. You’ll only be charged this amount.</p>
      </div>
  
      {isExpanded && (
        <div className="mt-2 animate__animated animate__fadeIn">
          {/* Customer Details */}
          <input
            type="email"
            placeholder="Email *"
            required
            value={customerData.email}
            onChange={(e) =>
              setCustomerData({ ...customerData, email: e.target.value })
            }
            className="form-control form-control-sm mb-2"
          />
          <input
            type="text"
            placeholder="Full Name *"
            required
            value={customerData.name}
            onChange={(e) =>
              setCustomerData({ ...customerData, name: e.target.value })
            }
            className="form-control form-control-sm mb-2"
          />
          <input
            type="text"
            placeholder="Street Address *"
            required
            value={customerData.address.line1}
            onChange={(e) =>
              setCustomerData({
                ...customerData,
                address: { ...customerData.address, line1: e.target.value },
              })
            }
            className="form-control form-control-sm mb-2"
          />
          <div className="d-flex gap-2 mb-2">
            <input
              type="text"
              placeholder="Postal Code *"
              required
              value={customerData.address.postal_code}
              onChange={(e) =>
                setCustomerData({
                  ...customerData,
                  address: {
                    ...customerData.address,
                    postal_code: e.target.value,
                  },
                })
              }
              className="form-control form-control-sm"
            />
            <input
              type="text"
              placeholder="City *"
              required
              value={customerData.address.city}
              onChange={(e) =>
                setCustomerData({
                  ...customerData,
                  address: { ...customerData.address, city: e.target.value },
                })
              }
              className="form-control form-control-sm"
            />
          </div>
        </div>
      )}
  
      {/* Card Element */}
      <CardElement
      
      onFocus={handleFocus}
      onFocusOut={handleFocusOut}
        options={{
          style: {
            base: {
              fontSize: "14px",
            },
          },
        }}
      />
      <div id="payment-message" className="text-danger small"></div>
  
      {/* Link to Stripe Security Info */}
      <div className="small mt-7">
        <a
          href="https://stripe.com/docs/security"
          target="_blank"
          rel="noopener noreferrer"
          className="text-secondary"
        >
          Learn more about how your payment information is secured.
        </a>
      </div>
  
      <button
        type="submit"
        disabled={!stripe || loading}
        className="btn btn-secondary btn-sm mt-2"
      >
        {loading ? "Processing..." : `Pay €${price.toFixed(2)}`}
      </button>
    </form>
  );
  
};
export default CheckoutForm;

&&& FILE: ./frontend\src\colors.css
&&& CONTENT:
:root {
    --bs-primary: #2a7a92; /* Your desired primary color */
    --bs-secondary: #3e66bb;
    --bs-secondary-selected: #244ca3;
    --bs-success: #28a745;
    --bs-danger: #f38093;
    --bs-warning: #ffc107;
    --bs-info: #17a2b8;
    --bs-light: #f8f9fa;
    --bs-dark: #343a40;
  }
  
/*   
  .btn-primary {
    background-color: var(--bs-primary) !important;
    border-color: var(--bs-primary) !important;
  }
  .btn-secondary {
    background-color: var(--bs-secondary) !important;
    border-color: var(--bs-secondary) !important;
  }
  
  .btn-danger {
    background-color: var(--bs-danger) !important;
    border-color: var(--bs-danger) !important;
  }
   */

   
/*------------------------------------
- COLOR primary
------------------------------------*/
.alert-primary {
  color: #0e2a32;
  background-color: #a3d5e4;
  border-color: #93cee0;
}

.alert-primary hr {
  border-top-color: #7fc5da;
}

.alert-primary .alert-link {
  color: #03090b;
}

.badge-primary {
  color: #fff;
  background-color: #2a7a92;
}

.badge-primary[href]:hover, .badge-primary[href]:focus {
  color: #fff;
  background-color: #1e586a;
}

.bg-primary {
  background-color: #2a7a92 !important;
}

a.bg-primary:hover, a.bg-primary:focus,
button.bg-primary:hover,
button.bg-primary:focus {
  background-color: #1e586a !important;
}

.border-primary {
  border-color: #2a7a92 !important;
}

.btn-primary {
  color: #fff;
  background-color: #2a7a92;
  border-color: #2a7a92;
}

.btn-primary:hover {
  color: #fff;
  background-color: #226276;
  border-color: #1e586a;
}

.btn-primary:focus, .btn-primary.focus {
  box-shadow: 0 0 0 0.2rem rgba(42, 122, 146, 0.5);
}

.btn-primary.disabled, .btn-primary:disabled {
  color: #fff;
  background-color: #2a7a92;
  border-color: #2a7a92;
}

.btn-primary:not(:disabled):not(.disabled):active, .btn-primary:not(:disabled):not(.disabled).active, .show > .btn-primary.dropdown-toggle {
  color: #fff;
  background-color: #1e586a;
  border-color: #1b4e5e;
}

.btn-primary:not(:disabled):not(.disabled):active:focus, .btn-primary:not(:disabled):not(.disabled).active:focus, .show > .btn-primary.dropdown-toggle:focus {
  box-shadow: 0 0 0 0.2rem rgba(42, 122, 146, 0.5);
}

.btn-outline-primary {
  color: #2a7a92;
  background-color: transparent;
  border-color: #2a7a92;
}

.btn-outline-primary:hover {
  color: #fff;
  background-color: #2a7a92;
  border-color: #2a7a92;
}

.btn-outline-primary:focus, .btn-outline-primary.focus {
  box-shadow: 0 0 0 0.2rem rgba(42, 122, 146, 0.5);
}

.btn-outline-primary.disabled, .btn-outline-primary:disabled {
  color: #2a7a92;
  background-color: transparent;
}

.btn-outline-primary:not(:disabled):not(.disabled):active, .btn-outline-primary:not(:disabled):not(.disabled).active, .show > .btn-outline-primary.dropdown-toggle {
  color: #fff;
  background-color: #2a7a92;
  border-color: #2a7a92;
}

.btn-outline-primary:not(:disabled):not(.disabled):active:focus, .btn-outline-primary:not(:disabled):not(.disabled).active:focus, .show > .btn-outline-primary.dropdown-toggle:focus {
  box-shadow: 0 0 0 0.2rem rgba(42, 122, 146, 0.5);
}

.list-group-item-primary {
  color: #0e2a32;
  background-color: #93cee0;
}

.list-group-item-primary.list-group-item-action:hover, .list-group-item-primary.list-group-item-action:focus {
  color: #0e2a32;
  background-color: #7fc5da;
}

.list-group-item-primary.list-group-item-action.active {
  color: #fff;
  background-color: #0e2a32;
  border-color: #0e2a32;
}

.table-primary,
.table-primary > th,
.table-primary > td {
  background-color: #93cee0;
}

.table-hover .table-primary:hover {
  background-color: #7fc5da;
}

.table-hover .table-primary:hover > td,
.table-hover .table-primary:hover > th {
  background-color: #7fc5da;
}

.text-primary {
  color: #2a7a92 !important;
}

a.text-primary:hover, a.text-primary:focus {
  color: #1e586a !important;
}

   
/*------------------------------------
- COLOR secondary
------------------------------------*/
.alert-secondary {
  color: #1f335f;
  background-color: #d4ddf0;
  border-color: #c4d1eb;
}

.alert-secondary hr {
  border-top-color: #b1c2e5;
}

.alert-secondary .alert-link {
  color: #121e38;
}

.badge-secondary {
  color: #fff;
  background-color: #3e66bb;
}

.badge-secondary[href]:hover, .badge-secondary[href]:focus {
  color: #fff;
  background-color: #315194;
}

.bg-secondary {
  background-color: #3e66bb !important;
}

a.bg-secondary:hover, a.bg-secondary:focus,
button.bg-secondary:hover,
button.bg-secondary:focus {
  background-color: #315194 !important;
}

.border-secondary {
  border-color: #3e66bb !important;
}

.btn-secondary {
  color: #fff;
  background-color: #3e66bb;
  border-color: #3e66bb;
}

.btn-secondary:hover {
  color: #fff;
  background-color: #3557a0;
  border-color: #315194;
}

.btn-secondary:focus, .btn-secondary.focus {
  box-shadow: 0 0 0 0.2rem rgba(62, 102, 187, 0.5);
}

.btn-secondary.disabled, .btn-secondary:disabled {
  color: #fff;
  background-color: #3e66bb;
  border-color: #3e66bb;
}

.btn-secondary:not(:disabled):not(.disabled):active, .btn-secondary:not(:disabled):not(.disabled).active, .show > .btn-secondary.dropdown-toggle {
  color: #fff;
  background-color: #315194;
  border-color: #2d4a89;
}

.btn-secondary:not(:disabled):not(.disabled):active:focus, .btn-secondary:not(:disabled):not(.disabled).active:focus, .show > .btn-secondary.dropdown-toggle:focus {
  box-shadow: 0 0 0 0.2rem rgba(62, 102, 187, 0.5);
}

.btn-outline-secondary {
  color: #3e66bb;
  background-color: transparent;
  border-color: #3e66bb;
}

.btn-outline-secondary:hover {
  color: #fff;
  background-color: #3e66bb;
  border-color: #3e66bb;
}

.btn-outline-secondary:focus, .btn-outline-secondary.focus {
  box-shadow: 0 0 0 0.2rem rgba(62, 102, 187, 0.5);
}

.btn-outline-secondary.disabled, .btn-outline-secondary:disabled {
  color: #3e66bb;
  background-color: transparent;
}

.btn-outline-secondary:not(:disabled):not(.disabled):active, .btn-outline-secondary:not(:disabled):not(.disabled).active, .show > .btn-outline-secondary.dropdown-toggle {
  color: #fff;
  background-color: #3e66bb;
  border-color: #3e66bb;
}

.btn-outline-secondary:not(:disabled):not(.disabled):active:focus, .btn-outline-secondary:not(:disabled):not(.disabled).active:focus, .show > .btn-outline-secondary.dropdown-toggle:focus {
  box-shadow: 0 0 0 0.2rem rgba(62, 102, 187, 0.5);
}

.list-group-item-secondary {
  color: #1f335f;
  background-color: #c4d1eb;
}

.list-group-item-secondary.list-group-item-action:hover, .list-group-item-secondary.list-group-item-action:focus {
  color: #1f335f;
  background-color: #b1c2e5;
}

.list-group-item-secondary.list-group-item-action.active {
  color: #fff;
  background-color: #1f335f;
  border-color: #1f335f;
}

.table-secondary,
.table-secondary > th,
.table-secondary > td {
  background-color: #c4d1eb;
}

.table-hover .table-secondary:hover {
  background-color: #b1c2e5;
}

.table-hover .table-secondary:hover > td,
.table-hover .table-secondary:hover > th {
  background-color: #b1c2e5;
}

.text-secondary {
  color: #3e66bb !important;
}

a.text-secondary:hover, a.text-secondary:focus {
  color: #315194 !important;
}

/*------------------------------------
- COLOR danger
------------------------------------*/
.alert-danger {
  color: #d31b39;
  background-color: #10512f128;
  border-color: #10211d119;
}

.alert-danger hr {
  border-top-color: #100107105;
}

.alert-danger .alert-link {
  color: #a6152d;
}

.badge-danger {
  color: #212529;
  background-color: #ee7b8e;
}

.badge-danger[href]:hover, .badge-danger[href]:focus {
  color: #212529;
  background-color: #e84d67;
}

.bg-danger {
  background-color: #ee7b8e !important;
}

a.bg-danger:hover, a.bg-danger:focus,
button.bg-danger:hover,
button.bg-danger:focus {
  background-color: #e84d67 !important;
}

.border-danger {
  border-color: #ee7b8e !important;
}

.btn-danger {
  color: #212529;
  background-color: #ee7b8e;
  border-color: #ee7b8e;
}

.btn-danger:hover {
  color: #212529;
  background-color: #e95b72;
  border-color: #e84d67;
}

.btn-danger:focus, .btn-danger.focus {
  box-shadow: 0 0 0 0.2rem rgba(238, 123, 142, 0.5);
}

.btn-danger.disabled, .btn-danger:disabled {
  color: #212529;
  background-color: #ee7b8e;
  border-color: #ee7b8e;
}

.btn-danger:not(:disabled):not(.disabled):active, .btn-danger:not(:disabled):not(.disabled).active, .show > .btn-danger.dropdown-toggle {
  color: #212529;
  background-color: #e84d67;
  border-color: #e6405b;
}

.btn-danger:not(:disabled):not(.disabled):active:focus, .btn-danger:not(:disabled):not(.disabled).active:focus, .show > .btn-danger.dropdown-toggle:focus {
  box-shadow: 0 0 0 0.2rem rgba(238, 123, 142, 0.5);
}

.btn-outline-danger {
  color: #ee7b8e;
  background-color: transparent;
  border-color: #ee7b8e;
}

.btn-outline-danger:hover {
  color: #212529;
  background-color: #ee7b8e;
  border-color: #ee7b8e;
}

.btn-outline-danger:focus, .btn-outline-danger.focus {
  box-shadow: 0 0 0 0.2rem rgba(238, 123, 142, 0.5);
}

.btn-outline-danger.disabled, .btn-outline-danger:disabled {
  color: #ee7b8e;
  background-color: transparent;
}

.btn-outline-danger:not(:disabled):not(.disabled):active, .btn-outline-danger:not(:disabled):not(.disabled).active, .show > .btn-outline-danger.dropdown-toggle {
  color: #212529;
  background-color: #ee7b8e;
  border-color: #ee7b8e;
}

.btn-outline-danger:not(:disabled):not(.disabled):active:focus, .btn-outline-danger:not(:disabled):not(.disabled).active:focus, .show > .btn-outline-danger.dropdown-toggle:focus {
  box-shadow: 0 0 0 0.2rem rgba(238, 123, 142, 0.5);
}

.list-group-item-danger {
  color: #d31b39;
  background-color: #10211d119;
}

.list-group-item-danger.list-group-item-action:hover, .list-group-item-danger.list-group-item-action:focus {
  color: #d31b39;
  background-color: #100107105;
}

.list-group-item-danger.list-group-item-action.active {
  color: #212529;
  background-color: #d31b39;
  border-color: #d31b39;
}

.table-danger,
.table-danger > th,
.table-danger > td {
  background-color: #10211d119;
}

.table-hover .table-danger:hover {
  background-color: #100107105;
}

.table-hover .table-danger:hover > td,
.table-hover .table-danger:hover > th {
  background-color: #100107105;
}

.text-danger {
  color: #ee7b8e !important;
}

a.text-danger:hover, a.text-danger:focus {
  color: #e84d67 !important;
}


&&& FILE: ./frontend\src\draw-control.ts
&&& CONTENT:
import MapboxDraw from '@mapbox/mapbox-gl-draw';
import {useControl} from 'react-map-gl';

import type {MapRef, ControlPosition} from 'react-map-gl';

type DrawControlProps = ConstructorParameters<typeof MapboxDraw>[0] & {
  position?: ControlPosition;

  onCreate?: (evt: {features: object[]}) => void;
  onUpdate?: (evt: {features: object[]; action: string}) => void;
  onDelete?: (evt: {features: object[]}) => void;
};

export default function DrawControl(props: DrawControlProps) {
  useControl<MapboxDraw>(
    () => new MapboxDraw(props),
    ({map}: {map: MapRef}) => {
      map.on('draw.create', props.onCreate);
      map.on('draw.update', props.onUpdate);
      map.on('draw.delete', props.onDelete);
    },
    {
      position: props.position
    }
  );

  return null;
}

DrawControl.defaultProps = {
  onCreate: () => {},
  onUpdate: () => {},
  onDelete: () => {}
};

&&& FILE: ./frontend\src\FloatingPanel.tsx
&&& CONTENT:
import React, { useState, useEffect } from "react";
import { loadStripe } from "@stripe/stripe-js";
import { Elements } from "@stripe/react-stripe-js";
import CheckoutForm from "./CheckoutForm";
import "bootstrap/dist/css/bootstrap.min.css";
// Import FontAwesome
import { FontAwesomeIcon } from '@fortawesome/react-fontawesome';
import { faChevronUp, faChevronDown } from '@fortawesome/free-solid-svg-icons';

// Initialize Stripe with your publishable key
const stripePromise = loadStripe(
  "hidden_api"
);

interface FloatingPanelProps {
  onDrawPolygon: () => void;
  onRemovePolygon: () => void;
  onFetchObjFile: () => void;
  polygonArea: number | null; // in square kilometers
  onSearch: (query: string) => Promise<any>;
  onSelectResult: (result: any) => void;
}

const FloatingPanel: React.FC<FloatingPanelProps> = ({
  onDrawPolygon,
  onRemovePolygon,
  onFetchObjFile,
  polygonArea,
  onSearch,
  onSelectResult,
}) => {
  const [price, setPrice] = useState<number>(0);

  const [searchQuery, setSearchQuery] = useState("");
  const [searchResults, setSearchResults] = useState<any[]>([]);
  const [isSearching, setIsSearching] = useState(false);

  // New state for expanded tab
  const [expandedTab, setExpandedTab] = useState<string | null>(null);

  const [isMobile, setIsMobile] = useState<boolean>(
    window.innerWidth <= 768
  );

  useEffect(() => {
    if (polygonArea !== null) {
      if (polygonArea < 0.01) {
        setPrice(0);
      } else {
        setPrice(50 * polygonArea);
      }
    }
  }, [polygonArea]);

  // Handle window resize to update isMobile state
  useEffect(() => {
    const handleResize = () => {
      setIsMobile(window.innerWidth <= 768);
    };

    window.addEventListener("resize", handleResize);
    return () => window.removeEventListener("resize", handleResize);
  }, []);

  // Handle search functionality
  const handleSearch = async (e: React.ChangeEvent<HTMLInputElement>) => {
    const query = e.target.value;
    setSearchQuery(query);

    if (query.length > 2) {
      setIsSearching(true);
      try {
        const results = await onSearch(query);
        setSearchResults(results);
      } catch (error) {
        console.error("Search error:", error);
        setSearchResults([]);
      }
      setIsSearching(false);
    } else {
      setSearchResults([]);
    }
  };

  // Extracted content functions for reuse
  const renderAuswahlContent = () => (
    <>
      {/* Search Input */}
      <div className="w-100 mb-3 position-relative">
        <input
          type="text"
          className="form-control form-control-sm"
          placeholder="Standort finden..."
          value={searchQuery}
          onChange={handleSearch}
        />

        {/* Search Results */}
        {searchResults.length > 0 && (
          <div
            className="position-absolute bg-white shadow-sm rounded mt-1 w-100 overflow-auto"
            style={{
              maxHeight: "150px",
              zIndex: 1060,
            }}
          >
            {searchResults.map((result, index) => (
              <div
                key={index}
                className="p-2 hover-bg-light cursor-pointer"
                onClick={() => {
                  onSelectResult(result);
                  setSearchResults([]);
                  setSearchQuery("");
                }}
                style={{ cursor: "pointer" }}
              >
                {result.display_name || result.name}
              </div>
            ))}
          </div>
        )}

        {isSearching && (
          <div className="text-center mt-2">
            <small>Suche...</small>
          </div>
        )}
      </div>

      {/* Polygon Area and Price Display */}
      {polygonArea !== null ? (
        <div className="text-center mb-3">
          <strong>Gebietfläche:</strong> {polygonArea.toFixed(3)} km²
        </div>
      ) : (
        <p className="text-center mb-3">Zeichnen Sie das Gebiet ein</p>
      )}

      {/* Action Buttons */}
      <div className="btn-group w-100" role="group">
        <button
          type="button"
          className="btn btn-secondary btn mt-2"
          onClick={onDrawPolygon}
        >
          Polygon zeichnen
        </button>
        <button
          type="button"
          className="btn btn-danger btn mt-2"
          onClick={onRemovePolygon}
        >
          Entfernen
        </button>
      </div>
    </>
  );

  const renderHerunterladenContent = () => (
    <>
      <Elements stripe={stripePromise}>
        <CheckoutForm price={price} onFetchObjFile={onFetchObjFile} />
      </Elements>
    </>
  );

  // Mobile Layout
  if (isMobile) {
    return (
      <div
        className="position-fixed"
        style={{
          bottom: "30px",
          left: "0",
          right: "0",
          zIndex: 1050,
          pointerEvents: "none",
        }}
      >
        {/* Auswahl Tab */}
        <div
          style={{
            pointerEvents: "auto",
            marginBottom: expandedTab === 'Auswahl' ? '0' : '10px',
          }}
        >
          <div
            className={`bg-white shadow p-3 d-flex justify-content-between align-items-center ${
              expandedTab === 'Auswahl' ? 'rounded-top' : 'rounded'
            }`}
            style={{
              margin: "0 20px",
              cursor: "pointer",
            }}
            onClick={() => {
              setExpandedTab(expandedTab === 'Auswahl' ? null : 'Auswahl');
            }}
          >
            <h5 className="mb-0">Auswahl</h5>
            {/* Expand/Collapse Icon */}
            <FontAwesomeIcon
              icon={expandedTab === 'Auswahl' ? faChevronUp : faChevronDown}
            />
          </div>
          {expandedTab === 'Auswahl' && (
            <div
              className="bg-white shadow p-3 rounded-bottom"
              style={{
                margin: "0 20px 20px",
              }}
            >
              {renderAuswahlContent()}
            </div>
          )}
        </div>

        {/* Herunterladen Tab */}
        <div
          style={{
            pointerEvents: "auto",
            marginBottom: expandedTab === 'Herunterladen' ? '0' : '10px',
          }}
        >
          <div
            className={`bg-white shadow p-3 d-flex justify-content-between align-items-center ${
              expandedTab === 'Herunterladen' ? 'rounded-top' : 'rounded'
            }`}
            style={{
              margin: "0 20px",
              cursor: "pointer",
            }}
            onClick={() => {
              setExpandedTab(expandedTab === 'Herunterladen' ? null : 'Herunterladen');
            }}
          >
            <h5 className="mb-0">Herunterladen</h5>
            {/* Expand/Collapse Icon */}
            <FontAwesomeIcon
              icon={expandedTab === 'Herunterladen' ? faChevronUp : faChevronDown}
            />
          </div>
          {expandedTab === 'Herunterladen' && (
            <div
              className="bg-white shadow p-3 rounded-bottom"
              style={{
                margin: "0 20px 20px",
              }}
            >
              {renderHerunterladenContent()}
            </div>
          )}
        </div>
      </div>
    );
  }

  // Desktop Layout remains unchanged
  return (
    <div
      className="d-flex align-items-end"
      style={{
        position: "absolute",
        bottom: "20px",
        left: "20px",
        right: "20px",
        gap: "20px",
        zIndex: 1050,
        pointerEvents: "none",
      }}
    >
      <div
        className="d-flex gap-3"
        style={{
          marginRight: "auto",
          pointerEvents: "none",
        }}
      >
        {/* Auswahl Panel */}
        <div
          className="bg-white rounded shadow p-3 d-flex flex-column align-items-center justify-content-center"
          style={{
            width: "300px",
            minHeight: "200px",
            pointerEvents: "auto",
          }}
        >
          <h5 className="mb-3 text-center">Auswahl</h5>
          {renderAuswahlContent()}
        </div>
      </div>

      {/* Herunterladen Panel */}
      <div
        className="bg-white rounded shadow p-3 d-flex flex-column "
        style={{
          width: "300px",
          minHeight: "50px",
          marginLeft: "auto",
          transition: "height 0.3s ease-in-out",
          pointerEvents: "auto",
        }}
      >
        <h5 className="mb-3 text-center">Herunterladen</h5>
        {renderHerunterladenContent()}
      </div>
    </div>
  );
};

export default FloatingPanel;


&&& FILE: ./frontend\src\index.tsx
&&& CONTENT:
import React from 'react';
import ReactDOM from 'react-dom/client';
import App from './App';

ReactDOM.createRoot(document.getElementById('root')!).render(<App />);


&&& FILE: ./frontend\src\Legals.tsx
&&& CONTENT:
import React, { useState, useEffect } from 'react';
import ReactMarkdown from 'react-markdown';
import remarkGfm from 'remark-gfm';

const LegalDocumentPanel: React.FC<{
  documentType: string;
  isOpen: boolean;
  setOpenDocument: (doc: string | null) => void;
}> = ({ documentType, isOpen, setOpenDocument }) => {
  const [content, setContent] = useState('');

  useEffect(() => {
    if (isOpen && !content) {
      const fetchContent = async () => {
        try {
          const response = await fetch(`/docs/${documentType}.md`);
          const text = await response.text();
          setContent(text);
        } catch (error) {
          console.error(`Error loading ${documentType}:`, error);
          setContent('Failed to load content.');
        }
      };
      fetchContent();
    }
  }, [isOpen, content, documentType]);

  const handleClick = () => {
    setOpenDocument(isOpen ? null : documentType);
  };

  return ( 
    <>
      <button
        onClick={handleClick}
        className="btn btn-sm btn-light"
        style={{
          display: 'inline-block',
          padding: '0.25rem 0.5rem',           
           fontSize: "0.8rem",

        }}
      >
        {documentType.charAt(0).toUpperCase() + documentType.slice(1)}
      </button>

      {isOpen && (
        <div
          className="bg-white rounded shadow p-3"
          style={{
            position: 'absolute',
            top: '50px',
            right: '10px',
            maxWidth: '600px',
            width: '80vw',
            maxHeight: '50vh',
            overflowY: 'auto',
            zIndex: 1070,
          }}
        >
          <button
            className="btn btn-sm btn-close float-end"
            onClick={() => setOpenDocument(null)}
          />
          <h5 className="mb-3">
            {documentType.charAt(0).toUpperCase() + documentType.slice(1)}
          </h5>
          <ReactMarkdown remarkPlugins={[remarkGfm]}>{content}</ReactMarkdown>
        </div>
      )}
    </>
  );
};

const LegalDocuments: React.FC = () => {
  const [openDocument, setOpenDocument] = useState<string | null>(null);

  return (
    <div
      style={{
        display: 'flex',
        gap: '10px',
        pointerEvents: "auto"
      }}
    >
      <LegalDocumentPanel
        documentType="quellen"
        isOpen={openDocument === 'quellen'}
        setOpenDocument={setOpenDocument}
      />
      <LegalDocumentPanel
        documentType="impressum"
        isOpen={openDocument === 'impressum'}
        setOpenDocument={setOpenDocument}
      />
      <LegalDocumentPanel
        documentType="datenschutz"
        isOpen={openDocument === 'datenschutz'}
        setOpenDocument={setOpenDocument}
      />
      <LegalDocumentPanel
        documentType="AGB & Widerruf"
        isOpen={openDocument === 'AGB & Widerruf'}
        setOpenDocument={setOpenDocument}
      />
    </div>
  );
};


export default LegalDocuments;


&&& FILE: ./frontend\src\Logo.tsx
&&& CONTENT:
import React, { useState, useEffect } from "react";
import "bootstrap/dist/css/bootstrap.min.css";
import "./styles.css";

const Logo: React.FC = () => {
  const [isExpanded, setIsExpanded] = useState(true); // Control visibility
  const [timeoutId, setTimeoutId] = useState<number | null>(null);

  const toggleDescription = () => {
    setIsExpanded((prev) => !prev);

    // Clear the timeout if the user manually toggles the button
    if (timeoutId) {
      clearTimeout(timeoutId);
      setTimeoutId(null);
    }

    // Reset auto-hide timer if expanded
    if (!isExpanded) {
      const id = window.setTimeout(() => setIsExpanded(false), 30000);
      setTimeoutId(id);
    }
  };

  useEffect(() => {
    // Auto-hide description after 30 seconds
    const id = window.setTimeout(() => setIsExpanded(false), 30000);
    setTimeoutId(id);

    return () => {
      if (id) clearTimeout(id);
    };
  }, []);

  return (
    <div
      className="d-inline-block px-4 py-2 rounded shadow bg-light"
      style={{
        color: "black",
        pointerEvents: "auto",
        fontWeight: 500,
        width: "450px",
        maxWidth: "80%",
        position: "relative",
      }}
    >
      <div
        className="space-grotesk-regular text-center"
        style={{
          fontSize: "2rem",
        }}
      >
        EasyOpenData
      </div>

      {isExpanded && (
        <div
          className="text-left mb-3 ibm-plex-sans-light"
          style={{
            position: "absolute", // Make it float independently
            top: "3rem",          // Adjust the vertical positioning
            left: 0,
            width: "100%",        // Ensure it spans the logo's width
            fontSize: "1rem",
            fontFamily: "IBM Plex Sans, sans-serif",
            fontWeight: 300, // Matches ibm-plex-sans-light
            marginTop: "0.5rem",
            lineHeight: "1.5",
            backgroundColor: "#f8f9fa",
            padding: "0.5rem",
            borderRadius: "5px",
            boxShadow: "0px 4px 6px rgba(0, 0, 0, 0.1)",
          }}

        >
          <strong>
            Zeichnen Sie ein Polygon auf der Karte ein und laden präzise Gebäudegeometrie als .obj-Dateien herunter.
            Nach Zahlung erhalten Sie direkt Ihren Download-Link.<br />
            Kosten: Polygonfläche × 50 €/km²<br />
          </strong>
        </div>
      )}

      <button
        onClick={toggleDescription}
        style={{
          position: "absolute",
          top: "0.2rem",
          right: "0.2rem",
          background: "none",
          border: "none",
          cursor: "pointer",
          fontSize: "1rem",
          fontWeight: "bold",
        }}
      >
        {isExpanded ? "–" : "+"}
      </button>
    </div>
  );
};

export default Logo;


&&& FILE: ./frontend\src\styles.css
&&& CONTENT:
/* @import url('https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap'); */
@import url('https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&family=Space+Grotesk:wght@300..700&display=swap');


html,
body {
  height: 100%;
  overflow: hidden;  /* Prevent scrolling */
}

/* Override all fonts */
* {
  font-family: "IBM Plex Sans", sans-serif;
  font-optical-sizing: auto;
  font-weight: 300;
  font-style: normal;
}

.ibm-plex-sans-thin {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 100;
  font-style: normal;
}

.ibm-plex-sans-extralight {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 200;
  font-style: normal;
}

.ibm-plex-sans-light {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 300;
  font-style: normal;
}

.ibm-plex-sans-regular {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 400;
  font-style: normal;
}

.ibm-plex-sans-medium {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 500;
  font-style: normal;
}

.ibm-plex-sans-semibold {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 600;
  font-style: normal;
}

.ibm-plex-sans-bold {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 700;
  font-style: normal;
}

.ibm-plex-sans-thin-italic {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 100;
  font-style: italic;
}

.ibm-plex-sans-extralight-italic {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 200;
  font-style: italic;
}

.ibm-plex-sans-light-italic {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 300;
  font-style: italic;
}

.ibm-plex-sans-regular-italic {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 400;
  font-style: italic;
}

.ibm-plex-sans-medium-italic {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 500;
  font-style: italic;
}

.ibm-plex-sans-semibold-italic {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 600;
  font-style: italic;
}

.ibm-plex-sans-bold-italic {
  font-family: "IBM Plex Sans", sans-serif;
  font-weight: 700;
  font-style: italic;
}

.space-grotesk-light {
  font-family: "Space Grotesk", sans-serif;
  font-optical-sizing: auto;
  font-weight: 300;
  font-style: normal;
}

.space-grotesk-regular {
  font-family: "Space Grotesk", sans-serif;
  font-optical-sizing: auto;
  font-weight: 400;
  font-style: normal;
}

.space-grotesk-bold {
  font-family: "Space Grotesk", sans-serif;
  font-optical-sizing: auto;
  font-weight: 500;
  font-style: normal;
}

.space-grotesk-extrabold {
  font-family: "Space Grotesk", sans-serif;
  font-optical-sizing: auto;
  font-weight: 700;
  font-style: normal;
}

.top-bar-container {
  position: absolute;
  top: 10px;
  left: 10px;
  right: 10px;
  display: flex;
  flex-wrap: wrap;
  justify-content: center;
  /* Default mobile layout: everything centered */
  align-items: center;
  gap: 10px;
  z-index: 1060;
  pointer-events: none;
}

.top-bar-section {
  flex: 1 1 auto;
  display: flex;
  justify-content: center;
  pointer-events: auto;
  align-self: flex-start; /* Ensure alignment to the top of the container */
}


/* On wider screens, switch to a spaced layout:
   - Legals on the left
   - Logo on the right
*/
@media (min-width: 1000px) {
  .top-bar-container {
    justify-content: space-between;
  }

  .legals {
    justify-content: flex-start;
  }

  .logo {
    justify-content: flex-end;
  }
}

