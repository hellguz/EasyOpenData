&&& FILE: ./.gitignore
&&& CONTENT:
# Ignore local data files
data_local/

# Python cache
__pycache__/
*.pyc

# Environment variables
.env

# Other ignores
*.so

*.gml
*.gfs

#venv
venv/
.venv/

&&& FILE: ./commands.md
&&& CONTENT:
### Generate Backend
Please create a backend  repo for my project. I will paste README of it in the end of this message.
I want to use FastAPI+PostGIS for it. I plan to start with 3d buildings, so paste parcels and terrain only as place holders. I plan to create 17 scripts in folder (/data_ingest) (one per bundesland) for uploading their specific .gml files (stored in /data_local/bundesland_name/file1.gml, file2.gml,..)

**___README___**

I want you to reply with ONE code block structured like this:

&&& FILE: path/to/file.ext
&&& CONTENT:
content
of
the
file

&&& FILE: path/to/another/file.ext
&&& CONTENT:
content
of
another
file

&&& FILE: ./README.md
&&& CONTENT:
# EasyOpenData
## Open Data Extractor for German Spatial Datasets

## Overview
This project aims to provide an easy-to-use platform for accessing and downloading spatial data from German open data sources, covering all Bundesländer. The system standardizes the diverse formats in which these datasets are available and presents them to users via an intuitive web interface.

Users can interact with a map, select areas of interest (via polygons or rectangles), and choose the data layers they wish to download. Available data types include:
- Parcels
- Terrain
- 3D Buildings (LOD1/LOD2)

The platform processes the selected data and provides it in the user's desired format after payment, either as a direct download or via email.

---

## Key Features
- **Map-Based User Interface**: Allows users to interact with spatial data visually by selecting areas on a map.
- **Multi-Format Support**: Data is standardized into a unified format, enabling seamless querying and export in multiple formats (e.g., GeoJSON, glTF).
- **Dynamic Data Processing**: Only fetches and processes data for the area selected by the user, ensuring efficiency.
- **Real-Time Visualization**: Users can preview data layers on the map as they select their areas of interest.
- **Scalable Backend**: Built for performance, capable of handling large datasets from multiple regions.

---

## Technologies Used

### **Backend**

- **PostGIS**: Spatial database for storing and querying geographic data efficiently.
- **Python FastAPI**: Backend framework for building APIs to handle user requests and interact with PostGIS.
- **GDAL**: Used for data conversion between formats like CityGML, GeoJSON, and glTF.
- **Py3Dtiles/Trimesh**: For converting 3D data to glTF format dynamically.
- **Docker**: Containerized deployment for portability and scalability.

### **Frontend**

- **React + Leaflet**: Interactive map interface for 2D visualization and area selection.
- **CesiumJS**: 3D visualization for previewing datasets like LOD1/LOD2 buildings.

### **Infrastructure**

- **PostgreSQL + PostGIS**: Backend database for spatial data storage and indexing.
- **NGINX**: For serving static files and proxying API requests.
- **Cloud Storage**: For storing processed datasets ready for download (e.g., AWS S3, Google Cloud Storage).

---

## Workflow

1. **Data Ingestion**:
   - Import spatial data from various Bundesländer open data portals.
   - Convert datasets into a unified format (e.g., CityGML or PostGIS-compatible geometry).

2. **Data Storage**:
   - Store standardized data in a PostGIS database with spatial indexing for efficient querying.

3. **User Interaction**:
   - Users draw polygons/rectangles on the map to define areas of interest.
   - Backend processes the request, fetching relevant data using spatial queries.

4. **Data Processing**:
   - Convert queried data to web-friendly formats:
     - GeoJSON for 2D previews.
     - glTF for 3D previews.
   - Generate downloadable files in formats requested by the user.

5. **Data Delivery**:
   - Users download processed data directly from the browser or receive it via email after payment.

---

## Project Goals

1. **Standardization**: Harmonize data formats across all Bundesländer for consistent access and processing.
2. **Ease of Use**: Create a user-friendly interface for both novice and advanced users.
3. **Scalability**: Design a backend architecture capable of handling large datasets and high user demand.
4. **Flexibility**: Enable the export of data in various formats (e.g., GeoJSON, glTF, Shapefile).

---

## Example Usage

### **Frontend**

1. User opens the webpage and views a map interface.
2. Selects an area of interest by drawing a polygon or rectangle.
3. Chooses the desired data layers (e.g., 3D buildings).
4. Proceeds to payment and downloads the processed data.

### **Backend**

1. Receives the user’s area and data type requests via API.
2. Queries PostGIS for intersecting objects within the selected area.
3. Dynamically converts data into the requested format (e.g., glTF).
4. Returns the processed data for download.

---

## Setup

### **Backend**

1. Install PostgreSQL and PostGIS:

   ```bash
   sudo apt install postgresql postgis
   ```

2. Clone the repository and install dependencies:

   ```bash
   git clone https://github.com/your-repo/open-data-extractor.git
   cd open-data-extractor/backend
   pip install -r requirements.txt
   ```

3. Import sample data into PostGIS:

   ```bash
   ogr2ogr -f "PostgreSQL" PG:"dbname=your_db user=your_user password=your_pass" sample_data.gml
   ```

4. Run the FastAPI server:

   ```bash
   uvicorn main:app --reload
   ```

### **Frontend**

1. Install Node.js and dependencies:

   ```bash
   cd open-data-extractor/frontend
   npm install
   ```

2. Run the React development server:

   ```bash
   npm start
   ```

---

## Future Roadmap

- Add support for additional data layers (e.g., terrain, parcels).
- Implement caching for frequently requested data.
- Introduce user accounts for managing downloads and licenses.
- Explore additional formats for exporting data (e.g., OBJ, IFC).
- Expand coverage beyond Germany to other European countries.

---

## Contributing

Contributions are welcome! Please fork the repository and submit a pull request with your changes. Ensure all code adheres to the style guide and includes proper documentation.

---

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.

---

## Contact

For questions, feedback, or support, contact us at:

- **Email**: support@opendataextractor.com

&&& FILE: ./backend\.env
&&& CONTENT:
DATABASE_URL=postgresql+asyncpg://postgres:barcelona@localhost:5432/easyopendata_database


&&& FILE: ./backend\.gitignore
&&& CONTENT:
# Ignore local data files
data_local/

# Python cache
__pycache__/
*.pyc

# Environment variables
.env

# Other ignores
*.so

*.gml
*.gfs

#venv
venv/
.venv/


&&& FILE: ./backend\database.py
&&& CONTENT:
import os
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from models import Base

DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql+asyncpg://postgres:barcelona@localhost:5432/easyopendata_database')

engine = create_async_engine(DATABASE_URL, echo=True)
async_session = sessionmaker(
    bind=engine, class_=AsyncSession, expire_on_commit=False
)

# Function to create tables
async def init_db():
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)



&&& FILE: ./backend\main.py
&&& CONTENT:
# ./backend/main.py
from fastapi import FastAPI, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from database import async_session, init_db
from models import Building
from sqlalchemy.future import select
from geoalchemy2.functions import ST_AsGeoJSON, ST_Intersects, ST_GeomFromText, ST_SimplifyPreserveTopology
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import json
import math
# import redis  # Commented out Redis import

# redis_client = redis.Redis(host='localhost', port=6379, db=0)  # Commented out Redis client

app = FastAPI()

# Configure CORS
origins = [
    "http://localhost:8080",  # Frontend origin
    # Add other origins if needed
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,  # Allowed origins
    allow_credentials=True,
    allow_methods=["*"],    # Allow all HTTP methods
    allow_headers=["*"],    # Allow all headers
)

@app.on_event("startup")
async def on_startup():
    await init_db()

async def get_db():
    async with async_session() as session:
        yield session

@app.get("/")
async def read_root():
    return {"message": "Hello World"}

# Utility function to calculate tile bounding box
def tile_bbox(x: int, y: int, z: int):
    """
    Calculate the bounding box for a given tile in EPSG:4326.

    Args:
        x (int): Tile X coordinate.
        y (int): Tile Y coordinate.
        z (int): Zoom level.

    Returns:
        tuple: (min_lon, min_lat, max_lon, max_lat)
    """
    n = 2.0 ** z
    lon_deg_min = x / n * 360.0 - 180.0
    lat_rad_min = math.atan(math.sinh(math.pi * (1 - 2 * y / n)))
    lat_deg_min = math.degrees(lat_rad_min)

    lon_deg_max = (x + 1) / n * 360.0 - 180.0
    lat_rad_max = math.atan(math.sinh(math.pi * (1 - 2 * (y + 1) / n)))
    lat_deg_max = math.degrees(lat_rad_max)

    return (lon_deg_min, lat_deg_min, lon_deg_max, lat_deg_max)

@app.get("/buildings/tiles/{z}/{x}/{y}")
async def get_buildings_tile(z: int, x: int, y: int, db: AsyncSession = Depends(get_db)):
    """
    Get buildings within the bounding box of a specific tile.
    """
    try:
        # tile_key = f"{z}/{x}/{y}"
        # cached_data = redis_client.get(tile_key)
        # if cached_data:
        #     geojson = json.loads(cached_data)
        #     headers = {
        #         "Cache-Control": "public, max-age=86400"
        #     }
        #     return JSONResponse(content=geojson, headers=headers)

        # Calculate bounding box for the tile
        bbox = tile_bbox(x, y, z)
        bbox_wkt = f'POLYGON(({bbox[0]} {bbox[1]}, {bbox[0]} {bbox[3]}, {bbox[2]} {bbox[3]}, {bbox[2]} {bbox[1]}, {bbox[0]} {bbox[1]}))'

        # Determine simplification tolerance based on zoom level
        tolerance = get_simplification_tolerance(z)

        # Query buildings within the bounding box with simplification
        query = select(
            Building.ogc_fid,
            Building.name,
            ST_AsGeoJSON(ST_SimplifyPreserveTopology(Building.geometry, tolerance)).label('geometry')
        ).where(
            ST_Intersects(Building.geometry, ST_GeomFromText(bbox_wkt, 4326))
        )

        result = await db.execute(query)
        buildings = result.fetchall()

        # Construct GeoJSON FeatureCollection
        features = []
        for building in buildings:
            geometry = json.loads(building.geometry)
            feature = {
                "type": "Feature",
                "geometry": geometry,
                "properties": {
                    "id": building.ogc_fid,
                    "name": building.name,
                    "height": calculate_building_height(geometry)  # Function to calculate height
                }
            }
            features.append(feature)

        geojson = {
            "type": "FeatureCollection",
            "features": features
        }

        headers = {
            "Cache-Control": "public, max-age=86400"  # Cache for 1 day
        }

        # # After generating geojson
        # redis_client.set(tile_key, json.dumps(geojson), ex=86400)  # Cache for 1 day

        return JSONResponse(content=geojson, headers=headers)

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def get_simplification_tolerance(z: int):
    """Determine simplification tolerance based on zoom level."""
    if z < 10:
        return 0.01
    elif z < 14:
        return 0.005
    else:
        return 0.001

def calculate_building_height(geometry_geojson):
    """
    Calculate the height of a building from its geometry.

    Args:
        geometry_geojson (dict): GeoJSON geometry.

    Returns:
        float: Height in meters.
    """
    # Assuming geometry is a MultiPolygonZ or PolygonZ
    # Extract Z values and calculate height
    z_values = []
    if geometry_geojson['type'] == 'MultiPolygon':
        for polygon in geometry_geojson['coordinates']:
            for ring in polygon:
                for coord in ring:
                    if len(coord) == 3:
                        z_values.append(coord[2])
    elif geometry_geojson['type'] == 'Polygon':
        for ring in geometry_geojson['coordinates']:
            for coord in ring:
                if len(coord) == 3:
                    z_values.append(coord[2])

    if z_values:
        return max(z_values) - min(z_values)
    else:
        return 30.0  # Default height if Z is not available


&&& FILE: ./backend\models.py
&&& CONTENT:
from sqlalchemy import Column, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from geoalchemy2 import Geometry

Base = declarative_base()

class Building(Base):
    __tablename__ = 'building'

    ogc_fid = Column(Integer, primary_key=True)
    name = Column(String)
    geometry = Column(Geometry('MULTIPOLYGONZ', srid=4326))  # Adjust geometry type as needed


&&& FILE: ./backend\requirements.txt
&&& CONTENT:
fastapi
uvicorn[standard]
asyncpg
psycopg2-binary
sqlalchemy
geoalchemy2
# Additional dependencies
python-multipart
geojson
anyio
pydantic
packaging
redis

&&& FILE: ./backend\setup_database.md
&&& CONTENT:
# Database Setup Instructions for Windows

Follow these steps to create the PostgreSQL database with the PostGIS extension on Windows:

---

## 1. Install PostgreSQL and PostGIS

### **Download the Installer**

1. Visit the PostgreSQL official website: [PostgreSQL Downloads](https://www.postgresql.org/download/windows/).
2. Click on **"Download the installer"** to go to the EnterpriseDB download page.
3. Download the latest version of the PostgreSQL installer for Windows.

### **Run the Installer**

1. Run the downloaded `.exe` installer.
2. Follow the installation wizard steps:
   - **Installation Directory**: Choose your preferred installation directory.
   - **Select Components**: Ensure that **"PostGIS Bundle"** is checked to install PostGIS along with PostgreSQL.
   - **Password Setup**: Set a password for the default `postgres` superuser. Remember this password.
   - **Port Number**: Default is `5432`. You can change it if needed.
   - **Locale**: Choose the default locale or your preferred setting.
3. Complete the installation and wait for it to finish.

---

## 2. Create a New PostgreSQL Database and User

### **Open pgAdmin**

1. After installation, open **pgAdmin 4** from the Start Menu.
2. When prompted, enter the password you set for the `postgres` user during installation.

### **Create a New Database**

1. In pgAdmin, expand the server tree to see **"Databases"**.
2. Right-click on **"Databases"** and select **"Create"** > **"Database..."**.
3. In the **"Database"** dialog:
   - **Database Name**: Enter `your_db`.
   - **Owner**: Select `postgres` or your preferred user.
4. Click **"Save"**.

### **Create a New User (Role)**

1. Expand the **"Login/Group Roles"** under your server.
2. Right-click on **"Login/Group Roles"** and select **"Create"** > **"Login/Group Role..."**.
3. In the **"Properties"** tab:
   - **Role Name**: Enter `your_user`.
4. In the **"Definition"** tab:
   - **Password**: Enter `your_password`.
   - **Confirm Password**: Re-enter the password.
5. In the **"Privileges"** tab:
   - Set **"Can login?"** to **"Yes"**.
6. Click **"Save"**.

### **Grant Privileges to the User**

1. In pgAdmin, navigate to **"Databases"** > **"your_db"** > **"Schemas"** > **"public"**.
2. Right-click on **"public"** schema and select **"Properties"**.
3. Go to the **"Privileges"** tab.
4. Click on the **"Add"** icon (a plus sign).
5. In the new row:
   - **Role**: Select `your_user`.
   - **Privileges**: Check all the boxes (or at least **"Usage"** and **"Create"**).
6. Click **"Save"**.

### **Enable the PostGIS Extension**

1. Right-click on **"your_db"** and select **"Query Tool"**.
2. In the Query Editor, run the following SQL command:

   ```sql
   CREATE EXTENSION postgis;
   ```

3. Click the **"Execute/Refresh"** button (lightning bolt icon) to run the query.
4. You should see a message indicating that the extension was created successfully.

---

## 3. Update Your Database Configuration

### **Modify the `.env` File**

In your project directory, locate the `.env` file inside the `./backend` folder and update the `DATABASE_URL`:

```
DATABASE_URL=postgresql+asyncpg://your_user:your_password@localhost:5432/your_db
```

---

## 4. Run the Application to Create Tables

The `init_db()` function in `database.py` will automatically create the necessary tables when the application starts.

### **Open Command Prompt or PowerShell**

Navigate to your project's backend directory:

```cmd
cd path\to\your\project\backend
```

### **Create a Virtual Environment (Optional but Recommended)**

```cmd
python -m venv venv
venv\Scripts\activate
```

### **Install Dependencies**

Ensure all the required Python packages are installed:

```cmd
pip install -r requirements.txt
```

### **Start the FastAPI Application**

```cmd
uvicorn main:app --reload
```

---

## 5. Verify the Tables Have Been Created

### **Using pgAdmin**

1. In pgAdmin, right-click on **"Tables"** under **"your_db"** > **"Schemas"** > **"public"**, and select **"Refresh"**.
2. Expand the **"Tables"** section.
3. You should see the `buildings` table listed.

---

## 6. Ingest Data into the Database

Use the provided data ingestion scripts to populate the database with building data.

### **Install GDAL for Windows**

1. Download the GDAL Windows binaries from [GIS Internals](https://www.gisinternals.com/query.html?content=filelist&file=release-1930-x64-gdal-3-4-1-mapserver-7-6-4.zip).
2. Extract the contents to a directory (e.g., `C:\Program Files\GDAL`).
3. Add the GDAL bin directory to your system PATH:
   - Open **Control Panel** > **System** > **Advanced system settings**.
   - Click on **"Environment Variables"**.
   - Under **"System Variables"**, find and edit the **"Path"** variable.
   - Add the path to the GDAL `bin` directory (e.g., `C:\Program Files\GDAL\bin`).
4. Set the `GDAL_DATA` environment variable:
   - In **"System Variables"**, click **"New"**.
   - **Variable name**: `GDAL_DATA`
   - **Variable value**: `C:\Program Files\GDAL\gdal-data`

### **Place Your GML Files**

Add your `.gml` files into the appropriate directories under `data_local\{bundesland_name}\`.

### **Run the Ingestion Script**

```cmd
python data_ingest\ingest_baden_wuerttemberg.py
```

Replace `ingest_baden_wuerttemberg.py` with the script corresponding to your Bundesland.

**Note**: You may need to install `osgeo` dependencies for GDAL to work with Python scripts.

---

## 7. Test the Endpoint

You can now test the `/buildings` endpoint to retrieve buildings within a given boundary.

### **Example Request Using cURL**

```cmd
curl -X POST "http://localhost:8000/buildings" -H "Content-Type: application/json" -d "{\"type\":\"Polygon\",\"coordinates\":[[[9.0,48.0],[9.1,48.0],[9.1,48.1],[9.0,48.1],[9.0,48.0]]]}"
```

### **Example Request Using Python**

```python
import requests

url = "http://localhost:8000/buildings"
geometry = {
    "type": "Polygon",
    "coordinates": [
        [
            [9.0, 48.0],
            [9.1, 48.0],
            [9.1, 48.1],
            [9.0, 48.1],
            [9.0, 48.0]
        ]
    ]
}

response = requests.post(url, json=geometry)
print(response.json())
```

---

## Notes

- Ensure that the SRID (Spatial Reference System Identifier) matches between your data and the database. The default SRID in the model is `4326`.
- If you encounter any issues, check the application logs for errors and verify your database connection settings.
- Make sure that the `psycopg2` package is installed properly. On Windows, you might need to install `psycopg2-binary`.

---

## Troubleshooting

### **Common Issues**

- **GDAL Not Found**: Ensure that GDAL is correctly installed and added to your system PATH.
- **Database Connection Errors**: Double-check your `DATABASE_URL` in the `.env` file.
- **Permission Denied**: Run Command Prompt or PowerShell as an administrator if you encounter permission issues.
- **Port Conflicts**: Ensure that port `5432` (PostgreSQL) and `8000` (FastAPI default) are not being used by other applications.

---

## Additional Resources

- [PostgreSQL Documentation](https://www.postgresql.org/docs/)
- [PostGIS Documentation](https://postgis.net/documentation/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [GDAL Documentation](https://gdal.org/)



&&& FILE: ./backend\start_postgres.bat
&&& CONTENT:
pg_ctl -D "C:\PostgreSQL\17.1\data" -l logfile start
pause


&&& FILE: ./backend\data_ingest\ingest_baden_wuerttemberg.py
&&& CONTENT:
import os
import glob
import subprocess

DATA_DIR = 'data_local/baden_wuerttemberg'
DATABASE_URL = os.getenv('DATABASE_URL', 'PG:dbname=your_db user=your_user password=your_pass')

def ingest_gml_files():
    gml_files = glob.glob(os.path.join(DATA_DIR, '*.gml'))
    for gml_file in gml_files:
        cmd = [
            'ogr2ogr',
            '-f', 'PostgreSQL',
            '-overwrite',
            '-progress',
            DATABASE_URL,
            gml_file
        ]
        subprocess.run(cmd)

if __name__ == '__main__':
    ingest_gml_files()



&&& FILE: ./backend\data_ingest\ingest_bayern.py
&&& CONTENT:
import os
import glob
import subprocess

DATA_DIR = 'backend/data_local/bayern'
DATABASE_URL = os.getenv('DATABASE_URL', 'PG:dbname=easyopendata_database user=postgres password=barcelona')

def ingest_gml_files():
    gml_files = glob.glob(os.path.join(DATA_DIR, '*.gml'))
    for gml_file in gml_files:
        cmd = [
            'ogr2ogr',
            '-f', 'PostgreSQL',
            '-overwrite',
            '-progress',
            '-lco', 'GEOMETRY_NAME=geometry',
            '-skipfailures',
            '-nlt', 'CONVERT_TO_LINEAR',
            '-nlt', 'MULTIPOLYGON',
            '-s_srs', 'EPSG:25832',
            '-t_srs', 'EPSG:4326',
            DATABASE_URL,
            gml_file
        ]
        subprocess.run(cmd)

if __name__ == '__main__':
    ingest_gml_files()




&&& FILE: ./backend\data_ingest\ingest_thuringia
&&& CONTENT:
import os
import glob
import subprocess

DATA_DIR = 'backend/data_local/thuringia'
DATABASE_URL = os.getenv('DATABASE_URL', 'PG:dbname=easyopendata_database user=postgres password=barcelona')

def ingest_gml_files():
    gml_files = glob.glob(os.path.join(DATA_DIR, '*.gml'))
    for gml_file in gml_files:
        cmd = [
            'ogr2ogr',
            '-f', 'PostgreSQL',
            '-overwrite',
            '-progress',
            '-lco', 'GEOMETRY_NAME=geometry',
            '-skipfailures',
            '-nlt', 'CONVERT_TO_LINEAR',
            '-nlt', 'MULTIPOLYGON',
            '-s_srs', 'EPSG:25832',
            '-t_srs', 'EPSG:4326',
            DATABASE_URL,
            gml_file
        ]
        subprocess.run(cmd)

if __name__ == '__main__':
    ingest_gml_files()




&&& FILE: ./backend\data_ingest\transform_gml.py
&&& CONTENT:
#!/usr/bin/env python3
"""
transform_gml.py

A script to transform a CityGML file by embedding polygon geometries directly within
lod2Solid's surfaceMember elements, removing external xlink references.

Usage:
    python transform_gml.py input.gml output.gml
"""

### python .\transform_gml.py .\..\data_local\bayern\raw\650_5478.gml .\..\data_local\bayern\650_5478_trs.gml


import sys
import os
from lxml import etree

def get_all_namespaces(gml_tree):
    """
    Extracts all namespaces from the GML tree and assigns a unique prefix to the default namespace.

    Args:
        gml_tree (etree.ElementTree): Parsed GML tree.

    Returns:
        dict: Namespace prefix to URI mapping.
    """
    nsmap = gml_tree.getroot().nsmap.copy()
    # Handle default namespace (None key)
    if None in nsmap:
        nsmap['default'] = nsmap.pop(None)
    # Ensure 'xlink' is included
    if 'xlink' not in nsmap:
        # Attempt to find the xlink namespace
        for prefix, uri in nsmap.items():
            if uri == 'http://www.w3.org/1999/xlink':
                nsmap['xlink'] = uri
                break
        else:
            # If not found, add it manually
            nsmap['xlink'] = 'http://www.w3.org/1999/xlink'
    return nsmap

def transform_gml(input_file, output_file):
    """
    Transforms the input GML file by embedding polygons into surfaceMember elements.

    Args:
        input_file (str): Path to the input GML file.
        output_file (str): Path to the output transformed GML file.
    """
    # Parse the GML file
    print(f"Parsing input GML file: {input_file}")
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(input_file, parser)
    root = tree.getroot()

    # Extract all namespaces
    namespaces = get_all_namespaces(tree)
    print("Namespaces detected:")
    for prefix, uri in namespaces.items():
        print(f"  Prefix: '{prefix}' => URI: '{uri}'")

    # Build a dictionary of gml:id to Polygon elements for quick lookup
    print("Indexing all <gml:Polygon> elements by gml:id...")
    polygon_dict = {}
    for polygon in root.xpath('.//gml:Polygon', namespaces=namespaces):
        polygon_id = polygon.get('{http://www.opengis.net/gml}id')
        if polygon_id:
            polygon_dict[polygon_id] = polygon
    print(f"Indexed {len(polygon_dict)} polygons.")

    # Find all <gml:surfaceMember> elements with xlink:href
    print("Finding all <gml:surfaceMember> elements with xlink:href...")
    surface_members = root.xpath('.//gml:surfaceMember[@xlink:href]', namespaces=namespaces)
    print(f"Found {len(surface_members)} <gml:surfaceMember> elements with xlink:href.")

    for sm in surface_members:
        href = sm.get('{http://www.w3.org/1999/xlink}href')
        if not href:
            continue
        # Extract the referenced polygon ID (remove the '#' prefix)
        polygon_id = href.lstrip('#')
        print(f"Processing surfaceMember referencing Polygon ID: {polygon_id}")
        polygon = polygon_dict.get(polygon_id)
        if not polygon:
            print(f"Warning: Polygon with gml:id='{polygon_id}' not found. Skipping.")
            continue
        # Deep copy the polygon element
        polygon_copy = etree.fromstring(etree.tostring(polygon))
        # Remove any existing 'gml:id' to avoid duplicate IDs
        polygon_copy.attrib.pop('{http://www.opengis.net/gml}id', None)
        # Replace the surfaceMember's xlink:href attribute with the actual Polygon
        sm.clear()  # Remove existing children and attributes
        sm.append(polygon_copy)
        print(f"Embedded Polygon ID: {polygon_id} into surfaceMember.")

    # Optionally, remove standalone <gml:Polygon> elements that were referenced
    print("Removing standalone <gml:Polygon> elements that were referenced...")
    removed_count = 0
    # for polygon_id in polygon_dict.keys():
    #     # Find and remove the standalone polygon
    #     polygons_to_remove = root.xpath(f'.//gml:Polygon[@gml:id="{polygon_id}"]', namespaces=namespaces)
    #     for polygon in polygons_to_remove:
    #         parent = polygon.getparent()
    #         if parent is not None:
    #             parent.remove(polygon)
    #             removed_count += 1
    #             print(f"Removed standalone Polygon ID: {polygon_id}.")
    # print(f"Removed {removed_count} standalone polygons.")

    # Write the transformed GML to the output file
    print(f"Writing transformed GML to: {output_file}")
    tree.write(output_file, pretty_print=True, xml_declaration=True, encoding='UTF-8')
    print("Transformation complete.")

def main():
    if len(sys.argv) != 3:
        print("Usage: python transform_gml.py input.gml output.gml")
        sys.exit(1)
    
    input_file = sys.argv[1]
    output_file = sys.argv[2]

    if not os.path.isfile(input_file):
        print(f"Error: Input file '{input_file}' does not exist.")
        sys.exit(1)

    try:
        transform_gml(input_file, output_file)
    except etree.XMLSyntaxError as e:
        print(f"XML Syntax Error: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"An error occurred during transformation: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()


&&& FILE: ./backend\data_local\README.md
&&& CONTENT:
# Data Local Directory

Place your local GML data files here under their respective Bundesland directories.

Example:

- data_local/baden_wuerttemberg/file1.gml
- data_local/bayern/file2.gml

Ensure the directory structure matches the Bundesland names used in the data_ingest scripts.



&&& FILE: ./backend\postgis_docker\docker-compose.yml
&&& CONTENT:

services:

  db:
    image: postgis/postgis:17-3.5
    restart: always
    # set shared memory limit when using docker-compose
    shm_size: 128mb
    # or set shared memory limit when deploy via swarm stack
    #volumes:
    #  - type: tmpfs
    #    target: /dev/shm
    #    tmpfs:
    #      size: 134217728 # 128*2^20 bytes = 128Mb
    ports:
    - 5432:5432
    environment:
      POSTGRES_PASSWORD: barcelona


&&& FILE: ./frontend\index.html
&&& CONTENT:
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>EasyOpenData - Nürnberg Buildings</title>
    <!-- Leaflet CSS -->
    <link
      rel="stylesheet"
      href="https://unpkg.com/leaflet@1.9.3/dist/leaflet.css"
    />
    <style>
      html,
      body,
      #map {
        width: 100%;
        height: 100%;
        margin: 0;
        padding: 0;
        overflow: hidden;
        background-color: #2e2e2e; /* Optional: Dark background */
      }
    </style>
  </head>
  <body>
    <div id="map"></div>
    <!-- Leaflet JS -->
    <script src="https://unpkg.com/leaflet@1.9.3/dist/leaflet.js"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function () {
        console.log("DOM fully loaded and parsed");

        // Initialize the Leaflet map
        const map = L.map("map").setView([49.4497, 11.0683], 14); // Centered on Nürnberg
        console.log("Leaflet map initialized");

        // Add CartoDB Dark Matter tile layer
        L.tileLayer(
          "https://{s}.basemaps.cartocdn.com/dark_all/{z}/{x}/{y}{r}.png",
          {
            attribution:
              '&copy; <a href="https://carto.com/attributions">CARTO</a>',
            subdomains: "abcd",
            maxZoom: 19,
          }
        ).addTo(map);
        console.log("Dark Matter tile layer added to map");

        // Layer Group to manage building layers
        const buildingsLayerGroup = L.layerGroup().addTo(map);

        // Debounce function to limit the rate of tile loading
        function debounce(func, delay) {
          let debounceTimer;
          return function () {
            const context = this;
            const args = arguments;
            clearTimeout(debounceTimer);
            debounceTimer = setTimeout(() => func.apply(context, args), delay);
          };
        }

        // Convert longitude to tile number
        function long2tile(lon, lat, zoom) {
          const n = Math.pow(2, zoom);
          const xtile = Math.floor(((lon + 180) / 360) * n);
          const ytile = Math.floor(
            ((1 -
              Math.log(
                Math.tan((lat * Math.PI) / 180) +
                  1 / Math.cos((lat * Math.PI) / 180)
              ) /
                Math.PI) /
              2) *
              n
          );
          return { x: xtile, y: ytile };
        }

        // Fetch and render building data for visible tiles
        async function loadBuildings() {
          const bounds = map.getBounds();
          const zoom = map.getZoom();

          // Determine tile range for current bounds
          const nwTile = long2tile(bounds.getWest(), bounds.getNorth(), zoom);
          const seTile = long2tile(bounds.getEast(), bounds.getSouth(), zoom);

          for (let x = nwTile.x; x <= seTile.x; x++) {
            for (let y = nwTile.y; y <= seTile.y; y++) {
              await fetchTileData(x, y, zoom); // Fetch only missing tiles
            }
          }
        }

        const fetchedTiles = {}; // Cache to track loaded tiles

        // Remove tiles outside the current viewport
        function removeOutdatedTiles() {
          const bounds = map.getBounds();
          for (const tileKey in fetchedTiles) {
            const [z, x, y] = tileKey.split("/").map(Number);
            const tileBounds = getTileBounds(x, y, z);
            if (!bounds.intersects(tileBounds)) {
              fetchedTiles[tileKey] = false; // Allow re-fetch if needed
              buildingsLayerGroup.eachLayer((layer) => {
                if (layer.tileKey === tileKey)
                  buildingsLayerGroup.removeLayer(layer);
              });
            }
          }
        }

        // Dynamically calculate bounds for a tile
        function getTileBounds(x, y, z) {
          const [minLon, minLat, maxLon, maxLat] = tile_bbox(x, y, z);
          return L.latLngBounds([minLat, minLon], [maxLat, maxLon]);
        }

        // Calculate the bounding box for a given tile in EPSG:4326
        function tile_bbox(x, y, z) {
          const n = Math.pow(2, z);
          const lon_deg_min = (x / n) * 360.0 - 180.0;
          const lat_rad_min = Math.atan(Math.sinh(Math.PI * (1 - (2 * y) / n)));
          const lat_deg_min = (lat_rad_min * 180.0) / Math.PI;

          const lon_deg_max = ((x + 1) / n) * 360.0 - 180.0;
          const lat_rad_max = Math.atan(
            Math.sinh(Math.PI * (1 - (2 * (y + 1)) / n))
          );
          const lat_deg_max = (lat_rad_max * 180.0) / Math.PI;

          return [lon_deg_min, lat_deg_min, lon_deg_max, lat_deg_max];
        }

        // Fetch tiles only if not in cache
        async function fetchTileData(x, y, z) {
          const tileKey = `${z}/${x}/${y}`;
          if (fetchedTiles[tileKey]) return; // Skip if already fetched

          fetchedTiles[tileKey] = true; // Mark as fetched
          const url = `http://localhost:8000/buildings/tiles/${z}/${x}/${y}`;
          try {
            const response = await fetch(url);
            if (!response.ok)
              throw new Error(`HTTP error! status: ${response.status}`);
            const data = await response.json();

            // Add to map and track tileKey for removal
            const geojsonLayer = L.geoJSON(data, {
              style: { color: "yellow", weight: 1 },
            });
            geojsonLayer.tileKey = tileKey; // Tag layer with tileKey
            buildingsLayerGroup.addLayer(geojsonLayer);
          } catch (error) {
            console.error(`Error fetching tile ${tileKey}:`, error);
            fetchedTiles[tileKey] = false; // Allow retry if failed
          }
        }

        // Initial load
        loadBuildings();

        // Load buildings on map move end with debounce
        // Updated moveend handler with debounce and cleanup
        map.on(
          "moveend",
          debounce(() => {
            removeOutdatedTiles();
            loadBuildings();
          }, 300)
        );
      });
    </script>
  </body>
</html>


&&& FILE: ./frontend\README.md
&&& CONTENT:
# EasyOpenData Frontend

## Overview

This is a simple frontend HTML page that displays 2D buildings of Nürnberg using Leaflet. It fetches building data from the backend and renders them on an interactive map.

## Setup

1. **Serve the Frontend**

   Use Python's HTTP server to serve the frontend files.

   ```bash
   cd frontend
   python -m http.server 8080


&&& FILE: ./utils\small-portion.py
&&& CONTENT:
import json

# Path to the large GeoJSON file
input_file = r"output_dir\650_5478_repr.geojson"

# Path for the output smaller GeoJSON file
output_file = "650_5478_repr_small.geojson"

# Desired size of the output file in bytes (50 MB)
desired_size = 1 * 1024 * 1024

def extract_small_geojson(input_file, output_file, max_size):
    try:
        with open(input_file, 'r', encoding='utf-8') as infile:
            # Initialize variables to construct the new GeoJSON
            small_data = {
                "type": "FeatureCollection",
                "features": []
            }
            
            # Read the opening part of the GeoJSON file
            line = infile.readline()
            while line.strip() != '"features": [':
                line = infile.readline()

            # Read features one by one
            current_size = 0
            for line in infile:
                if line.strip() == ']':
                    break  # End of features list

                # Remove trailing comma if present
                feature_str = line.rstrip(',\n')
                
                # Parse feature JSON
                feature = json.loads(feature_str)
                
                # Add feature to the new collection
                small_data["features"].append(feature)
                
                # Update current size
                current_size += len(feature_str.encode('utf-8'))
                
                # Stop if the current size exceeds or reaches the desired size
                if current_size >= max_size:
                    break

            # Write closing brackets for the JSON structure
            with open(output_file, 'w', encoding='utf-8') as outfile:
                json.dump(small_data, outfile, ensure_ascii=False, indent=2)
            
            print(f"Extracted {len(small_data['features'])} features into {output_file}")

    except Exception as e:
        print(f"An error occurred: {e}")

# Run the function to extract a smaller GeoJSON file
extract_small_geojson(input_file, output_file, desired_size)

