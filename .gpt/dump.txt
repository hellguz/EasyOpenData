&&& FILE: ./.gitignore
&&& CONTENT:
# Ignore local data files
data_local/

# Python cache
__pycache__/
*.pyc

# Environment variables
.env

# Other ignores
*.so

#venv
venv/
.venv/

&&& FILE: ./commands.md
&&& CONTENT:
### Generate Backend
Please create a backend  repo for my project. I will paste README of it in the end of this message.
I want to use FastAPI+PostGIS for it. I plan to start with 3d buildings, so paste parcels and terrain only as place holders. I plan to create 17 scripts in folder (/data_ingest) (one per bundesland) for uploading their specific .gml files (stored in /data_local/bundesland_name/file1.gml, file2.gml,..)

**___README___**

I want you to reply with ONE code block structured like this:

&&& FILE: path/to/file.ext
&&& CONTENT:
content
of
the
file

&&& FILE: path/to/another/file.ext
&&& CONTENT:
content
of
another
file

&&& FILE: ./README.md
&&& CONTENT:
# EasyOpenData
## Open Data Extractor for German Spatial Datasets

## Overview
This project aims to provide an easy-to-use platform for accessing and downloading spatial data from German open data sources, covering all Bundesländer. The system standardizes the diverse formats in which these datasets are available and presents them to users via an intuitive web interface.

Users can interact with a map, select areas of interest (via polygons or rectangles), and choose the data layers they wish to download. Available data types include:
- Parcels
- Terrain
- 3D Buildings (LOD1/LOD2)

The platform processes the selected data and provides it in the user's desired format after payment, either as a direct download or via email.

---

## Key Features
- **Map-Based User Interface**: Allows users to interact with spatial data visually by selecting areas on a map.
- **Multi-Format Support**: Data is standardized into a unified format, enabling seamless querying and export in multiple formats (e.g., GeoJSON, glTF).
- **Dynamic Data Processing**: Only fetches and processes data for the area selected by the user, ensuring efficiency.
- **Real-Time Visualization**: Users can preview data layers on the map as they select their areas of interest.
- **Scalable Backend**: Built for performance, capable of handling large datasets from multiple regions.

---

## Technologies Used

### **Backend**

- **PostGIS**: Spatial database for storing and querying geographic data efficiently.
- **Python FastAPI**: Backend framework for building APIs to handle user requests and interact with PostGIS.
- **GDAL**: Used for data conversion between formats like CityGML, GeoJSON, and glTF.
- **Py3Dtiles/Trimesh**: For converting 3D data to glTF format dynamically.
- **Docker**: Containerized deployment for portability and scalability.

### **Frontend**

- **React + Leaflet**: Interactive map interface for 2D visualization and area selection.
- **CesiumJS**: 3D visualization for previewing datasets like LOD1/LOD2 buildings.

### **Infrastructure**

- **PostgreSQL + PostGIS**: Backend database for spatial data storage and indexing.
- **NGINX**: For serving static files and proxying API requests.
- **Cloud Storage**: For storing processed datasets ready for download (e.g., AWS S3, Google Cloud Storage).

---

## Workflow

1. **Data Ingestion**:
   - Import spatial data from various Bundesländer open data portals.
   - Convert datasets into a unified format (e.g., CityGML or PostGIS-compatible geometry).

2. **Data Storage**:
   - Store standardized data in a PostGIS database with spatial indexing for efficient querying.

3. **User Interaction**:
   - Users draw polygons/rectangles on the map to define areas of interest.
   - Backend processes the request, fetching relevant data using spatial queries.

4. **Data Processing**:
   - Convert queried data to web-friendly formats:
     - GeoJSON for 2D previews.
     - glTF for 3D previews.
   - Generate downloadable files in formats requested by the user.

5. **Data Delivery**:
   - Users download processed data directly from the browser or receive it via email after payment.

---

## Project Goals

1. **Standardization**: Harmonize data formats across all Bundesländer for consistent access and processing.
2. **Ease of Use**: Create a user-friendly interface for both novice and advanced users.
3. **Scalability**: Design a backend architecture capable of handling large datasets and high user demand.
4. **Flexibility**: Enable the export of data in various formats (e.g., GeoJSON, glTF, Shapefile).

---

## Example Usage

### **Frontend**

1. User opens the webpage and views a map interface.
2. Selects an area of interest by drawing a polygon or rectangle.
3. Chooses the desired data layers (e.g., 3D buildings).
4. Proceeds to payment and downloads the processed data.

### **Backend**

1. Receives the user’s area and data type requests via API.
2. Queries PostGIS for intersecting objects within the selected area.
3. Dynamically converts data into the requested format (e.g., glTF).
4. Returns the processed data for download.

---

## Setup

### **Backend**

1. Install PostgreSQL and PostGIS:

   ```bash
   sudo apt install postgresql postgis
   ```

2. Clone the repository and install dependencies:

   ```bash
   git clone https://github.com/your-repo/open-data-extractor.git
   cd open-data-extractor/backend
   pip install -r requirements.txt
   ```

3. Import sample data into PostGIS:

   ```bash
   ogr2ogr -f "PostgreSQL" PG:"dbname=your_db user=your_user password=your_pass" sample_data.gml
   ```

4. Run the FastAPI server:

   ```bash
   uvicorn main:app --reload
   ```

### **Frontend**

1. Install Node.js and dependencies:

   ```bash
   cd open-data-extractor/frontend
   npm install
   ```

2. Run the React development server:

   ```bash
   npm start
   ```

---

## Future Roadmap

- Add support for additional data layers (e.g., terrain, parcels).
- Implement caching for frequently requested data.
- Introduce user accounts for managing downloads and licenses.
- Explore additional formats for exporting data (e.g., OBJ, IFC).
- Expand coverage beyond Germany to other European countries.

---

## Contributing

Contributions are welcome! Please fork the repository and submit a pull request with your changes. Ensure all code adheres to the style guide and includes proper documentation.

---

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.

---

## Contact

For questions, feedback, or support, contact us at:

- **Email**: support@opendataextractor.com

&&& FILE: ./backend\.env
&&& CONTENT:
DATABASE_URL=postgresql+asyncpg://user:password@localhost:5432/your_db


&&& FILE: ./backend\database.py
&&& CONTENT:
import os
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql+asyncpg://user:password@localhost:5432/your_db')

engine = create_async_engine(DATABASE_URL, echo=True)
async_session = sessionmaker(
    bind=engine, class_=AsyncSession, expire_on_commit=False
)



&&& FILE: ./backend\main.py
&&& CONTENT:
from fastapi import FastAPI, Depends
from sqlalchemy.ext.asyncio import AsyncSession
from database import async_session

app = FastAPI()

async def get_db():
    async with async_session() as session:
        yield session

@app.get("/")
async def read_root():
    return {"message": "Hello World"}

# Additional routes and endpoints will be added here.



&&& FILE: ./backend\requirements.txt
&&& CONTENT:
fastapi
uvicorn[standard]
asyncpg
psycopg2-binary
sqlalchemy
geoalchemy2
gdal



&&& FILE: ./backend\data_ingest\ingest_baden_wuerttemberg.py
&&& CONTENT:
import os
import glob
import subprocess

DATA_DIR = 'data_local/baden_wuerttemberg'
DATABASE_URL = os.getenv('DATABASE_URL', 'PG:dbname=your_db user=your_user password=your_pass')

def ingest_gml_files():
    gml_files = glob.glob(os.path.join(DATA_DIR, '*.gml'))
    for gml_file in gml_files:
        cmd = [
            'ogr2ogr',
            '-f', 'PostgreSQL',
            '-overwrite',
            '-progress',
            DATABASE_URL,
            gml_file
        ]
        subprocess.run(cmd)

if __name__ == '__main__':
    ingest_gml_files()



&&& FILE: ./backend\data_ingest\ingest_bayern.py
&&& CONTENT:
import os
import glob
import subprocess

DATA_DIR = 'data_local/bayern'
DATABASE_URL = os.getenv('DATABASE_URL', 'PG:dbname=your_db user=your_user password=your_pass')

def ingest_gml_files():
    gml_files = glob.glob(os.path.join(DATA_DIR, '*.gml'))
    for gml_file in gml_files:
        cmd = [
            'ogr2ogr',
            '-f', 'PostgreSQL',
            '-overwrite',
            '-progress',
            DATABASE_URL,
            gml_file
        ]
        subprocess.run(cmd)

if __name__ == '__main__':
    ingest_gml_files()

# ... Create similar scripts for other Bundesländer ...



&&& FILE: ./backend\data_local\README.md
&&& CONTENT:
# Data Local Directory

Place your local GML data files here under their respective Bundesland directories.

Example:

- data_local/baden_wuerttemberg/file1.gml
- data_local/bayern/file2.gml

Ensure the directory structure matches the Bundesland names used in the data_ingest scripts.



&&& FILE: ./utils\small-portion.py
&&& CONTENT:
import json

# Path to the large GeoJSON file
input_file = r"output_dir\650_5478_repr.geojson"

# Path for the output smaller GeoJSON file
output_file = "650_5478_repr_small.geojson"

# Desired size of the output file in bytes (50 MB)
desired_size = 1 * 1024 * 1024

def extract_small_geojson(input_file, output_file, max_size):
    try:
        with open(input_file, 'r', encoding='utf-8') as infile:
            # Initialize variables to construct the new GeoJSON
            small_data = {
                "type": "FeatureCollection",
                "features": []
            }
            
            # Read the opening part of the GeoJSON file
            line = infile.readline()
            while line.strip() != '"features": [':
                line = infile.readline()

            # Read features one by one
            current_size = 0
            for line in infile:
                if line.strip() == ']':
                    break  # End of features list

                # Remove trailing comma if present
                feature_str = line.rstrip(',\n')
                
                # Parse feature JSON
                feature = json.loads(feature_str)
                
                # Add feature to the new collection
                small_data["features"].append(feature)
                
                # Update current size
                current_size += len(feature_str.encode('utf-8'))
                
                # Stop if the current size exceeds or reaches the desired size
                if current_size >= max_size:
                    break

            # Write closing brackets for the JSON structure
            with open(output_file, 'w', encoding='utf-8') as outfile:
                json.dump(small_data, outfile, ensure_ascii=False, indent=2)
            
            print(f"Extracted {len(small_data['features'])} features into {output_file}")

    except Exception as e:
        print(f"An error occurred: {e}")

# Run the function to extract a smaller GeoJSON file
extract_small_geojson(input_file, output_file, desired_size)

